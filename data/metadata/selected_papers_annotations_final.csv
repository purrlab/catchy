index,title,venue_published,year_published,authors,abstract,pdf_path,img_path,semanticscholar_id,citation_count_openalex,citations_per_year_openalex,citation_count_semantics,citations_per_year_semantics
131,Intrinsic Characterization of Dynamic Surfaces,CVPR,2013,"['Tony Tung', 'Takashi Matsuyama']","This paper presents a novel approach to characterize deformable surface using intrinsic property dynamics. 3D dynamic surfaces representing humans in motion can be obtained using multiple view stereo reconstruction methods or depth cameras. Nowadays these technologies have become capable to capture surface variations in real-time, and give details such as clothing wrinkles and deformations. Assuming repetitive patterns in the deformations, we propose to model complex surface variations using sets of linear dynamical systems (LDS) where observations across time are given by surface intrinsic properties such as local curvatures. We introduce an approach based on bags of dynamical systems, where each surface feature to be represented in the codebook is modeled by a set of LDS equipped with timing structure. Experiments are performed on datasets of real-world dynamical surfaces and show compelling results for description, classification and segmentation.",./data/pdfs/CVPR2013/Intrinsic Characterization of Dynamic Surfaces.pdf,./data/imgs/CVPR2013/Intrinsic Characterization of Dynamic Surfaces.png,3dfc3a22b86cbbb34f9ab9acdcfe952bfeaa0378,5.0,"{2018: 2, 2014: 3}",6.0,"{2018: 1, 2017: 1, 2014: 4}"
327,Multi-agent Event Detection: Localization and Role Assignment,CVPR,2013,"['Suha Kwak', 'Bohyung Han', 'Joon Hee Han']","We present a joint estimation technique of event localization and role assignment when the target video event is described by a scenario. Specifically, to detect multi-agent events from video, our algorithm identifies agents involved in an event and assigns roles to the participating agents. Instead of iterating through all possible agent-role combinations, we formulate the joint optimization problem as two efficient sub problems-quadratic programming for role assignment followed by linear programming for event localization. Additionally, we reduce the computational complexity significantly by applying role-specific event detectors to each agent independently. We test the performance of our algorithm in natural videos, which contain multiple target events and nonparticipating agents.",./data/pdfs/CVPR2013/Multi-agent Event Detection: Localization and Role Assignment.pdf,./data/imgs/CVPR2013/Multi-agent Event Detection: Localization and Role Assignment.png,1e5b3a3fd5aba9ca2214934e651d8d4ee68494df,23.0,"{2021: 3, 2020: 1, 2019: 1, 2017: 1, 2016: 7, 2015: 7, 2014: 2, 2013: 1}",16.0,"{2021: 1, 2020: 1, 2017: 1, 2016: 6, 2015: 5, 2014: 2}"
270,Towards Efficient and Exact MAP-Inference for Large Scale Discrete Computer Vision Problems via Combinatorial Optimization,CVPR,2013,"['Jörg Hendrik Kappes', 'M. Speth', 'Gerhard Reinelt', 'Christoph Schnörr']","Discrete graphical models (also known as discrete Markov random fields) are a major conceptual tool to model the structure of optimization problems in computer vision. While in the last decade research has focused on fast approximative methods, algorithms that provide globally optimal solutions have come more into the research focus in the last years. However, large scale computer vision problems seemed to be out of reach for such methods. In this paper we introduce a promising way to bridge this gap based on partial optimality and structural properties of the underlying problem factorization. Combining these preprocessing steps, we are able to solve grids of size 2048×2048 in less than 90 seconds. On the hitherto unsolvable Chinese character dataset of Nowozin et. al we obtain provably optimal results in 56% of the instances and achieve competitive runtimes on other recent benchmark problems. While in the present work only generalized Potts models are considered, an extension to general graphical models seems to be feasible.",./data/pdfs/CVPR2013/Towards Efficient and Exact MAP-Inference for Large Scale Discrete Computer Vision Problems via Combinatorial Optimization.pdf,./data/imgs/CVPR2013/Towards Efficient and Exact MAP-Inference for Large Scale Discrete Computer Vision Problems via Combinatorial Optimization.png,e7221bb783befce955b7b6a2cf8edf0f640b2cb5,32.0,"{2023: 1, 2022: 1, 2021: 1, 2020: 3, 2019: 2, 2018: 2, 2017: 1, 2016: 3, 2015: 8, 2014: 7, 2013: 3}",33.0,"{2023: 1, 2022: 1, 2021: 2, 2020: 3, 2019: 4, 2018: 2, 2017: 1, 2016: 2, 2015: 3, 2014: 11, 2013: 3}"
372,Multi-target Tracking by Rank-1 Tensor Approximation,CVPR,2013,"['Xinchu Shi', 'Haibin Ling', 'Junliang Xing', 'Weiming Hu']","In this paper we formulate multi-target tracking (MTT) as a rank-1 tensor approximation problem and propose an ℓ <sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sub> norm tensor power iteration solution. In particular, a high order tensor is constructed based on trajectories in the time window, with each tensor element as the affinity of the corresponding trajectory candidate. The local assignment variables are the ℓ <sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sub> normalized vectors, which are used to approximate the rank-1 tensor. Our approach provides a flexible and effective formulation where both pairwise and high-order association energies can be used expediently. We also show the close relation between our formulation and the multi-dimensional assignment (MDA) model. To solve the optimization in the rank-1 tensor approximation, we propose an algorithm that iteratively powers the intermediate solution followed by an ℓ <sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sub> normalization. Aside from effectively capturing high-order motion information, the proposed solver runs efficiently with proved convergence. The experimental validations are conducted on two challenging datasets and our method demonstrates promising performances on both.",./data/pdfs/CVPR2013/Multi-target Tracking by Rank-1 Tensor Approximation.pdf,./data/imgs/CVPR2013/Multi-target Tracking by Rank-1 Tensor Approximation.png,b281f6cf99eeb8dbb9bb0c31a57827c8c0493e7f,48.0,"{2024: 1, 2023: 1, 2022: 1, 2021: 2, 2020: 2, 2019: 5, 2018: 7, 2017: 7, 2016: 11, 2015: 4, 2014: 7}",50.0,"{2024: 1, 2022: 1, 2021: 3, 2020: 1, 2019: 7, 2018: 5, 2017: 6, 2016: 16, 2015: 3, 2014: 7}"
239,Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis,CVPR,2013,"['Nikolaos Kyriazis', 'Antonis Argyros']","In several hand-object(s) interaction scenarios, the change in the objects' state is a direct consequence of the hand's motion. This has a straightforward representation in Newtonian dynamics. We present the first approach that exploits this observation to perform model-based 3D tracking of a table-top scene comprising passive objects and an active hand. Our forward modelling of 3D hand-object(s) interaction regards both the appearance and the physical state of the scene and is parameterized over the hand motion (26 DoFs) between two successive instants in time. We demonstrate that our approach manages to track the 3D pose of all objects and the 3D pose and articulation of the hand by only searching for the parameters of the hand motion. In the proposed framework, covert scene state is inferred by connecting it to the overt state, through the incorporation of physics. Thus, our tracking approach treats a variety of challenging observability issues in a principled manner, without the need to resort to heuristics.",./data/pdfs/CVPR2013/Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis.pdf,./data/imgs/CVPR2013/Physically Plausible 3D Scene Tracking: The Single Actor Hypothesis.png,7f2312fc106697dfbae8e05015999593834ccc17,63.0,"{2024: 4, 2023: 2, 2022: 1, 2021: 4, 2020: 7, 2019: 6, 2018: 6, 2017: 10, 2016: 6, 2015: 7, 2014: 10}",72.0,"{2024: 3, 2022: 1, 2021: 3, 2020: 5, 2019: 9, 2018: 10, 2017: 11, 2016: 6, 2015: 13, 2014: 11}"
67,Tensor-Based Human Body Modeling,CVPR,2013,"['Yinpeng Chen', 'Zicheng Liu', 'Zhengyou Zhang']","In this paper, we present a novel approach to model 3D human body with variations on both human shape and pose, by exploring a tensor decomposition technique. 3D human body modeling is important for 3D reconstruction and animation of realistic human body, which can be widely used in Tele-presence and video game applications. It is challenging due to a wide range of shape variations over different people and poses. The existing SCAPE model is popular in computer vision for modeling 3D human body. However, it considers shape and pose deformations separately, which is not accurate since pose deformation is person-dependent. Our tensor-based model addresses this issue by jointly modeling shape and pose deformations. Experimental results demonstrate that our tensor-based model outperforms the SCAPE model quite significantly. We also apply our model to capture human body using Microsoft Kinect sensors with excellent results.",./data/pdfs/CVPR2013/Tensor-Based Human Body Modeling.pdf,./data/imgs/CVPR2013/Tensor-Based Human Body Modeling.png,14f5c200bf7a83c404e4573612db78e1fd0c364e,96.0,"{2024: 1, 2023: 4, 2022: 5, 2021: 6, 2020: 8, 2019: 8, 2018: 13, 2017: 16, 2016: 7, 2015: 15, 2014: 12}",97.0,"{2024: 1, 2023: 1, 2022: 5, 2021: 6, 2020: 7, 2019: 13, 2018: 12, 2017: 16, 2016: 10, 2015: 15, 2014: 10, 2013: 1}"
3,"3D-Based Reasoning with Blocks, Support, and Stability",CVPR,2013,"['Zhaoyin Jia', 'Andrew Gallagher', 'Ashutosh Saxena', 'Tsuhan Chen']","3D volumetric reasoning is important for truly understanding a scene. Humans are able to both segment each object in an image, and perceive a rich 3D interpretation of the scene, e.g., the space an object occupies, which objects support other objects, and which objects would, if moved, cause other objects to fall. We propose a new approach for parsing RGB-D images using 3D block units for volumetric reasoning. The algorithm fits image segments with 3D blocks, and iteratively evaluates the scene based on block interaction properties. We produce a 3D representation of the scene based on jointly optimizing over segmentations, block fitting, supporting relations, and object stability. Our algorithm incorporates the intuition that a good 3D representation of the scene is the one that fits the data well, and is a stable, self-supporting (i.e., one that does not topple) arrangement of objects. We experiment on several datasets including controlled and real indoor scenarios. Results show that our stability-reasoning framework improves RGB-D segmentation and scene volumetric representation.","./data/pdfs/CVPR2013/3D-Based Reasoning with Blocks, Support, and Stability.pdf","./data/imgs/CVPR2013/3D-Based Reasoning with Blocks, Support, and Stability.png",57e1cfaa392a82955609994f79a536bb143ccc01,97.0,"{2021: 4, 2020: 13, 2019: 8, 2018: 7, 2017: 9, 2016: 15, 2015: 19, 2014: 18, 2013: 4}",123.0,"{2024: 2, 2023: 1, 2022: 1, 2021: 5, 2020: 11, 2019: 6, 2018: 16, 2017: 11, 2016: 22, 2015: 19, 2014: 23, 2013: 6}"
234,Online Dominant and Anomalous Behavior Detection in Videos,CVPR,2013,"['Mehrsan Javan', 'Martin D. Levine']","We present a novel approach for video parsing and simultaneous online learning of dominant and anomalous behaviors in surveillance videos. Dominant behaviors are those occurring frequently in videos and hence, usually do not attract much attention. They can be characterized by different complexities in space and time, ranging from a scene background to human activities. In contrast, an anomalous behavior is defined as having a low likelihood of occurrence. We do not employ any models of the entities in the scene in order to detect these two kinds of behaviors. In this paper, video events are learnt at each pixel without supervision using densely constructed spatio-temporal video volumes. Furthermore, the volumes are organized into large contextual graphs. These compositions are employed to construct a hierarchical codebook model for the dominant behaviors. By decomposing spatio-temporal contextual information into unique spatial and temporal contexts, the proposed framework learns the models of the dominant spatial and temporal events. Thus, it is ultimately capable of simultaneously modeling high-level behaviors as well as low-level spatial, temporal and spatio-temporal pixel level changes.",./data/pdfs/CVPR2013/Online Dominant and Anomalous Behavior Detection in Videos.pdf,./data/imgs/CVPR2013/Online Dominant and Anomalous Behavior Detection in Videos.png,2dccec3c1a8a17883cece784e8f0fc0af413eb83,166.0,"{2024: 3, 2023: 10, 2022: 4, 2021: 12, 2020: 14, 2019: 9, 2018: 31, 2017: 25, 2016: 22, 2015: 27, 2014: 7, 2013: 2}",167.0,"{2023: 8, 2022: 5, 2021: 11, 2020: 14, 2019: 14, 2018: 27, 2017: 27, 2016: 28, 2015: 19, 2014: 12, 2013: 2}"
52,Self-Paced Learning for Long-Term Tracking,CVPR,2013,"['James Steven Supančič', 'Deva Ramanan']","We address the problem of long-term object tracking, where the object may become occluded or leave-the-view. In this setting, we show that an accurate appearance model is considerably more effective than a strong motion model. We develop simple but effective algorithms that alternate between tracking and learning a good appearance model given a track. We show that it is crucial to learn from the ""right"" frames, and use the formalism of self-paced curriculum learning to automatically select such frames. We leverage techniques from object detection for learning accurate appearance-based templates, demonstrating the importance of using a large negative training set (typically not used for tracking). We describe both an offline algorithm (that processes frames in batch) and a linear-time on-line (i.e. causal) algorithm that approaches real-time performance. Our models significantly outperform prior art, reducing the average error on benchmark videos by a factor of 4.",./data/pdfs/CVPR2013/Self-Paced Learning for Long-Term Tracking.pdf,./data/imgs/CVPR2013/Self-Paced Learning for Long-Term Tracking.png,1c721511e4c0e21bd264ca71c0d909528511b7ad,301.0,"{2025: 2, 2024: 9, 2023: 10, 2022: 12, 2021: 23, 2020: 28, 2019: 47, 2018: 48, 2017: 36, 2016: 33, 2015: 37, 2014: 13, 2013: 2, 2012: 1}",303.0,"{2024: 2, 2023: 7, 2022: 9, 2021: 20, 2020: 31, 2019: 40, 2018: 49, 2017: 50, 2016: 41, 2015: 32, 2014: 18, 2013: 4}"
121,Salient Object Detection: A Discriminative Regional Feature Integration Approach,CVPR,2013,"['Huaizu Jiang', 'Jingdong Wang', 'Zejian Yuan', 'Wu Yang', 'Nanning Zheng', 'Shipeng Li']","Salient object detection has been attracting a lot of interest, and recently various heuristic computational models have been designed. In this paper, we regard saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, uses the supervised learning approach to map the regional feature vector to a saliency score, and finally fuses the saliency scores across multiple levels, yielding the saliency map. The contributions lie in two-fold. One is that we show our approach, which integrates the regional contrast, regional property and regional background ness descriptors together to form the master saliency map, is able to produce superior saliency maps to existing algorithms most of which combine saliency maps heuristically computed from different types of features. The other is that we introduce a new regional feature vector, background ness, to characterize the background, which can be regarded as a counterpart of the objectness descriptor [2]. The performance evaluation on several popular benchmark data sets validates that our approach outperforms existing state-of-the-arts.",./data/pdfs/CVPR2013/Salient Object Detection: A Discriminative Regional Feature Integration Approach.pdf,./data/imgs/CVPR2013/Salient Object Detection: A Discriminative Regional Feature Integration Approach.png,ec76ea04ccb07048c2abb527774888b6a33c7edc,1176.0,"{2025: 6, 2024: 38, 2023: 62, 2022: 61, 2021: 106, 2020: 129, 2019: 162, 2018: 158, 2017: 176, 2016: 135, 2015: 106, 2014: 32, 2013: 3}",1223.0,"{2024: 21, 2023: 53, 2022: 62, 2021: 100, 2020: 168, 2019: 169, 2018: 179, 2017: 198, 2016: 136, 2015: 103, 2014: 33, 2013: 1}"
6731,Using Projection Kurtosis Concentration Of Natural Images For Blind Noise Covariance Matrix Estimation,CVPR,2014,"['Xing Zhang', 'Siwei Lyu']","Kurtosis of 1D projections provides important statistical characteristics of natural images. In this work, we first provide a theoretical underpinning to a recently observed phenomenon known as projection kurtosis concentration that the kurtosis of natural images over different band-pass channels tend to concentrate around a typical value. Based on this analysis, we further describe a new method to estimate the covariance matrix of correlated Gaussian noise from a noise corrupted image using random band-pass filters. We demonstrate the effectiveness of our blind noise covariance matrix estimation method on natural images.",./data/pdfs/CVPR2014/Using Projection Kurtosis Concentration Of Natural Images For Blind Noise Covariance Matrix Estimation.pdf,./data/imgs/CVPR2014/Using Projection Kurtosis Concentration Of Natural Images For Blind Noise Covariance Matrix Estimation.png,8fd1eec9abbcde53971cea1b01252829cfc857bd,3.0,"{2017: 2, 2016: 1}",4.0,"{2024: 1, 2023: 1, 2017: 1, 2016: 1}"
6836,Hierarchical Feature Hashing for Fast Dimensionality Reduction,CVPR,2014,"['Bin Zhao', 'Eric P. Xing']","Curse of dimensionality is a practical and challenging problem in image categorization, especially in cases with a large number of classes. Multi-class classification encounters severe computational and storage problems when dealing with these large scale tasks. In this paper, we propose hierarchical feature hashing to effectively reduce dimensionality of parameter space without sacrificing classification accuracy, and at the same time exploit information in semantic taxonomy among categories. We provide detailed theoretical analysis on our proposed hashing method. Moreover, experimental results on object recognition and scene classification further demonstrate the effectiveness of hierarchical feature hashing.",./data/pdfs/CVPR2014/Hierarchical Feature Hashing for Fast Dimensionality Reduction.pdf,./data/imgs/CVPR2014/Hierarchical Feature Hashing for Fast Dimensionality Reduction.png,6e428db07a54a824f77a4c1a8fe9e70d6049e79c,9.0,"{2023: 2, 2022: 1, 2017: 1, 2016: 1, 2015: 4}",11.0,"{2023: 1, 2021: 1, 2017: 1, 2016: 3, 2015: 3, 2014: 2}"
6565,Multi-target Tracking with Motion Context in Tensor Power Iteration,CVPR,2014,"['Xinchu Shi', 'Haibin Ling', 'Weiming Hu', 'Chunfeng Yuan', 'Junliang Xing']","Interactions between moving targets often provide discriminative clues for multiple target tracking (MTT), though many existing approaches ignore such interactions due to difficulty in effectively handling them. In this paper, we model interactions between neighbor targets by pair-wise motion context, and further encode such context into the global association optimization. To solve the resulting global non-convex maximization, we propose an effective and efficient power iteration framework. This solution enjoys two advantages for MTT: First, it allows us to combine the global energy accumulated from individual trajectories and the between-trajectory interaction energy into a united optimization, which can be solved by the proposed power iteration algorithm. Second, the framework is flexible to accommodate various types of pairwise context models and we in fact studied two different context models in this paper. For evaluation, we apply the proposed methods to four public datasets involving different challenging scenarios such as dense aerial borne traffic tracking, dense point set tracking, and semi-crowded pedestrian tracking. In all the experiments, our approaches demonstrate very promising results in comparison with state-of-the-art trackers.",./data/pdfs/CVPR2014/Multi-target Tracking with Motion Context in Tensor Power Iteration.pdf,./data/imgs/CVPR2014/Multi-target Tracking with Motion Context in Tensor Power Iteration.png,e8109017563b4692aeaff85f00273217d52f00ee,30.0,"{2022: 1, 2021: 1, 2020: 4, 2019: 2, 2018: 3, 2017: 1, 2016: 9, 2015: 7, 2014: 2}",28.0,"{2022: 1, 2020: 1, 2019: 2, 2018: 2, 2017: 2, 2016: 11, 2015: 6, 2014: 3}"
6952,Multi-Object Tracking via Constrained Sequential Labeling,CVPR,2014,"['Sheng Chen', 'Alan Fern', 'Siniša Todorović']","This paper presents a new approach to tracking people in crowded scenes, where people are subject to long-term (partial) occlusions and may assume varying postures and articulations. In such videos, detection-based trackers give poor performance since detecting people occurrences is not reliable, and common assumptions about locally smooth trajectories do not hold. Rather, we use temporal mid-level features (e.g., supervoxels or dense point trajectories) as a more coherent spatiotemporal basis for handling occlusion and pose variations. Thus, we formulate tracking as labeling mid-level features by object identifiers, and specify a new approach, called constrained sequential labeling (CSL), for performing this labeling. CSL uses a cost function to sequentially assign labels while respecting the implications of hard constraints computed via constraint propagation. A key feature of this approach is that it allows for the use of flexible cost functions and constraints that capture complex dependencies that cannot be represented in standard network-flow formulations. To exploit this flexibility we describe how to learn constraints and give a provably correct learning algorithms for cost functions that achieves finitetime convergence at a rate that improves with the strength of the constraints. Our experimental results indicate that CSL outperforms the state-of-the-art on challenging real-world videos of volleyball, basketball, and pedestrians walking.",./data/pdfs/CVPR2014/Multi-Object Tracking via Constrained Sequential Labeling.pdf,./data/imgs/CVPR2014/Multi-Object Tracking via Constrained Sequential Labeling.png,4822c1bf765cf99193a231c000c19ae5d0c10a00,40.0,"{2024: 1, 2022: 1, 2021: 3, 2020: 2, 2019: 2, 2018: 5, 2017: 5, 2016: 11, 2015: 7, 2014: 3}",43.0,"{2023: 1, 2022: 2, 2021: 2, 2020: 3, 2019: 3, 2018: 2, 2017: 7, 2016: 13, 2015: 7, 2014: 3}"
7046,A Minimal Solution to the Generalized Pose-and-Scale Problem,CVPR,2014,"['Jonathan Ventura', 'Clemens Arth', 'Gerhard Reitmayr', 'Dieter Schmalstieg']","We propose a novel solution to the generalized camera pose problem which includes the internal scale of the generalized camera as an unknown parameter. This further generalization of the well-known absolute camera pose problem has applications in multi-frame loop closure. While a well-calibrated camera rig has a fixed and known scale, camera trajectories produced by monocular motion estimation necessarily lack a scale estimate. Thus, when performing loop closure in monocular visual odometry, or registering separate structure-from-motion reconstructions, we must estimate a seven degree-of-freedom similarity transform from corresponding observations. Existing approaches solve this problem, in specialized configurations, by aligning 3D triangulated points or individual camera pose estimates. Our approach handles general configurations of rays and points and directly estimates the full similarity transformation from the 2D-3D correspondences. Four correspondences are needed in the minimal case, which has eight possible solutions. The minimal solver can be used in a hypothesize-and-test architecture for robust transformation estimation. Our solver also produces a least-squares estimate in the overdetermined case. The approach is evaluated experimentally on synthetic and real datasets, and is shown to produce higher accuracy solutions to multi-frame loop closure than existing approaches.",./data/pdfs/CVPR2014/A Minimal Solution to the Generalized Pose-and-Scale Problem.pdf,./data/imgs/CVPR2014/A Minimal Solution to the Generalized Pose-and-Scale Problem.png,152213bc5b226bcd8674a70fffc449d6d6ab86ac,58.0,"{2023: 6, 2022: 3, 2021: 5, 2020: 11, 2019: 10, 2018: 6, 2017: 4, 2016: 8, 2015: 3, 2014: 2}",55.0,"{2023: 5, 2022: 3, 2021: 3, 2020: 12, 2019: 8, 2018: 6, 2017: 4, 2016: 8, 2015: 3, 2014: 3}"
6945,Persistent Tracking for Wide Area Aerial Surveillance,CVPR,2014,"['Jan Prokaj', 'Gérard Medioni']","Persistent surveillance of large geographic areas from unmanned aerial vehicles allows us to learn much about the daily activities in the region of interest. Nearly all of the approaches addressing tracking in this imagery are detection-based and rely on background subtraction or frame differencing to provide detections. This, however, makes it difficult to track targets once they slow down or stop, which is not acceptable for persistent tracking, our goal. We present a multiple target tracking approach that does not exclusively rely on background subtraction and is better able to track targets through stops. It accomplishes this by effectively running two trackers in parallel: one based on detections from background subtraction providing target initialization and reacquisition, and one based on a target state regressor providing frame to frame tracking. We evaluated the proposed approach on a long sequence from a wide area aerial imagery dataset, and the results show improved object detection rates and ID-switch rates with limited increases in false alarms compared to the competition.",./data/pdfs/CVPR2014/Persistent Tracking for Wide Area Aerial Surveillance.pdf,./data/imgs/CVPR2014/Persistent Tracking for Wide Area Aerial Surveillance.png,c844cde672df9d0a262427fb484358309a6428cc,67.0,"{2024: 2, 2022: 3, 2021: 4, 2020: 7, 2019: 6, 2018: 12, 2017: 8, 2016: 15, 2015: 9, 2014: 1}",63.0,"{2024: 1, 2022: 3, 2021: 5, 2020: 7, 2019: 4, 2018: 9, 2017: 9, 2016: 14, 2015: 10, 2014: 1}"
6988,Similarity Comparisons for Interactive Fine-Grained Categorization,CVPR,2014,"['Catherine Wah', 'Grant Van Horn', 'Steve Branson', 'Subhransu Maji', 'Pietro Perona', 'Serge Belongie']","Current human-in-the-loop fine-grained visual categorization systems depend on a predefined vocabulary of attributes and parts, usually determined by experts. In this work, we move away from that expert-driven and attribute-centric paradigm and present a novel interactive classification system that incorporates computer vision and perceptual similarity metrics in a unified framework. At test time, users are asked to judge relative similarity between a query image and various sets of images, these general queries do not require expert-defined terminology and are applicable to other domains and basic-level categories, enabling a flexible, efficient, and scalable system for fine-grained categorization with humans in the loop. Our system outperforms existing state-of-the-art systems for relevance feedback-based image retrieval as well as interactive classification, resulting in a reduction of up to 43% in the average number of questions needed to correctly classify an image.",./data/pdfs/CVPR2014/Similarity Comparisons for Interactive Fine-Grained Categorization.pdf,./data/imgs/CVPR2014/Similarity Comparisons for Interactive Fine-Grained Categorization.png,0e73d2b0f943cf8559da7f5002414ccc26bc77cd,85.0,"{2024: 2, 2023: 2, 2022: 2, 2021: 11, 2020: 7, 2019: 10, 2018: 18, 2017: 9, 2016: 12, 2015: 11, 2014: 1}",98.0,"{2023: 3, 2022: 3, 2021: 8, 2020: 6, 2019: 10, 2018: 18, 2017: 15, 2016: 16, 2015: 18, 2014: 1}"
6908,Efficient High-Resolution Stereo Matching using Local Plane Sweeps,CVPR,2014,"['Sudipta N. Sinha', 'Daniel Scharstein', 'Richard Szeliski']","We present a stereo algorithm designed for speed and efficiency that uses local slanted plane sweeps to propose disparity hypotheses for a semi-global matching algorithm. Our local plane hypotheses are derived from initial sparse feature correspondences followed by an iterative clustering step. Local plane sweeps are then performed around each slanted plane to produce out-of-plane parallax and matching-cost estimates. A final global optimization stage, implemented using semi-global matching, assigns each pixel to one of the local plane hypotheses. By only exploring a small fraction of the whole disparity space volume, our technique achieves significant speedups over previous algorithms and achieves state-of-the-art accuracy on high-resolution stereo pairs of up to 19 megapixels.",./data/pdfs/CVPR2014/Efficient High-Resolution Stereo Matching using Local Plane Sweeps.pdf,./data/imgs/CVPR2014/Efficient High-Resolution Stereo Matching using Local Plane Sweeps.png,fd8542a3f52436f81fc4e3668eaeb3d0fba7644b,110.0,"{2024: 3, 2022: 2, 2021: 14, 2020: 11, 2019: 17, 2018: 14, 2017: 13, 2016: 20, 2015: 14, 2014: 2}",125.0,"{2024: 1, 2023: 1, 2022: 3, 2021: 10, 2020: 13, 2019: 19, 2018: 15, 2017: 16, 2016: 21, 2015: 23, 2014: 3}"
6678,Illumination-Aware Age Progression,CVPR,2014,"['Ira Kemelmacher-Shlizerman', 'Supasorn Suwajanakorn', 'Steven M. Seitz']","We present an approach that takes a single photograph of a child as input and automatically produces a series of age-progressed outputs between 1 and 80 years of age, accounting for pose, expression, and illumination. Leveraging thousands of photos of children and adults at many ages from the Internet, we first show how to compute average image subspaces that are pixel-to-pixel aligned and model variable lighting. These averages depict a prototype man and woman aging from 0 to 80, under any desired illumination, and capture the differences in shape and texture between ages. Applying these differences to a new photo yields an age progressed result. Contributions include relightable age subspaces, a novel technique for subspace-to-subspace alignment, and the most extensive evaluation of age progression techniques in the literature.",./data/pdfs/CVPR2014/Illumination-Aware Age Progression.pdf,./data/imgs/CVPR2014/Illumination-Aware Age Progression.png,763cd502f3a7d9ae8b92658d05eba26ac234f066,247.0,"{2025: 1, 2024: 12, 2023: 11, 2022: 16, 2021: 29, 2020: 40, 2019: 31, 2018: 39, 2017: 28, 2016: 25, 2015: 14, 2014: 1}",207.0,"{2024: 4, 2023: 8, 2022: 14, 2021: 17, 2020: 30, 2019: 33, 2018: 37, 2017: 28, 2016: 22, 2015: 13, 2014: 1}"
6762,"Cross-view Action Modeling, Learning and Recognition",CVPR,2014,"['Jiang Wang', 'Xiaohan Nie', 'Xia Yin', 'Ying Wu', 'Song‐Chun Zhu']","Existing methods on video-based action recognition are generally view-dependent, i.e., performing recognition from the same views seen in the training data. We present a novel multiview spatio-temporal and-or graph (MST-AOG) representation for cross-view action recognition, i.e., the recognition is performed on the video from an unknown and unseen view. As a compositional model, MST-AOG compactly represents the hierarchical combinatorial structures of cross-view actions by explicitly modeling the geometry, appearance and motion variations. This paper proposes effective methods to learn the structure and parameters of MST-AOG. The inference based on MST-AOG enables action recognition from novel views. The training of MST-AOG takes advantage of the 3D human skeleton data obtained from Kinect cameras to avoid annotating enormous multi-view video frames, which is error-prone and time-consuming, but the recognition does not need 3D information and is based on 2D video input. A new Multiview Action3D dataset has been created and will be released. Extensive experiments have demonstrated that this new action representation significantly improves the accuracy and robustness for cross-view action recognition on 2D videos.","./data/pdfs/CVPR2014/Cross-view Action Modeling, Learning and Recognition.pdf","./data/imgs/CVPR2014/Cross-view Action Modeling, Learning and Recognition.png",5de214630011554bd07b41ec5bd493c7f65c532e,508.0,"{2025: 17, 2024: 94, 2023: 80, 2022: 55, 2021: 62, 2020: 61, 2019: 38, 2018: 39, 2017: 25, 2016: 28, 2015: 6, 2014: 1}",446.0,"{2024: 32, 2023: 77, 2022: 57, 2021: 55, 2020: 59, 2019: 43, 2018: 49, 2017: 41, 2016: 24, 2015: 7, 2014: 2}"
9065,Graph-Based Simplex Method for Pairwise Energy Minimization With Binary Variables,CVPR,2015,['Daniel Průša'],We show how the simplex algorithm can be tailored to the linear programming relaxation of pairwise energy minimization with binary variables. A special structure formed by basic and nonbasic variables in each stage of the algorithm is identified and utilized to perform the whole iterative process combinatorially over the input energy minimization graph rather than algebraically over the simplex tableau. This leads to a new efficient solver. We demonstrate that for some computer vision instances it performs even better than methods reducing binary energy minimization to finding maximum flow in a network.,./data/pdfs/CVPR2015/Graph-Based Simplex Method for Pairwise Energy Minimization With Binary Variables.pdf,./data/imgs/CVPR2015/Graph-Based Simplex Method for Pairwise Energy Minimization With Binary Variables.png,9f96eb9faff1e47df2036d63ee6fc5a4a0979c64,4.0,"{2020: 1, 2017: 2, 2016: 1}",6.0,"{2020: 1, 2018: 1, 2017: 2, 2016: 1, 2015: 1}"
8659,A Fast Algorithm for Elastic Shape Distances Between Closed Planar Curves,CVPR,2015,"['Günay Doğan', 'Javier Bernal', 'Charles Hagwood']","Effective computational tools for shape analysis are needed in many areas of science and engineering. We address this and propose a new fast iterative algorithm to compute the elastic geodesic distance between shapes of closed planar curves. The original algorithm for this has cubic time complexity with respect to the number of nodes per curve. Hence it is not suitable for large shape data sets. We aim for large-scale shape analysis and thus propose an iterative algorithm based on the original one but with quadratic time complexity. In practice, we observe subquadratic, almost linear running times, and that our algorithm scales very well with large numbers of nodes. The key to our algorithm is the decoupling of the optimization for the starting point and rotation from that of the reparametrization, and the development of fast dynamic programming and iterative nonlinear constrained optimization algorithms that work in tandem to compute optimal reparametrizations fast.",./data/pdfs/CVPR2015/A Fast Algorithm for Elastic Shape Distances Between Closed Planar Curves.pdf,./data/imgs/CVPR2015/A Fast Algorithm for Elastic Shape Distances Between Closed Planar Curves.png,742ea0ab0e511176a2e1476447f343e6b887b1df,15.0,"{2024: 1, 2022: 1, 2021: 5, 2018: 1, 2017: 1, 2016: 5, 2015: 1}",18.0,"{2023: 1, 2022: 3, 2021: 5, 2019: 1, 2018: 2, 2017: 1, 2016: 3, 2015: 2}"
9112,A Geodesic-Preserving Method for Image Warping,CVPR,2015,"['Dongping Li', 'Kaiming He', 'Jian Sun', 'Kun Zhou']","The manipulation of panoramic/wide-angle images is usually achieved via image warping. Though various techniques have been developed for preserving shapes and straight lines for warping, these are not sufficient for panoramic/wide-angle images. The image projections will turn the straight lines into curved ""geodesic lines"", and it is fundamentally impossible to keep all these lines straight. In this work, we propose a geodesic-preserving method for content-aware image warping. An energy term is introduced to preserve the geodesic appearance of the geodesic lines, and can be used with shape-preserving terms. Our method is demonstrated in various applications, including rectangling panoramas, resizing panoramic/wide-angle images, and wide-angle image manipulation. An extension to ellipse preservation for general images is also presented.",./data/pdfs/CVPR2015/A Geodesic-Preserving Method for Image Warping.pdf,./data/imgs/CVPR2015/A Geodesic-Preserving Method for Image Warping.png,cecb1f0f90ba789218b1b094c554731d119f379e,27.0,"{2024: 7, 2023: 4, 2022: 3, 2021: 3, 2019: 3, 2018: 5, 2017: 1, 2016: 1}",30.0,"{2024: 6, 2023: 3, 2022: 6, 2021: 2, 2020: 1, 2019: 3, 2018: 4, 2017: 3, 2016: 1, 2015: 1}"
8704,Beyond the Shortest Path : Unsupervised Domain Adaptation by Sampling Subspaces Along the Spline Flow,CVPR,2015,"['Rui Caseiro', 'João F. Henriques', 'Pedro Martins', 'Jorge Batista']","Recently, a particular paradigm [18] in the domain adaptation field has received considerable attention by introducing novel and important insights to the problem. In this case, the source and target domains are represented in the form of subspaces, which are treated as points on the Grassmann manifold. The geodesic curve between them is sampled to obtain intermediate points. Then a classifier is learnt using the projections of the data onto these subspaces. Despite its relevance and popularity, this paradigm [18] contains some limitations. Firstly, in real-world applications, that simple curve (i.e. shortest path) does not provide the necessary flexibility to model the domain shift between the training and testing data sets. Secondly, by using the geodesic curve, we are restricted to only one source domain, which does not allow to take fully advantage of the multiple datasets that are available nowadays. It is then, natural to ask whether this popular concept could be extended to deal with more complex curves and to integrate multi-sources domains. This is a hard problem considering the Riemannian structure of the space, but we propose a mathematically well-founded idea that enables us to solve it. We exploit the geometric insight of rolling maps [30] to compute a spline curve on the Grassmann manifold. The benefits of the proposed idea are demonstrated through several empirical studies on standard datasets. This novel concept allows to explicitly integrate multi-source domains while the previous one [18] uses the mean of all sources. This enables to model better the domain shift and take fully advantage of the training datasets.",./data/pdfs/CVPR2015/Beyond the Shortest Path : Unsupervised Domain Adaptation by Sampling Subspaces Along the Spline Flow.pdf,./data/imgs/CVPR2015/Beyond the Shortest Path : Unsupervised Domain Adaptation by Sampling Subspaces Along the Spline Flow.png,a83c8432a443dfcd3f0f9abdc6f20bc54d23f8ba,59.0,"{2024: 1, 2023: 3, 2022: 1, 2021: 3, 2020: 7, 2019: 9, 2018: 12, 2017: 9, 2016: 11, 2015: 3}",54.0,"{2022: 1, 2021: 2, 2020: 4, 2019: 11, 2018: 9, 2017: 10, 2016: 13, 2015: 4}"
8749,DEEP-CARVING: Discovering Visual Attributes by Carving Deep Neural Nets,CVPR,2015,"['Sukrit Shankar', 'Vikas Garg', 'Roberto Cipolla']","Most of the approaches for discovering visual attributes in images demand significant supervision, which is cumbersome to obtain. In this paper, we aim to discover visual attributes in a weakly supervised setting that is commonly encountered with contemporary image search engines. For instance, given a noun (say forest) and its associated attributes (say dense, sunlit, autumn), search engines can now generate many valid images for any attribute-noun pair (dense forests, autumn forests, etc). However, images for an attributenoun pair do not contain any information about other attributes (like which forests in the autumn are dense too). Thus, a weakly supervised scenario occurs: each of the M attributes corresponds to a class such that a training image in class m ∈ {1, . . . , M} contains a single label that indicates the presence of the mth attribute only. The task is to discover all the attributes present in a test image. Deep Convolutional Neural Networks (CNNs) [20] have enjoyed remarkable success in vision applications recently. However, in a weakly supervised scenario, widely used CNN training procedures do not learn a robust modelfor predicting multiple attribute labels simultaneously. The primary reason is that the attributes highly co-occur within the training data, and unlike objects, do not generally exist as well-defined spatial boundaries within the image. To ameliorate this limitation, we propose Deep-Carving, a novel training procedure with CNNs, that helps the net efficiently carve itselffor the task of multiple attribute prediction. During training, the responses of the feature maps are exploited in an ingenious way to provide the net with multiple pseudo-labels (for training images) for subsequent iterations. The process is repeated periodically after a fixed number of iterations, and enables the net carve itself iteratively for efficiently disentangling features. Additionally, we contribute a noun-adjective pairing inspired Natural Scenes Attributes Dataset to the research community, CAMITNSAD, containing a number of co-occurring attributes within a noun category. We describe, in detail, salient aspects of this dataset. Our experiments on CAMITNSAD and the SUN Attributes Dataset [29], with weak supervision, clearly demonstrate that the Deep-Carved CNNs consistently achieve considerable improvement in the precision of attribute prediction over popular baseline methods.",./data/pdfs/CVPR2015/DEEP-CARVING: Discovering Visual Attributes by Carving Deep Neural Nets.pdf,./data/imgs/CVPR2015/DEEP-CARVING: Discovering Visual Attributes by Carving Deep Neural Nets.png,aeef40ee709f7f945cb3c5cbc449761332fd4462,46.0,"{2024: 1, 2022: 2, 2020: 2, 2019: 5, 2018: 8, 2017: 15, 2016: 12, 2015: 1}",60.0,"{2022: 3, 2020: 3, 2019: 7, 2018: 7, 2017: 15, 2016: 21, 2015: 4}"
8548,A Maximum Entropy Feature Descriptor for Age Invariant Face Recognition,CVPR,2015,"['Dihong Gong', 'Zhifeng Li', 'Dacheng Tao', 'Jianzhuang Liu', 'Xuelong Li']","In this paper, we propose a new approach to overcome the representation and matching problems in age invariant face recognition. First, a new maximum entropy feature descriptor (MEFD) is developed that encodes the microstructure of facial images into a set of discrete codes in terms of maximum entropy. By densely sampling the encoded face image, sufficient discriminatory and expressive information can be extracted for further analysis. A new matching method is also developed, called identity factor analysis (IFA), to estimate the probability that two faces have the same underlying identity. The effectiveness of the framework is confirmed by extensive experimentation on two face aging datasets, MORPH (the largest public-domain face aging dataset) and FGNET. We also conduct experiments on the famous LFW dataset to demonstrate the excellent generalizability of our new approach.",./data/pdfs/CVPR2015/A Maximum Entropy Feature Descriptor for Age Invariant Face Recognition.pdf,./data/imgs/CVPR2015/A Maximum Entropy Feature Descriptor for Age Invariant Face Recognition.png,1d3dd9aba79a53390317ec1e0b7cd742cba43132,108.0,"{2024: 2, 2023: 7, 2022: 11, 2021: 19, 2020: 16, 2019: 18, 2018: 11, 2017: 16, 2016: 6, 2015: 2}",96.0,"{2023: 4, 2022: 7, 2021: 16, 2020: 14, 2019: 15, 2018: 14, 2017: 17, 2016: 6, 2015: 3}"
8961,Matching-CNN Meets KNN: Quasi-Parametric Human Parsing,CVPR,2015,"['Si Liu', 'Xiaodan Liang', 'Luoqi Liu', 'Xiaohui Shen', 'Shuicheng Yan', 'Changsheng Xu', 'Liang Lin', 'Xiaochun Cao', 'Shuicheng Yan']","Both parametric and non-parametric approaches have demonstrated encouraging performances in the human parsing task, namely segmenting a human image into several semantic regions (e.g., hat, bag, left arm, face). In this work, we aim to develop a new solution with the advantages of both methodologies, namely supervision from annotated data and the flexibility to use newly annotated (possibly uncommon) images, and present a quasi-parametric human parsing model. Under the classic K Nearest Neighbor (KNN)-based nonparametric framework, the parametric Matching Convolutional Neural Network (M-CNN) is proposed to predict the matching confidence and displacements of the best matched region in the testing image for a particular semantic region in one KNN image. Given a testing image, we first retrieve its KNN images from the annotated/manually-parsed human image corpus. Then each semantic region in each KNN image is matched with confidence to the testing image using M-CNN, and the matched regions from all KNN images are further fused, followed by a superpixel smoothing procedure to obtain the ultimate human parsing result. The M-CNN differs from the classic CNN [12] in that the tailored cross image matching filters are introduced to characterize the matching between the testing image and the semantic region of a KNN image. The cross image matching filters are defined at different convolutional layers, each aiming to capture a particular range of displacements. Comprehensive evaluations over a large dataset with 7,700 annotated human images well demonstrate the significant performance gain from the quasi-parametric model over the state-of-the-arts [29, 30], for the human parsing task.",./data/pdfs/CVPR2015/Matching-CNN Meets KNN: Quasi-Parametric Human Parsing.pdf,./data/imgs/CVPR2015/Matching-CNN Meets KNN: Quasi-Parametric Human Parsing.png,690e990c586e9a0876f2ff00a28c87bd617f01e7,135.0,"{2024: 3, 2023: 6, 2022: 12, 2021: 7, 2020: 11, 2019: 21, 2018: 22, 2017: 22, 2016: 26, 2015: 5}",166.0,"{2024: 1, 2023: 9, 2022: 9, 2021: 8, 2020: 19, 2019: 22, 2018: 25, 2017: 31, 2016: 29, 2015: 13}"
8576,Subgraph Decomposition for Multi-Target Tracking,CVPR,2015,"['Siyu Tang', 'Bjoern Andres', 'Mykhaylo Andriluka', 'Bernt Schiele']","Tracking multiple targets in a video, based on a finite set of detection hypotheses, is a persistent problem in computer vision. A common strategy for tracking is to first select hypotheses spatially and then to link these over time while maintaining disjoint path constraints [14, 15, 24]. In crowded scenes multiple hypotheses will often be similar to each other making selection of optimal links an unnecessary hard optimization problem due to the sequential treatment of space and time. Embracing this observation, we propose to link and cluster plausible detections jointly across space and time. Specifically, we state multi-target tracking as a Minimum Cost Subgraph Multicut Problem. Evidence about pairs of detection hypotheses is incorporated whether the detections are in the same frame, neighboring frames or distant frames. This facilitates long-range re-identification and within-frame clustering. Results for published benchmark sequences demonstrate the superiority of this approach.",./data/pdfs/CVPR2015/Subgraph Decomposition for Multi-Target Tracking.pdf,./data/imgs/CVPR2015/Subgraph Decomposition for Multi-Target Tracking.png,65c7e127436cfa8cd76e2d302a820b0308c31106,207.0,"{2024: 7, 2023: 13, 2022: 18, 2021: 24, 2020: 33, 2019: 36, 2018: 31, 2017: 28, 2016: 11, 2015: 6}",180.0,"{2024: 2, 2023: 3, 2022: 11, 2021: 23, 2020: 31, 2019: 33, 2018: 31, 2017: 24, 2016: 13, 2015: 8, 2014: 1}"
8666,Taking a Deeper Look at Pedestrians,CVPR,2015,"['Jan Hosang', 'Mohamed Omran', 'Rodrigo Benenson', 'Bernt Schiele']","In this paper we study the use of convolutional neural networks (convnets) for the task of pedestrian detection. Despite their recent diverse successes, convnets historically underperform compared to other pedestrian detectors. We deliberately omit explicitly modelling the problem into the network (e.g. parts or occlusion modelling) and show that we can reach competitive performance without bells and whistles. In a wide range of experiments we analyse small and big convnets, their architectural choices, parameters, and the influence of different training data, including pretraining on surrogate tasks. We present the best convnet detectors on the Caltech and KITTI dataset. On Caltech our convnets reach top performance both for the Caltech1x and Caltech10x training setup. Using additional data at training time our strongest convnet model is competitive even to detectors that use additional data (optical flow) at test time.",./data/pdfs/CVPR2015/Taking a Deeper Look at Pedestrians.pdf,./data/imgs/CVPR2015/Taking a Deeper Look at Pedestrians.png,3054aa4f655c279cc79223970f80b11c67231b0b,291.0,"{2025: 2, 2024: 4, 2023: 9, 2022: 14, 2021: 19, 2020: 35, 2019: 49, 2018: 50, 2017: 57, 2016: 35, 2015: 17}",348.0,"{2024: 6, 2023: 5, 2022: 19, 2021: 18, 2020: 37, 2019: 52, 2018: 73, 2017: 70, 2016: 46, 2015: 20, 2014: 1, 2013: 1}"
8764,Simultaneous Feature Learning and Hash Coding With Deep Neural Networks,CVPR,2015,"['Hanjiang Lai', 'Yan Pan', 'Ye Liu', 'Shuicheng Yan']","Similarity-preserving hashing is a widely-used method for nearest neighbour search in large-scale image retrieval tasks. For most existing hashing methods, an image is first encoded as a vector of hand-engineering visual features, followed by another separate projection or quantization step that generates binary codes. However, such visual feature vectors may not be optimally compatible with the coding process, thus producing sub-optimal hashing codes. In this paper, we propose a deep architecture for supervised hashing, in which images are mapped into binary codes via carefully designed deep neural networks. The pipeline of the proposed deep architecture consists of three building blocks: 1) a sub-network with a stack of convolution layers to produce the effective intermediate image features; 2) a divide-and-encode module to divide the intermediate image features into multiple branches, each encoded into one hash bit; and 3) a triplet ranking loss designed to characterize that one image is more similar to the second image than to the third one. Extensive evaluations on several benchmark image datasets show that the proposed simultaneous feature learning and hash coding pipeline brings substantial improvements over other state-of-the-art supervised or unsupervised hashing methods.",./data/pdfs/CVPR2015/Simultaneous Feature Learning and Hash Coding With Deep Neural Networks.pdf,./data/imgs/CVPR2015/Simultaneous Feature Learning and Hash Coding With Deep Neural Networks.png,73543e2f04bc496930e5d5e9790559d4510e7596,914.0,"{2025: 4, 2024: 27, 2023: 63, 2022: 67, 2021: 111, 2020: 129, 2019: 168, 2018: 138, 2017: 137, 2016: 53, 2015: 16}",791.0,"{2024: 13, 2023: 52, 2022: 57, 2021: 89, 2020: 128, 2019: 127, 2018: 131, 2017: 124, 2016: 53, 2015: 17}"
8329,Mirror Surface Reconstruction Under an Uncalibrated Camera,CVPR,2016,"['Kai Han', 'Kenneth K. Wong', 'Dirk Schnieders', 'Miaomiao Liu']","This paper addresses the problem of mirror surface reconstruction, and a solution based on observing the reflections of a moving reference plane on the mirror surface is proposed. Unlike previous approaches which require tedious work to calibrate the camera, our method can recover both the camera intrinsics and extrinsics together with the mirror surface from reflections of the reference plane under at least three unknown distinct poses. Our previous work has demonstrated that 3D poses of the reference plane can be registered in a common coordinate system using reflection correspondences established across images. This leads to a bunch of registered 3D lines formed from the reflection correspondences. Given these lines, we first derive an analytical solution to recover the camera projection matrix through estimating the line projection matrix. We then optimize the camera projection matrix by minimizing reprojection errors computed based on a cross-ratio formulation. The mirror surface is finally reconstructed based on the optimized cross-ratio constraint. Experimental results on both synthetic and real data are presented, which demonstrate the feasibility and accuracy of our method.",./data/pdfs/CVPR2016/Mirror Surface Reconstruction Under an Uncalibrated Camera.pdf,./data/imgs/CVPR2016/Mirror Surface Reconstruction Under an Uncalibrated Camera.png,58ddc303b25f988c5ed4b656da28f17624647443,3.0,"{2023: 1, 2021: 1, 2016: 1}",3.0,"{2023: 1, 2021: 1, 2016: 1}"
8049,Real-Time Depth Refinement for Specular Objects,CVPR,2016,"['Roy Or-El', 'Rom Hershkovitz', 'Aaron Wetzler', 'Guy Rosman', 'Alfred M. Bruckstein⋆', 'Ron Kimmel']","The introduction of consumer RGB-D scanners set off a major boost in 3D computer vision research. Yet, the precision of existing depth scanners is not accurate enough to recover fine details of a scanned object. While modern shading based depth refinement methods have been proven to work well with Lambertian objects, they break down in the presence of specularities. We present a novel shape from shading framework that addresses this issue and enhances both diffuse and specular objects' depth profiles. We take advantage of the built-in monochromatic IR projector and IR images of the RGB-D scanners and present a lighting model that accounts for the specular regions in the input image. Using this model, we reconstruct the depth map in real-time. Both quantitative tests and visual evaluations prove that the proposed method produces state of the art depth reconstruction results.",./data/pdfs/CVPR2016/Real-Time Depth Refinement for Specular Objects.pdf,./data/imgs/CVPR2016/Real-Time Depth Refinement for Specular Objects.png,76e0e308521945a71038cd7f0848cc8768372d77,20.0,"{2025: 1, 2024: 1, 2023: 1, 2021: 1, 2020: 3, 2019: 4, 2018: 7, 2017: 1, 2016: 1}",24.0,"{2024: 2, 2022: 2, 2020: 4, 2019: 2, 2018: 8, 2017: 4, 2016: 2}"
8499,Interactive Segmentation on RGBD Images via Cue Selection,CVPR,2016,"['Jie Feng', 'Brian Price', 'Scott Cohen', 'Shih‐Fu Chang']","Interactive image segmentation is an important problem in computer vision with many applications including image editing, object recognition and image retrieval. Most existing interactive segmentation methods only operate on color images. Until recently, very few works have been proposed to leverage depth information from low-cost sensors to improve interactive segmentation. While these methods achieve better results than color-based methods, they are still limited in either using depth as an additional color channel or simply combining depth with color in a linear way. We propose a novel interactive segmentation algorithm which can incorporate multiple feature cues like color, depth, and normals in an unified graph cut framework to leverage these cues more effectively. A key contribution of our method is that it automatically selects a single cue to be used at each pixel, based on the intuition that only one cue is necessary to determine the segmentation label locally. This is achieved by optimizing over both segmentation labels and cue labels, using terms designed to decide where both the segmentation and label cues should change. Our algorithm thus produces not only the segmentation mask but also a cue label map that indicates where each cue contributes to the final result. Extensive experiments on five large scale RGBD datasets show that our proposed algorithm performs significantly better than both other color-based and RGBD based algorithms in reducing the amount of user inputs as well as increasing segmentation accuracy.",./data/pdfs/CVPR2016/Interactive Segmentation on RGBD Images via Cue Selection.pdf,./data/imgs/CVPR2016/Interactive Segmentation on RGBD Images via Cue Selection.png,f025a44f492340da546aae2b7bb2f88cbd10933f,29.0,"{2025: 1, 2024: 1, 2022: 2, 2021: 2, 2020: 4, 2019: 2, 2018: 9, 2017: 7, 2016: 1}",26.0,"{2024: 1, 2022: 2, 2021: 1, 2020: 4, 2019: 3, 2018: 7, 2017: 6, 2016: 1, 2015: 1}"
8075,Structure From Motion With Objects,CVPR,2016,"['Marco Crocco', 'Cosimo Rubino', 'Alessio Del Bue']","This paper shows for the first time that is possible to reconstruct the position of rigid objects and to jointly recover affine camera calibration solely from a set of object detections in a video sequence. In practice, this work can be considered as the extension of Tomasi and Kanade factorization method using objects. Instead of using points to form a rank constrained measurement matrix, we can form a matrix with similar rank properties using 2D object detection proposals. In detail, we first fit an ellipse onto the image plane at each bounding box as given by the object detector. The collection of all the ellipses in the dual space is used to create a measurement matrix that gives a specific rank constraint. This matrix can be factorised and metrically upgraded in order to provide the affine camera matrices and the 3D position of the objects as an ellipsoid. Moreover, we recover the full 3D quadric thus giving additional information about object occupancy and 3D pose. Finally, we also show that 2D points measurements can be seamlessly included in the framework to reduce the number of objects required. This last aspect unifies the classical point-based Tomasi and Kanade approach with objects in a unique framework. Experiments with synthetic and real data show the feasibility of our approach for the affine camera case.",./data/pdfs/CVPR2016/Structure From Motion With Objects.pdf,./data/imgs/CVPR2016/Structure From Motion With Objects.png,39a31bf20afa536b60da32efc70ae0cc7f067328,43.0,"{2024: 4, 2023: 3, 2022: 5, 2021: 5, 2020: 4, 2019: 5, 2018: 10, 2017: 7}",47.0,"{2024: 1, 2023: 3, 2022: 6, 2021: 3, 2020: 8, 2019: 6, 2018: 14, 2017: 6}"
8185,Multi-Scale Patch Aggregation (MPA) for Simultaneous Detection and Segmentation,CVPR,2016,"['Shu Liu', 'Xiaojuan Qi', 'Jianping Shi', 'Hong Zhang', 'Jiaya Jia']","Aiming at simultaneous detection and segmentation (SD-S), we propose a proposal-free framework, which detect and segment object instances via mid-level patches. We design a unified trainable network on patches, which is followed by a fast and effective patch aggregation algorithm to infer object instances. Our method benefits from end-to-end training. Without object proposal generation, computation time can also be reduced. In experiments, our method yields results 62.1% and 61.8% in terms of mAPr on VOC2012 segmentation val and VOC2012 SDS val, which are state-of-the-art at the time of submission. We also report results on Microsoft COCO test-std/test-dev dataset in this paper.",./data/pdfs/CVPR2016/Multi-Scale Patch Aggregation (MPA) for Simultaneous Detection and Segmentation.pdf,./data/imgs/CVPR2016/Multi-Scale Patch Aggregation (MPA) for Simultaneous Detection and Segmentation.png,2e5a1da43c62ea9b0b5df832221a7b1cb641427e,87.0,"{2025: 2, 2024: 3, 2023: 4, 2022: 4, 2021: 10, 2020: 11, 2019: 16, 2018: 15, 2017: 18, 2016: 4}",75.0,"{2024: 4, 2023: 1, 2022: 8, 2021: 5, 2020: 11, 2019: 12, 2018: 12, 2017: 15, 2016: 6, 2014: 1}"
8151,Proposal Flow,CVPR,2016,"['Bumsub Ham', 'Minsu Cho', 'Cordelia Schmid', 'Jean Ponce']","Finding image correspondences remains a challenging problem in the presence of intra-class variations and large changes in scene layout. Semantic flow methods are designed to handle images depicting different instances of the same object or scene category. We introduce a novel approach to semantic flow, dubbed proposal flow, that establishes reliable correspondences using object proposals. Unlike prevailing semantic flow approaches that operate on pixels or regularly sampled local regions, proposal flow benefits from the characteristics of modern object proposals, that exhibit high repeatability at multiple scales, and can take advantage of both local and geometric consistency constraints among proposals. We also show that proposal flow can effectively be transformed into a conventional dense flow field. We introduce a new dataset that can be used to evaluate both general semantic flow techniques and region-based approaches such as proposal flow. We use this benchmark to compare different matching algorithms, object proposals, and region features within proposal flow, to the state of the art in semantic flow. This comparison, along with experiments on standard datasets, demonstrates that proposal flow significantly outperforms existing semantic flow methods in various settings.",./data/pdfs/CVPR2016/Proposal Flow.pdf,./data/imgs/CVPR2016/Proposal Flow.png,aeac8eecf1cadda77c821e004dcbc6a72ef99297,95.0,"{2024: 6, 2023: 5, 2022: 13, 2021: 15, 2020: 13, 2019: 15, 2018: 13, 2017: 14, 2016: 1}",121.0,"{2024: 6, 2023: 20, 2022: 20, 2021: 29, 2020: 16, 2019: 19, 2018: 7, 2017: 4}"
8208,Analyzing Classifiers: Fisher Vectors and Deep Neural Networks,CVPR,2016,"['Sebastian Lapuschkin', 'Alexander Binder', 'Grégoire Montavon', 'Klaus‐Robert Müller', 'Wojciech Samek']","Fisher vector (FV) classifiers and Deep Neural Networks (DNNs) are popular and successful algorithms for solving image classification problems. However, both are generally considered 'black box' predictors as the non-linear transformations involved have so far prevented transparent and interpretable reasoning. Recently, a principled technique, Layer-wise Relevance Propagation (LRP), has been developed in order to better comprehend the inherent structured reasoning of complex nonlinear classification models such as Bag of Feature models or DNNs. In this paper we (1) extend the LRP framework also for Fisher vector classifiers and then use it as analysis tool to (2) quantify the importance of context for classification, (3) qualitatively compare DNNs against FV classifiers in terms of important image regions and (4) detect potential flaws and biases in data. All experiments are performed on the PASCAL VOC 2007 and ILSVRC 2012 data sets.",./data/pdfs/CVPR2016/Analyzing Classifiers: Fisher Vectors and Deep Neural Networks.pdf,./data/imgs/CVPR2016/Analyzing Classifiers: Fisher Vectors and Deep Neural Networks.png,43dc45dca9c1e641c7855a033a91a71746ca8832,210.0,"{2025: 1, 2024: 12, 2023: 22, 2022: 37, 2021: 32, 2020: 34, 2019: 32, 2018: 14, 2017: 13, 2016: 13}",187.0,"{2024: 1, 2023: 16, 2022: 22, 2021: 26, 2020: 33, 2019: 32, 2018: 23, 2017: 13, 2016: 19, 2015: 2}"
8236,Actions ~ Transformations,CVPR,2016,"['Xiaolong Wang', 'Ali Farhadi', 'Abhinav Gupta']","What defines an action like ""kicking ball""? We argue that the true meaning of an action lies in the change or transformation an action brings to the environment. In this paper, we propose a novel representation for actions by modeling an action as a transformation which changes the state of the environment before the action happens (precondition) to the state after the action (effect). Motivated by recent advancements of video representation using deep learning, we design a Siamese network which models the action as a transformation on a high-level feature space. We show that our model gives improvements on standard action recognition datasets including UCF101 and HMDB51. More importantly, our approach is able to generalize beyond learned action categories and shows significant performance improvement on cross-category generalization on our new ACT dataset.",./data/pdfs/CVPR2016/Actions ~ Transformations.pdf,./data/imgs/CVPR2016/Actions ~ Transformations.png,8d093efcfadda3ac7de7b0e1ca7d9aa2005c2ec3,228.0,"{2024: 10, 2023: 10, 2022: 19, 2021: 21, 2020: 25, 2019: 39, 2018: 50, 2017: 45, 2016: 9}",222.0,"{2024: 4, 2023: 11, 2022: 17, 2021: 15, 2020: 22, 2019: 40, 2018: 46, 2017: 47, 2016: 20}"
8446,DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection,CVPR,2016,"['Nian Liu', 'Junwei Han']","Traditional salient object detection models often use hand-crafted features to formulate contrast and various prior knowledge, and then combine them artificially. In this work, we propose a novel end-to-end deep hierarchical saliency network (DHSNet) based on convolutional neural networks for detecting salient objects. DHSNet first makes a coarse global prediction by automatically learning various global structured saliency cues, including global contrast, objectness, compactness, and their optimal combination. Then a novel hierarchical recurrent convolutional neural network (HRCNN) is adopted to further hierarchically and progressively refine the details of saliency maps step by step via integrating local context information. The whole architecture works in a global to local and coarse to fine manner. DHSNet is directly trained using whole images and corresponding ground truth saliency masks. When testing, saliency maps can be generated by directly and efficiently feed forwarding testing images through the network, without relying on any other techniques. Evaluations on four benchmark datasets and comparisons with other 11 state-of-the-art algorithms demonstrate that DHSNet not only shows its significant superiority in terms of performance, but also achieves a real-time speed of 23 FPS on modern GPUs.",./data/pdfs/CVPR2016/DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection.pdf,./data/imgs/CVPR2016/DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection.png,c1c4bb9974990f70d46c9d4bd5cca7e7940273e6,844.0,"{2025: 7, 2024: 49, 2023: 58, 2022: 61, 2021: 121, 2020: 161, 2019: 177, 2018: 139, 2017: 63, 2016: 5}",684.0,"{2024: 13, 2023: 45, 2022: 45, 2021: 83, 2020: 162, 2019: 140, 2018: 126, 2017: 61, 2016: 8, 2014: 1}"
8087,"A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation",CVPR,2016,"['N. Michael Mayer', 'Eddy Ilg', 'Philip Häusser', 'Philipp Fischer', 'Daniel Cremers', 'Alexey Dosovitskiy', 'Thomas Brox']","Recent work has shown that optical flow estimation can be formulated as a supervised learning task and can be successfully solved with convolutional networks. Training of the so-called FlowNet was enabled by a large synthetically generated dataset. The present paper extends the concept of optical flow estimation via convolutional networks to disparity and scene flow estimation. To this end, we propose three synthetic stereo video datasets with sufficient realism, variation, and size to successfully train large networks. Our datasets are the first large-scale datasets to enable training and evaluating scene flow methods. Besides the datasets, we present a convolutional network for real-time disparity estimation that provides state-of-the-art results. By combining a flow and disparity estimation network and training it jointly, we demonstrate the first scene flow estimation with a convolutional network.","./data/pdfs/CVPR2016/A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation.pdf","./data/imgs/CVPR2016/A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation.png",1ced31e02234bc3d1092ffb2c7442ffbd51cb309,1375.0,"{2025: 3, 2024: 34, 2023: 52, 2022: 50, 2021: 328, 2020: 337, 2019: 313, 2018: 145, 2017: 88, 2016: 20, 2014: 4, 2012: 1}",2270.0,"{2024: 141, 2023: 347, 2022: 355, 2021: 346, 2020: 368, 2019: 314, 2018: 241, 2017: 113, 2016: 39, 2015: 1, 2014: 5}"
7617,Learning to Predict Stereo Reliability Enforcing Local Consistency of Confidence Maps,CVPR,2017,"['Matteo Poggi', 'Stefano Mattoccia']","Confidence measures estimate unreliable disparity assignments performed by a stereo matching algorithm and, as recently proved, can be used for several purposes. This paper aims at increasing, by means of a deep network, the effectiveness of state-of-the-art confidence measures exploiting the local consistency assumption. We exhaustively evaluated our proposal on 23 confidence measures, including 5 top-performing ones based on random-forests and CNNs, training our networks with two popular stereo algorithms and a small subset (25 out of 194 frames) of the KITTI 2012 dataset. Experimental results show that our approach dramatically increases the effectiveness of all the 23 confidence measures on the remaining frames. Moreover, without re-training, we report a further cross-evaluation on KITTI 2015 and Middlebury 2014 confirming that our proposal provides remarkable improvements for each confidence measure even when dealing with significantly different input data. To the best of our knowledge, this is the first method to move beyond conventional pixel-wise confidence estimation.",./data/pdfs/CVPR2017/Learning to Predict Stereo Reliability Enforcing Local Consistency of Confidence Maps.pdf,./data/imgs/CVPR2017/Learning to Predict Stereo Reliability Enforcing Local Consistency of Confidence Maps.png,4621ed5c4958f7cba86122a76af6039a66295f05,47.0,"{2024: 1, 2023: 1, 2022: 3, 2021: 8, 2020: 12, 2019: 11, 2018: 7, 2017: 4}",15.0,"{2023: 2, 2022: 1, 2021: 1, 2020: 6, 2019: 3, 2018: 1, 2017: 1}"
7437,Unsupervised Vanishing Point Detection and Camera Calibration From a Single Manhattan Image With Radial Distortion,CVPR,2017,"['Michel Antunes', 'João P. Barreto', 'Djamila Aouada', 'Björn Ottersten']","The article concerns the automatic calibration of a camera with radial distortion from a single image. It is known that, under the mild assumption of square pixels and zero skew, lines in the scene project into circles in the image, and three lines suffice to calibrate the camera up to an ambiguity between focal length and radial distortion. The calibration results highly depend on accurate circle estimation, which is hard to accomplish because lines tend to project into short circular arcs. To overcome this problem, we show that, given a short circular arc edge, it is possible to robustly determine a line that goes through the center of the corresponding circle. These lines, henceforth called Lines of Circle Centres (LCCs), are used in a new method that detects sets of parallel lines and estimates the calibration parameters, including the center and amount of distortion, focal length, and camera orientation with respect to the Manhattan frame. Extensive experiments in both semi-synthetic and real images show that our algorithm outperforms state-of-the-art approaches in unsupervised calibration from a single image, while providing more information.",./data/pdfs/CVPR2017/Unsupervised Vanishing Point Detection and Camera Calibration From a Single Manhattan Image With Radial Distortion.pdf,./data/imgs/CVPR2017/Unsupervised Vanishing Point Detection and Camera Calibration From a Single Manhattan Image With Radial Distortion.png,f315995e1c33ab7f5b47ba03a5c1ab902398862c,40.0,"{2024: 5, 2023: 8, 2022: 4, 2021: 3, 2020: 10, 2019: 5, 2018: 5}",30.0,"{2024: 2, 2023: 4, 2022: 5, 2021: 3, 2020: 6, 2019: 5, 2018: 4, 2017: 1}"
7872,"Face Normals ""In-The-Wild"" Using Fully Convolutional Networks",CVPR,2017,"['George Trigeorgis', 'Patrick Snape', 'Iasonas Kokkinos', 'Stefanos Zafeiriou']","In this work we pursue a data-driven approach to the problem of estimating surface normals from a single intensity image, focusing in particular on human faces. We introduce new methods to exploit the currently available facial databases for dataset construction and tailor a deep convolutional neural network to the task of estimating facial surface normals in-the-wild. We train a fully convolutional network that can accurately recover facial normals from images including a challenging variety of expressions and facial poses. We compare against state-of-the-art face Shape-from-Shading and 3D reconstruction techniques and show that the proposed network can recover substantially more accurate and realistic normals. Furthermore, in contrast to other existing face-specific surface recovery methods, we do not require the solving of an explicit alignment step due to the fully convolutional nature of our network.","./data/pdfs/CVPR2017/Face Normals ""In-The-Wild"" Using Fully Convolutional Networks.pdf","./data/imgs/CVPR2017/Face Normals ""In-The-Wild"" Using Fully Convolutional Networks.png",a97a1da1ababcb3226c6dbbc6d473c448e649da9,49.0,"{2024: 2, 2023: 1, 2022: 3, 2021: 7, 2020: 10, 2019: 10, 2018: 13, 2017: 3}",40.0,"{2023: 1, 2022: 4, 2021: 4, 2020: 9, 2019: 8, 2018: 11, 2017: 3}"
7356,S3Pool: Pooling With Stochastic Spatial Sampling,CVPR,2017,"['Shuangfei Zhai', 'Hui Wu', 'Abhishek Kumar', 'Yu Cheng', 'Yongxi Lu', 'Zhongfei Zhang', 'Rogério Feris']","Feature pooling layers (e.g., max pooling) in convolutional neural networks (CNNs) serve the dual purpose of providing increasingly abstract representations as well as yielding computational savings in subsequent convolutional layers. We view the pooling operation in CNNs as a two step procedure: first, a pooling window (e.g., 2× 2) slides over the feature map with stride one which leaves the spatial resolution intact, and second, downsampling is performed by selecting one pixel from each non-overlapping pooling window in an often uniform and deterministic (e.g., top-left) manner. Our starting point in this work is the observation that this regularly spaced downsampling arising from non-overlapping windows, although intuitive from a signal processing perspective (which has the goal of signal reconstruction), is not necessarily optimal for learning (where the goal is to generalize). We study this aspect and propose a novel pooling strategy with stochastic spatial sampling (S3Pool), where the regular downsampling is replaced by a more general stochastic version. We observe that this general stochasticity acts as a strong regularizer, and can also be seen as doing implicit data augmentation by introducing distortions in the feature maps. We further introduce a mechanism to control the amount of distortion to suit different datasets and architectures. To demonstrate the effectiveness of the proposed approach, we perform extensive experiments on several popular image classification benchmarks, observing excellent improvements over baseline models.",./data/pdfs/CVPR2017/S3Pool: Pooling With Stochastic Spatial Sampling.pdf,./data/imgs/CVPR2017/S3Pool: Pooling With Stochastic Spatial Sampling.png,eed498c1817499d4b482a691c14776495d88d60f,83.0,"{2025: 4, 2024: 7, 2023: 3, 2022: 19, 2021: 17, 2020: 7, 2019: 19, 2018: 7}",72.0,"{2024: 4, 2023: 9, 2022: 12, 2021: 12, 2020: 5, 2019: 19, 2018: 10, 2017: 1}"
7277,Spatially-Varying Blur Detection Based on Multiscale Fused and Sorted Transform Coefficients of Gradient Magnitudes,CVPR,2017,"['S. Alireza Golestaneh', 'Lina J. Karam']","The detection of spatially-varying blur without having any information about the blur type is a challenging task. In this paper, we propose a novel effective approach to address this blur detection problem from a single image without requiring any knowledge about the blur type, level, or camera settings. Our approach computes blur detection maps based on a novel High-frequency multiscale Fusion and Sort Transform (HiFST) of gradient magnitudes. The evaluations of the proposed approach on a diverse set of blurry images with different blur types, levels, and contents demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods qualitatively and quantitatively.",./data/pdfs/CVPR2017/Spatially-Varying Blur Detection Based on Multiscale Fused and Sorted Transform Coefficients of Gradient Magnitudes.pdf,./data/imgs/CVPR2017/Spatially-Varying Blur Detection Based on Multiscale Fused and Sorted Transform Coefficients of Gradient Magnitudes.png,aba8cdd3fdb776c427c634e56ff347598ac91944,101.0,"{2025: 2, 2024: 9, 2023: 16, 2022: 14, 2021: 17, 2020: 19, 2019: 13, 2018: 11}",90.0,"{2024: 6, 2023: 16, 2022: 14, 2021: 16, 2020: 20, 2019: 9, 2018: 8, 2017: 1}"
7364,What Is the Space of Attenuation Coefficients in Underwater Computer Vision?,CVPR,2017,"['Derya Akkaynak', 'Tali Treibitz', 'Tom Shlesinger', 'Yossi Loya', 'Raz Tamir', 'David Iluz']","Underwater image reconstruction methods require the knowledge of wideband attenuation coefficients per color channel. Current estimation methods for these coefficients require specialized hardware or multiple images, and none of them leverage the multitude of existing ocean optical measurements as priors. Here, we aim to constrain the set of physically-feasible wideband attenuation coefficients in the ocean by utilizing water attenuation measured worldwide by oceanographers. We calculate the space of valid wideband effective attenuation coefficients in the 3D RGB domain and find that a bound manifold in 3-space sufficiently represents the variation from the clearest to murkiest waters. We validate our model using in situ experiments in two different optical water bodies, the Red Sea and the Mediterranean. Moreover, we show that contradictory to the common image formation model, the coefficients depend on the imaging range and object reflectance, and quantify the errors resulting from ignoring these dependencies.",./data/pdfs/CVPR2017/What Is the Space of Attenuation Coefficients in Underwater Computer Vision?.pdf,./data/imgs/CVPR2017/What Is the Space of Attenuation Coefficients in Underwater Computer Vision?.png,64b9e48e71cd766b2503cc6079d52000a1133761,155.0,"{2025: 6, 2024: 42, 2023: 33, 2022: 16, 2021: 18, 2020: 13, 2019: 17, 2018: 4, 2017: 5}",128.0,"{2024: 15, 2023: 33, 2022: 23, 2021: 16, 2020: 15, 2019: 15, 2018: 6, 2017: 4, 2013: 1}"
7733,Deep Self-Taught Learning for Weakly Supervised Object Localization,CVPR,2017,"['Zequn Jie', 'Yunchao Wei', 'Xiaojie Jin', 'Jiashi Feng', 'Wei Liu']","Most existing weakly supervised localization (WSL) approaches learn detectors by finding positive bounding boxes based on features learned with image-level supervision. However, those features do not contain spatial location related information and usually provide poor-quality positive samples for training a detector. To overcome this issue, we propose a deep self-taught learning approach, which makes the detector learn the object-level features reliable for acquiring tight positive samples and afterwards re-train itself based on them. Consequently, the detector progressively improves its detection ability and localizes more informative positive samples. To implement such self-taught learning, we propose a seed sample acquisition method via image-to-object transferring and dense subgraph discovery to find reliable positive samples for initializing the detector. An online supportive sample harvesting scheme is further proposed to dynamically select the most confident tight positive samples and train the detector in a mutual boosting way. To prevent the detector from being trapped in poor optima due to overfitting, we propose a new relative improvement of predicted CNN scores for guiding the self-taught learning process. Extensive experiments on PASCAL 2007 and 2012 show that our approach outperforms the state-of-the-arts, strongly validating its effectiveness.",./data/pdfs/CVPR2017/Deep Self-Taught Learning for Weakly Supervised Object Localization.pdf,./data/imgs/CVPR2017/Deep Self-Taught Learning for Weakly Supervised Object Localization.png,b7779b0c9399ab0062c4bfa87fa101f0dc4df7a7,210.0,"{2024: 8, 2023: 14, 2022: 24, 2021: 37, 2020: 43, 2019: 49, 2018: 29, 2017: 6}",188.0,"{2024: 3, 2023: 14, 2022: 20, 2021: 32, 2020: 38, 2019: 40, 2018: 32, 2017: 9}"
7842,A Hierarchical Approach for Generating Descriptive Image Paragraphs,CVPR,2017,"['Jonathan Krause', 'Justin Johnson', 'Ranjay Krishna', 'Li Fei-Fei']","Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an image into a single sentence can describe visual content in only coarse detail. While one new captioning approach, dense captioning, can potentially describe images in finer levels of detail by captioning many regions within an image, it in turn is unable to produce a coherent story for an image. In this paper we overcome these limitations by generating entire paragraphs for describing images, which can tell detailed, unified stories. We develop a model that decomposes both images and paragraphs into their constituent parts, detecting semantic regions in images and using a hierarchical recurrent neural network to reason about language. Linguistic analysis confirms the complexity of the paragraph generation task, and thorough experiments on a new dataset of image and paragraph pairs demonstrate the effectiveness of our approach.",./data/pdfs/CVPR2017/A Hierarchical Approach for Generating Descriptive Image Paragraphs.pdf,./data/imgs/CVPR2017/A Hierarchical Approach for Generating Descriptive Image Paragraphs.png,3a7011346ce939e3251915e92ae2f252e4c7f777,375.0,"{2025: 2, 2024: 39, 2023: 46, 2022: 31, 2021: 69, 2020: 51, 2019: 86, 2018: 40, 2017: 10}",336.0,"{2024: 22, 2023: 50, 2022: 36, 2021: 58, 2020: 49, 2019: 65, 2018: 42, 2017: 13, 2015: 1}"
7683,Multi-Context Attention for Human Pose Estimation,CVPR,2017,"['Xiao Chu', 'Wei Yang', 'Wanli Ouyang', 'Cheng Ma', 'Alan Yuille', 'Xiaogang Wang']","In this paper, we propose to incorporate convolutional neural networks with a multi-context attention mechanism into an end-to-end framework for human pose estimation. We adopt stacked hourglass networks to generate attention maps from features at multiple resolutions with various semantics. The Conditional Random Field (CRF) is utilized to model the correlations among neighboring regions in the attention map. We further combine the holistic attention model, which focuses on the global consistency of the full human body, and the body part attention model, which focuses on detailed descriptions for different body parts. Hence our model has the ability to focus on different granularity from local salient regions to global semantic consistent spaces. Additionally, we design novel Hourglass Residual Units (HRUs) to increase the receptive field of the network. These units are extensions of residual units with a side branch incorporating filters with larger receptive field, hence features with various scales are learned and combined within the HRUs. The effectiveness of the proposed multi-context attention mechanism and the hourglass residual units is evaluated on two widely used human pose estimation benchmarks. Our approach outperforms all existing methods on both benchmarks over all the body parts. Code has been made publicly available.",./data/pdfs/CVPR2017/Multi-Context Attention for Human Pose Estimation.pdf,./data/imgs/CVPR2017/Multi-Context Attention for Human Pose Estimation.png,9e671c01163c9ce0ea5699ff81b5173dd03730cf,709.0,"{2025: 5, 2024: 50, 2023: 80, 2022: 95, 2021: 126, 2020: 133, 2019: 133, 2018: 71, 2017: 16}",609.0,"{2024: 12, 2023: 57, 2022: 72, 2021: 100, 2020: 131, 2019: 124, 2018: 87, 2017: 25, 2016: 1}"
7117,Speed Accuracy Trade-Offs for Modern Convolutional Object Detectors,CVPR,2017,"['Jonathan Huang', 'Vivek Rathod', 'Chen Sun', 'Menglong Zhu', 'Anoop Korattikara', 'Alireza Fathi', 'Ian Fischer', 'Zbigniew Wojna', 'Yang Song', 'Sergio Guadarrama', 'Kevin Murphy']","The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-toapples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [30], R-FCN [6] and SSD [25] systems, which we view as meta-architectures and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task.",./data/pdfs/CVPR2017/Speed Accuracy Trade-Offs for Modern Convolutional Object Detectors.pdf,./data/imgs/CVPR2017/Speed Accuracy Trade-Offs for Modern Convolutional Object Detectors.png,a312a573ef81793d56401e932ef6c9498791a3d1,2570.0,"{2025: 23, 2024: 121, 2023: 207, 2022: 277, 2021: 455, 2020: 485, 2019: 570, 2018: 358, 2017: 69}",2430.0,"{2024: 45, 2023: 168, 2022: 218, 2021: 376, 2020: 523, 2019: 572, 2018: 399, 2017: 127, 2016: 2}"
5648,Learning Patch Reconstructability for Accelerating Multi-View Stereo,CVPR,2018,"['Alex Poms', 'Chenglei Wu', 'Shoou-I Yu', 'Yaser Sheikh']","We present an approach to accelerate multi-view stereo (MVS) by prioritizing computation on image patches that are likely to produce accurate 3D surface reconstructions. Our key insight is that the accuracy of the surface reconstruction from a given image patch can be predicted significantly faster than performing the actual stereo matching. The intuition is that non-specular, fronto-parallel, in-focus patches are more likely to produce accurate surface reconstructions than highly specular, slanted, blurry patches - and that these properties can be reliably predicted from the image itself. By prioritizing stereo matching on a subset of patches that are highly reconstructable and also cover the 3D surface, we are able to accelerate MVS with minimal reduction in accuracy and completeness. To predict the reconstructability score of an image patch from a single view, we train an image-to-reconstructability neural network: the I2RNet. This reconstructability score enables us to efficiently identify image patches that are likely to provide the most accurate surface estimates before performing stereo matching. We demonstrate that the I2RNet, when trained on the ScanNet dataset, generalizes to the DTU and Tanks & Temples MVS datasets. By using our I2RNet with an existing MVS implementation, we show that our method can achieve more than a 30× speed-up over the baseline with only an minimal loss in completeness.",./data/pdfs/CVPR2018/Learning Patch Reconstructability for Accelerating Multi-View Stereo.pdf,./data/imgs/CVPR2018/Learning Patch Reconstructability for Accelerating Multi-View Stereo.png,0c4ffdd7694c0c40f9a3dfd9ffc9ac9d0f9f469d,10.0,"{2024: 1, 2022: 1, 2021: 3, 2019: 4, 2018: 1}",6.0,"{2021: 2, 2019: 3, 2018: 1}"
5778,Distributable Consistent Multi-Object Matching,CVPR,2018,"['Nan Hu', 'Qixing Huang', 'Boris Thibert', 'Leonidas Guibas']","In this paper we propose an optimization-based framework to multiple object matching. The framework takes maps computed between pairs of objects as input, and outputs maps that are consistent among all pairs of objects. The central idea of our approach is to divide the input object collection into overlapping sub-collections and enforce map consistency among each sub-collection. This leads to a distributed formulation, which is scalable to large-scale datasets. We also present an equivalence condition between this decoupled scheme and the original scheme. Experiments on both synthetic and real-world datasets show that our framework is competitive against state-of-the-art multi-object matching techniques.",./data/pdfs/CVPR2018/Distributable Consistent Multi-Object Matching.pdf,./data/imgs/CVPR2018/Distributable Consistent Multi-Object Matching.png,e74b7a51f941cfea567b5b6e189f4a56273edebb,30.0,"{2023: 4, 2021: 10, 2020: 7, 2019: 7, 2018: 2}",26.0,"{2023: 2, 2022: 1, 2021: 7, 2020: 6, 2019: 6, 2018: 4}"
6063,"Tensorize, Factorize and Regularize: Robust Visual Relationship Learning",CVPR,2018,"['Seong Jae Hwang', 'Hyunwoo J. Kim', 'Sathya N. Ravi', 'Maxwell D. Collins', 'Zirui Tao', 'Vikas Singh']","Visual relationships provide higher-level information of objects and their relations in an image - this enables a semantic understanding of the scene and helps downstream applications. Given a set of localized objects in some training data, visual relationship detection seeks to detect the most likely ""relationship"" between objects in a given image. While the specific objects may be well represented in training data, their relationships may still be infrequent. The empirical distribution obtained from seeing these relationships in a dataset does not model the underlying distribution well - a serious issue for most learning methods. In this work, we start from a simple multi-relational learning model, which in principle, offers a rich formalization for deriving a strong prior for learning visual relationships. While the inference problem for deriving the regularizer is challenging, our main technical contribution is to show how adapting recent results in numerical linear algebra lead to efficient algorithms for a factorization scheme that yields highly informative priors. The factorization provides sample size bounds for inference (under mild conditions) for the underlying [object, predicate, object] relationship learning task on its own and surprisingly outperforms (in some cases) existing methods even without utilizing visual features. Then, when integrated with an end-to-end architecture for visual relationship detection leveraging image data, we substantially improve the state-of-the-art.","./data/pdfs/CVPR2018/Tensorize, Factorize and Regularize: Robust Visual Relationship Learning.pdf","./data/imgs/CVPR2018/Tensorize, Factorize and Regularize: Robust Visual Relationship Learning.png",1b6d41795de1fd9a0da4227c83dc4dd038a229ec,70.0,"{2024: 2, 2023: 4, 2022: 5, 2021: 17, 2020: 21, 2019: 14, 2018: 7}",59.0,"{2024: 2, 2023: 4, 2022: 3, 2021: 15, 2020: 18, 2019: 8, 2018: 8, 2017: 1}"
6395,4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications,CVPR,2018,"['Shiyang Cheng', 'Irene Kotsia', 'Maja Pantić', 'Stefanos Zafeiriou']","The progress we are currently witnessing in many computer vision applications, including automatic face analysis, would not be made possible without tremendous efforts in collecting and annotating large scale visual databases. To this end, we propose 4DFAB, a new large scale database of dynamic high-resolution 3D faces (over 1,800,000 3D meshes). 4DFAB contains recordings of 180 subjects captured in four different sessions spanning over a five-year period. It contains 4D videos of subjects displaying both spontaneous and posed facial behaviours. The database can be used for both face and facial expression recognition, as well as behavioural biometrics. It can also be used to learn very powerful blendshapes for parametrising facial behaviour. In this paper, we conduct several experiments and demonstrate the usefulness of the database for various applications. The database will be made publicly available for research purposes.",./data/pdfs/CVPR2018/4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications.pdf,./data/imgs/CVPR2018/4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications.png,bc249c9be803af3d4a5a6de495e1c85578fce84b,106.0,"{2024: 10, 2023: 11, 2022: 9, 2021: 23, 2020: 29, 2019: 21, 2018: 2}",77.0,"{2024: 1, 2023: 14, 2022: 7, 2021: 12, 2020: 21, 2019: 16, 2018: 5, 2017: 1}"
5684,Anatomical Priors in Convolutional Networks for Unsupervised Biomedical Segmentation,CVPR,2018,"['Adrian V. Dalca', 'John V. Guttag', 'Mert R. Sabuncu']","We consider the problem of segmenting a biomedical image into anatomical regions of interest. We specifically address the frequent scenario where we have no paired training data that contains images and their manual segmentations. Instead, we employ unpaired segmentation images to build an anatomical prior. Critically these segmentations can be derived from imaging data from a different dataset and imaging modality than the current task. We introduce a generative probabilistic model that employs the learned prior through a convolutional neural network to compute segmentations in an unsupervised setting. We conducted an empirical analysis of the proposed approach in the context of structural brain MRI segmentation, using a multi-study dataset of more than 14,000 scans. Our results show that an anatomical prior can enable fast unsupervised segmentation which is typically not possible using standard convolutional networks. The integration of anatomical priors can facilitate CNN-based anatomical segmentation in a range of novel clinical problems, where few or no annotations are available and thus standard networks are not trainable. The code is freely available at http://github.com/adalca/neuron.",./data/pdfs/CVPR2018/Anatomical Priors in Convolutional Networks for Unsupervised Biomedical Segmentation.pdf,./data/imgs/CVPR2018/Anatomical Priors in Convolutional Networks for Unsupervised Biomedical Segmentation.png,896a89bf385a3bea1531f8d012172d4fa4393c37,118.0,"{2023: 5, 2022: 10, 2021: 32, 2020: 40, 2019: 30, 2018: 1}",131.0,"{2024: 2, 2023: 14, 2022: 14, 2021: 26, 2020: 44, 2019: 28, 2018: 3}"
5624,Flow Guided Recurrent Neural Encoder for Video Salient Object Detection,CVPR,2018,"['Guanbin Li', 'Yuan Xie', 'Tianhao Wei', 'Keze Wang', 'Liang Lin']","Image saliency detection has recently witnessed significant progress due to deep convolutional neural networks. However, extending state-of-the-art saliency detectors from image to video is challenging. The performance of salient object detection suffers from object or camera motion and the dramatic change of the appearance contrast in videos. In this paper, we present flow guided recurrent neural encoder (FGRNE), an accurate and end-to-end learning framework for video salient object detection. It works by enhancing the temporal coherence of the per-frame feature by exploiting both motion information in terms of optical flow and sequential feature evolution encoding in terms of LSTM networks. It can be considered as a universal framework to extend any FCN based static saliency detector to video salient object detection. Intensive experimental results verify the effectiveness of each part of FGRNE and confirm that our proposed method significantly outperforms state-of-the-art methods on the public benchmarks of DAVIS and FBMS.",./data/pdfs/CVPR2018/Flow Guided Recurrent Neural Encoder for Video Salient Object Detection.pdf,./data/imgs/CVPR2018/Flow Guided Recurrent Neural Encoder for Video Salient Object Detection.png,e50298e1c2c21cd4f33bc4b68d5da6f6d04796b5,184.0,"{2024: 20, 2023: 29, 2022: 20, 2021: 42, 2020: 35, 2019: 30, 2018: 6}",135.0,"{2024: 4, 2023: 20, 2022: 22, 2021: 26, 2020: 35, 2019: 17, 2018: 9, 2017: 2}"
6232,Towards Open-Set Identity Preserving Face Synthesis,CVPR,2018,"['Jianmin Bao', 'Dong Chen', 'Fang Wen', 'Houqiang Li', 'Gang Hua']","We propose a framework based on Generative Adversarial Networks to disentangle the identity and attributes of faces, such that we can conveniently recombine different identities and attributes for identity preserving face synthesis in open domains. Previous identity preserving face synthesis processes are largely confined to synthesizing faces with known identities that are already in the training dataset. To synthesize a face with identity outside the training dataset, our framework requires one input image of that subject to produce an identity vector, and any other input face image to extract an attribute vector capturing, e.g., pose, emotion, illumination, and even the background. We then recombine the identity vector and the attribute vector to synthesize a new face of the subject with the extracted attribute. Our proposed framework does not need to annotate the attributes of faces in any way. It is trained with an asymmetric loss function to better preserve the identity and stabilize the training process. It can also effectively leverage large amounts of unlabeled training face images to further improve the fidelity of the synthesized faces for subjects that are not presented in the labeled training face dataset. Our experiments demonstrate the efficacy of the proposed framework. We also present its usage in a much broader set of applications including face frontalization, face attribute morphing, and face adversarial example detection.",./data/pdfs/CVPR2018/Towards Open-Set Identity Preserving Face Synthesis.pdf,./data/imgs/CVPR2018/Towards Open-Set Identity Preserving Face Synthesis.png,c5b324f7f9abdffc1be83f640674beda81b74315,249.0,"{2025: 3, 2024: 26, 2023: 32, 2022: 34, 2021: 65, 2020: 47, 2019: 40, 2018: 2}",234.0,"{2024: 11, 2023: 37, 2022: 43, 2021: 49, 2020: 39, 2019: 43, 2018: 12}"
5985,Appearance-and-Relation Networks for Video Classification,CVPR,2018,"['Limin Wang', 'Wei Li', 'Wen Li', 'Luc Van Gool']","Spatiotemporal feature learning in videos is a fundamental problem in computer vision. This paper presents a new architecture, termed as Appearance-and-Relation Network (ARTNet), to learn video representation in an end-to-end manner. ARTNets are constructed by stacking multiple generic building blocks, called as SMART, whose goal is to simultaneously model appearance and relation from RGB input in a separate and explicit manner. Specifically, SMART blocks decouple the spatiotemporal learning module into an appearance branch for spatial modeling and a relation branch for temporal modeling. The appearance branch is implemented based on the linear combination of pixels or filter responses in each frame, while the relation branch is designed based on the multiplicative interactions between pixels or filter responses across multiple frames. We perform experiments on three action recognition benchmarks: Kinetics, UCF101, and HMDB51, demonstrating that SMART blocks obtain an evident improvement over 3D convolutions for spatiotemporal feature learning. Under the same training setting, ARTNets achieve superior performance on these three datasets to the existing state-of-the-art methods.1",./data/pdfs/CVPR2018/Appearance-and-Relation Networks for Video Classification.pdf,./data/imgs/CVPR2018/Appearance-and-Relation Networks for Video Classification.png,e6b534d8838f92461a478a3e737b73e08db94748,403.0,"{2025: 3, 2024: 33, 2023: 52, 2022: 59, 2021: 83, 2020: 94, 2019: 62, 2018: 17}",332.0,"{2024: 9, 2023: 40, 2022: 45, 2021: 57, 2020: 78, 2019: 67, 2018: 33, 2017: 3}"
6495,Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning,CVPR,2018,"['Yin Cui', 'Yang Song', 'Chen Sun', 'Andrew Howard', 'Serge Belongie']","Transferring the knowledge learned from large scale datasets (e.g., ImageNet) via fine-tuning offers an effective solution for domain-specific fine-grained visual categorization (FGVC) tasks (e.g., recognizing bird species or car make & model). In such scenarios, data annotation often calls for specialized domain knowledge and thus is difficult to scale. In this work, we first tackle a problem in large scale FGVC. Our method won first place in iNaturalist 2017 large scale species classification challenge. Central to the success of our approach is a training scheme that uses higher image resolution and deals with the long-tailed distribution of training data. Next, we study transfer learning via fine-tuning from large scale datasets to small scale, domain-specific FGVC datasets. We propose a measure to estimate domain similarity via Earth Mover's Distance and demonstrate that transfer learning benefits from pre-training on a source domain that is similar to the target domain by this measure. Our proposed transfer learning outperforms ImageNet pre-training and obtains state-of-the-art results on multiple commonly used FGVC datasets.",./data/pdfs/CVPR2018/Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning.pdf,./data/imgs/CVPR2018/Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning.png,89c3355f5bc7130ae4ed090c8accc52dd885d558,472.0,"{2025: 11, 2024: 56, 2023: 74, 2022: 61, 2021: 110, 2020: 87, 2019: 62, 2018: 5}",424.0,"{2024: 26, 2023: 65, 2022: 64, 2021: 105, 2020: 89, 2019: 59, 2018: 16}"
5996,MAttNet: Modular Attention Network for Referring Expression Comprehension,CVPR,2018,"['Licheng Yu', 'Zhe Lin', 'Xiaohui Shen', 'Shuicheng Yan', 'Xin Lu', 'Mohit Bansal', 'Tamara L. Berg']","In this paper, we address referring expression comprehension: localizing an image region described by a natural language expression. While most recent work treats expressions as a single unit, we propose to decompose them into three modular components related to subject appearance, location, and relationship to other objects. This allows us to flexibly adapt to expressions containing different types of information in an end-to-end framework. In our model, which we call the Modular Attention Network (MAttNet), two types of attention are utilized: language-based attention that learns the module weights as well as the word/phrase attention that each module should focus on; and visual attention that allows the subject and relationship modules to focus on relevant image components. Module weights combine scores from all three modules dynamically to output an overall score. Experiments show that MAttNet outperforms previous state-of-the-art methods by a large margin on both bounding-box-level and pixel-level comprehension tasks. Demo1 and code2 are provided.",./data/pdfs/CVPR2018/MAttNet: Modular Attention Network for Referring Expression Comprehension.pdf,./data/imgs/CVPR2018/MAttNet: Modular Attention Network for Referring Expression Comprehension.png,fdce9cbe5c726201575b3c8a8c1af0752f1af53f,742.0,"{2025: 6, 2024: 119, 2023: 133, 2022: 115, 2021: 132, 2020: 134, 2019: 82, 2018: 14}",691.0,"{2024: 47, 2023: 169, 2022: 156, 2021: 115, 2020: 101, 2019: 78, 2018: 25}"
814,MaxpoolNMS: Getting Rid of NMS Bottlenecks in Two-Stage Object Detectors,CVPR,2019,"['Lile Cai', 'Bin Zhao', 'Zhe Wang', 'Jie Lin', 'Chuan-Sheng Foo', 'Mohamed Sabry Aly', 'Vijay Chandrasekhar']","Modern convolutional object detectors have improved the detection accuracy significantly, which in turn inspired the development of dedicated hardware accelerators to achieve real-time performance by exploiting inherent parallelism in the algorithm. Non-maximum suppression (NMS) is an indispensable operation in object detection. In stark contrast to most operations, the commonly-adopted GreedyNMS algorithm does not foster parallelism, which can be a major performance bottleneck. In this paper, we introduce MaxpoolNMS, a parallelizable alternative to the NMS algorithm, which is based on max-pooling classification score maps. By employing a novel multi-scale multi-channel max-pooling strategy, our method is 20x faster than GreedyNMS while simultaneously achieves comparable accuracy, when quantified across various benchmarking datasets, i.e., MS COCO, KITTI and PASCAL VOC. Furthermore, our method is better suited for hardware-based acceleration than GreedyNMS.",./data/pdfs/CVPR2019/MaxpoolNMS: Getting Rid of NMS Bottlenecks in Two-Stage Object Detectors.pdf,./data/imgs/CVPR2019/MaxpoolNMS: Getting Rid of NMS Bottlenecks in Two-Stage Object Detectors.png,f14a0ea3e096ca72cb9485ddb77a94a5e492c32a,30.0,"{2024: 2, 2023: 6, 2022: 7, 2021: 8, 2020: 7}",18.0,"{2024: 1, 2023: 4, 2022: 1, 2021: 5, 2020: 6, 2019: 1}"
613,Sim-Real Joint Reinforcement Transfer for 3D Indoor Navigation,CVPR,2019,"['Fengda Zhu', 'Linchao Zhu', 'Yi Yang']","There has been an increasing interest in 3D indoor navigation, where a robot in an environment moves to a target according to an instruction. To deploy a robot for navigation in the physical world, lots of training data is required to learn an effective policy. It is quite labour intensive to obtain sufficient real environment data for training robots while synthetic data is much easier to construct by render-ing. Though it is promising to utilize the synthetic environments to facilitate navigation training in the real world, real environment are heterogeneous from synthetic environment in two aspects. First, the visual representation of the two environments have significant variances. Second, the houseplans of these two environments are quite different. There-fore two types of information,i.e. visual representation and policy behavior, need to be adapted in the reinforce mentmodel. The learning procedure of visual representation and that of policy behavior are presumably reciprocal. We pro-pose to jointly adapt visual representation and policy behavior to leverage the mutual impacts of environment and policy. Specifically, our method employs an adversarial feature adaptation model for visual representation transfer anda policy mimic strategy for policy behavior imitation. Experiment shows that our method outperforms the baseline by 19.47% without any additional human annotations.",./data/pdfs/CVPR2019/Sim-Real Joint Reinforcement Transfer for 3D Indoor Navigation.pdf,./data/imgs/CVPR2019/Sim-Real Joint Reinforcement Transfer for 3D Indoor Navigation.png,ee2ba264682d92a369d75b9a600948d314568e80,27.0,"{2023: 4, 2022: 4, 2021: 5, 2020: 7, 2019: 4, 2018: 3}",25.0,"{2023: 2, 2022: 3, 2021: 7, 2020: 7, 2019: 3, 2018: 3}"
1005,ZigZagNet: Fusing Top-Down and Bottom-Up Context for Object Segmentation,CVPR,2019,"['Di Lin', 'Dingguo Shen', 'Siting Shen', 'Yuanfeng Ji', 'Dani Lischinski', 'Daniel Cohen‐Or', 'Hui Huang']","Multi-scale context information has proven to be essential for object segmentation tasks. Recent works construct the multi-scale context by aggregating convolutional feature maps extracted by different levels of a deep neural network. This is typically done by propagating and fusing features in a one-directional, top-down and bottom-up, manner. In this work, we introduce ZigZagNet, which aggregates a richer multi-context feature map by using not only dense top-down and bottom-up propagation, but also by introducing pathways crossing between different levels of the top-down and the bottom-up hierarchies, in a zig-zag fashion. Furthermore, the context information is exchanged and aggregated over multiple stages, where the fused feature maps from one stage are fed into the next one, yielding a more comprehensive context for improved segmentation performance. Our extensive evaluation on the public benchmarks demonstrates that ZigZagNet surpasses the state-of-the-art accuracy for both semantic segmentation and instance segmentation tasks.",./data/pdfs/CVPR2019/ZigZagNet: Fusing Top-Down and Bottom-Up Context for Object Segmentation.pdf,./data/imgs/CVPR2019/ZigZagNet: Fusing Top-Down and Bottom-Up Context for Object Segmentation.png,566c5e11542ed56f62cc04110f4658b8d7a4b0ec,66.0,"{2024: 5, 2023: 7, 2022: 13, 2021: 23, 2020: 15, 2019: 3}",57.0,"{2024: 2, 2023: 7, 2022: 13, 2021: 20, 2020: 12, 2019: 3}"
1003,Object Instance Annotation With Deep Extreme Level Set Evolution,CVPR,2019,"['Zian Wang', 'David Acuna', 'Huan Ling', 'Amlan Kar', 'Sanja Fidler']","In this paper, we tackle the task of interactive object segmentation. We revive the old ideas on level set segmentation which framed object annotation as curve evolution. Carefully designed energy functions ensured that the curve was well aligned with image boundaries, and generally ""well behaved"". The Level Set Method can handle objects with complex shapes and topological changes such as merging and splitting, thus able to deal with occluded objects and objects with holes. We propose Deep Extreme Level Set Evolution that combines powerful CNN models with level set optimization in an end-to-end fashion. Our method learns to predict evolution parameters conditioned on the image and evolves the predicted initial contour to produce the final result. We make our model interactive by incorporating user clicks on the extreme boundary points, following DEXTR. We show that our approach significantly outperforms DEXTR on the static Cityscapes dataset and the video segmentation benchmark DAVIS, and performs on par on PASCAL and SBD.",./data/pdfs/CVPR2019/Object Instance Annotation With Deep Extreme Level Set Evolution.pdf,./data/imgs/CVPR2019/Object Instance Annotation With Deep Extreme Level Set Evolution.png,150b316a32fd9f10b5936f7fccd2179e83bd9ee7,91.0,"{2025: 3, 2024: 12, 2023: 7, 2022: 14, 2021: 16, 2020: 31, 2019: 8}",67.0,"{2024: 3, 2023: 5, 2022: 10, 2021: 19, 2020: 22, 2019: 8}"
1761,Mitigating Information Leakage in Image Representations: A Maximum Entropy Approach,CVPR,2019,"['Proteek Chandan Roy', 'Vishnu Naresh Boddeti']","Image recognition systems have demonstrated tremendous progress over the past few decades thanks, in part, to our ability of learning compact and robust representations of images. As we witness the wide spread adoption of these systems, it is imperative to consider the problem of unintended leakage of information from an image representation, which might compromise the privacy of the data owner. This paper investigates the problem of learning an image representation that minimizes such leakage of user information. We formulate the problem as an adversarial non-zero sum game of finding a good embedding function with two competing goals: to retain as much task dependent discriminative image information as possible, while simultaneously minimizing the amount of information, as measured by entropy, about other sensitive attributes of the user. We analyze the stability and convergence dynamics of the proposed formulation using tools from non-linear systems theory and compare to that of the corresponding adversarial zero-sum game formulation that optimizes likelihood as a measure of information content. Numerical experiments on UCI, Extended Yale B, CIFAR-10 and CIFAR-100 datasets indicate that our proposed approach is able to learn image representations that exhibit high task performance while mitigating leakage of predefined sensitive information.",./data/pdfs/CVPR2019/Mitigating Information Leakage in Image Representations: A Maximum Entropy Approach.pdf,./data/imgs/CVPR2019/Mitigating Information Leakage in Image Representations: A Maximum Entropy Approach.png,844f549adcf158883e06cd04a70c48cba18c8584,85.0,"{2025: 2, 2024: 9, 2023: 8, 2022: 13, 2021: 22, 2020: 23, 2019: 7}",81.0,"{2024: 3, 2023: 11, 2022: 19, 2021: 18, 2020: 24, 2019: 6}"
1310,Detailed Human Shape Estimation From a Single Image by Hierarchical Mesh Deformation,CVPR,2019,"['Hao Zhu', 'Xinxin Zuo', 'Sen Wang', 'Xun Cao', 'Ruigang Yang']","This paper presents a novel framework to recover detailed human body shapes from a single image. It is a challenging task due to factors such as variations in human shapes, body poses, and viewpoints. Prior methods typically attempt to recover the human body shape using a parametric based template that lacks the surface details. As such the resulting body shape appears to be without clothing. In this paper, we propose a novel learning-based framework that combines the robustness of parametric model with the flexibility of free-form 3D deformation. We use the deep neural networks to refine the 3D shape in a Hierarchical Mesh Deformation (HMD) framework, utilizing the constraints from body joints, silhouettes, and per-pixel shading information. We are able to restore detailed human body shapes beyond skinned models. Experiments demonstrate that our method has outperformed previous state-of-the-art approaches, achieving better accuracy in terms of both 2D IoU number and 3D metric distance. The code is available in https://github.com/zhuhao-nju/hmd.git.",./data/pdfs/CVPR2019/Detailed Human Shape Estimation From a Single Image by Hierarchical Mesh Deformation.pdf,./data/imgs/CVPR2019/Detailed Human Shape Estimation From a Single Image by Hierarchical Mesh Deformation.png,be237edaec30c5001b3b952ce2e8e9c02b06f32b,150.0,"{2025: 2, 2024: 19, 2023: 23, 2022: 25, 2021: 37, 2020: 40, 2019: 4}",141.0,"{2024: 9, 2023: 27, 2022: 31, 2021: 35, 2020: 32, 2019: 7}"
1604,Multispectral and Hyperspectral Image Fusion by MS HS Fusion Net,CVPR,2019,"['Qi Xie', 'Minghao Zhou', 'Qian Zhao', 'Deyu Meng', 'Wangmeng Zuo', 'Zongben Xu']","Hyperspectral imaging can help better understand the characteristics of different materials, compared with traditional image systems. However, only high-resolution multispectral (HrMS) and low-resolution hyperspectral (LrHS) images can generally be captured at video rate in practice. In this paper, we propose a model-based deep learning approach for merging an HrMS and LrHS images to generate a high-resolution hyperspectral (HrHS) image. In specific, we construct a novel MS/HS fusion model which takes the observation models of low-resolution images and the low-rankness knowledge along the spectral mode of HrHS image into consideration. Then we design an iterative algorithm to solve the model by exploiting the proximal gradient method. And then, by unfolding the designed algorithm, we construct a deep network, called MS/HS Fusion Net, with learning the proximal operators and model parameters by convolutional neural networks. Experimental results on simulated and real data substantiate the superiority of our method both visually and quantitatively as compared with state-of-the-art methods along this line of research.",./data/pdfs/CVPR2019/Multispectral and Hyperspectral Image Fusion by MS HS Fusion Net.pdf,./data/imgs/CVPR2019/Multispectral and Hyperspectral Image Fusion by MS HS Fusion Net.png,a1f76dca554fbabdec0155c0770f12417868d144,253.0,"{2025: 5, 2024: 34, 2023: 52, 2022: 59, 2021: 57, 2020: 38, 2019: 7}",187.0,"{2024: 13, 2023: 47, 2022: 45, 2021: 37, 2020: 34, 2019: 10, 2017: 1}"
1014,Leveraging the Invariant Side of Generative Zero-Shot Learning,CVPR,2019,"['Jingjing Li', 'Mengmeng Jing', 'Ke Lü', 'Zhengming Ding', 'Lei Zhu', 'Zi Huang']","Conventional zero-shot learning (ZSL) methods generally learn an embedding, e.g., visual-semantic mapping, to handle the unseen visual samples via an indirect manner. In this paper, we take the advantage of generative adversarial networks (GANs) and propose a novel method, named leveraging invariant side GAN (LisGAN), which can directly generate the unseen features from random noises which are conditioned by the semantic descriptions. Specifically, we train a conditional Wasserstein GANs in which the generator synthesizes fake unseen features from noises and the discriminator distinguishes the fake from real via a minimax game. Considering that one semantic description can correspond to various synthesized visual samples, and the semantic description, figuratively, is the soul of the generated features, we introduce soul samples as the invariant side of generative zero-shot learning in this paper. A soul sample is the meta-representation of one class. It visualizes the most semantically-meaningful aspects of each sample in the same category. We regularize that each generated sample (the varying side of generative ZSL) should be close to at least one soul sample (the invariant side) which has the same class label with it. At the zero-shot recognition stage, we propose to use two classifiers, which are deployed in a cascade way, to achieve a coarse-to-fine result. Experiments on five popular benchmarks verify that our proposed approach can outperform state-of-the-art methods with significant improvements.",./data/pdfs/CVPR2019/Leveraging the Invariant Side of Generative Zero-Shot Learning.pdf,./data/imgs/CVPR2019/Leveraging the Invariant Side of Generative Zero-Shot Learning.png,64bd7b8297495af6971cfbb92e17f06d01bab146,304.0,"{2025: 5, 2024: 40, 2023: 55, 2022: 70, 2021: 66, 2020: 56, 2019: 10, 2018: 1}",272.0,"{2024: 15, 2023: 55, 2022: 62, 2021: 64, 2020: 59, 2019: 16, 2018: 1}"
789,ABC: A Big CAD Model Dataset for Geometric Deep Learning,CVPR,2019,"['Sebastian Koch', 'Albert Matveev', 'Zhongshi Jiang', 'Francis Williams', 'Alexey Artemov', 'Evgeny Burnaev', 'Marc Alexa', 'Denis Zorin', 'Daniele Panozzo']","We introduce ABC-Dataset, a collection of one million Computer-Aided Design (CAD) models for research of geometric deep learning methods and applications. Each model is a collection of explicitly parametrized curves and surfaces, providing ground truth for differential quantities, patch segmentation, geometric feature detection, and shape reconstruction. Sampling the parametric descriptions of surfaces and curves allows generating data in different formats and resolutions, enabling fair comparisons for a wide range of geometric learning algorithms. As a use case for our dataset, we perform a large-scale benchmark for estimation of surface normals, comparing existing data driven methods and evaluating their performance against both the ground truth and traditional normal estimation methods.",./data/pdfs/CVPR2019/ABC: A Big CAD Model Dataset for Geometric Deep Learning.pdf,./data/imgs/CVPR2019/ABC: A Big CAD Model Dataset for Geometric Deep Learning.png,b7f364584df189977a2f15999eeaf66d062a5267,365.0,"{2025: 14, 2024: 77, 2023: 81, 2022: 60, 2021: 75, 2020: 50, 2019: 8}",328.0,"{2024: 44, 2023: 81, 2022: 72, 2021: 69, 2020: 50, 2019: 12}"
1677,Libra R-CNN: Towards Balanced Learning for Object Detection,CVPR,2019,"['Jiangmiao Pang', 'Kai Chen', 'Jianping Shi', 'Huajun Feng', 'Wanli Ouyang', 'Dahua Lin']","Compared with model architectures, the training process, which is also crucial to the success of detectors, has received relatively less attention in object detection. In this work, we carefully revisit the standard training practice of detectors, and find that the detection performance is often limited by the imbalance during the training process, which generally consists in three levels - sample level, feature level, and objective level. To mitigate the adverse effects caused thereby, we propose Libra R-CNN, a simple but effective framework towards balanced learning for object detection. It integrates three novel components: IoU-balanced sampling, balanced feature pyramid, and balanced L1 loss, respectively for reducing the imbalance at sample, feature, and objective level. Benefitted from the overall balanced design, Libra R-CNN significantly improves the detection performance. Without bells and whistles, it achieves 2.5 points and 2.0 points higher Average Precision (AP) than FPN Faster R-CNN and RetinaNet respectively on MSCOCO.",./data/pdfs/CVPR2019/Libra R-CNN: Towards Balanced Learning for Object Detection.pdf,./data/imgs/CVPR2019/Libra R-CNN: Towards Balanced Learning for Object Detection.png,32a69681c103807704f71b838454c7924ceec5ce,1472.0,"{2025: 50, 2024: 231, 2023: 311, 2022: 331, 2021: 348, 2020: 157, 2019: 48}",1113.0,"{2024: 87, 2023: 244, 2022: 292, 2021: 272, 2020: 158, 2019: 59, 2018: 1}"
3219,Alleviation of Gradient Exploding in GANs: Fake Can Be Real,CVPR,2020,"['Song Tao', 'Jia Wang']","In order to alleviate the notorious mode collapse phenomenon in generative adversarial networks (GANs), we propose a novel training method of GANs in which certain fake samples are considered as real ones during the training process. This strategy can reduce the gradient value that generator receives in the region where gradient exploding happens. We show the process of an unbalanced generation and a vicious circle issue resulted from gradient exploding in practical training, which explains the instability of GANs. We also theoretically prove that gradient exploding can be alleviated by penalizing the difference between discriminator outputs and fake-as-real consideration for very close real and fake samples. Accordingly, Fake-As-Real GAN (FARGAN) is proposed with a more stable training process and a more faithful generated distribution. Experiments on different datasets verify our theoretical analysis.",./data/pdfs/CVPR2020/Alleviation of Gradient Exploding in GANs: Fake Can Be Real.pdf,./data/imgs/CVPR2020/Alleviation of Gradient Exploding in GANs: Fake Can Be Real.png,b3d6a5f75e655ee1101ce348d627e257758ebbf3,27.0,"{2024: 10, 2023: 4, 2022: 8, 2021: 5}",13.0,"{2024: 1, 2023: 2, 2022: 3, 2021: 7}"
3033,Varicolored Image De-Hazing,CVPR,2020,"['Akshay Dudhane', 'Kuldeep Marotirao Biradar', 'Prashant W. Patil', 'Praful Hambarde', 'Subrahmanyam Murala']","The quality of images captured in bad weather is often affected by chromatic casts and low visibility due to the presence of atmospheric particles. Restoration of the color balance is often ignored in most of the existing image de-hazing methods. In this paper, we propose a varicolored end-to-end image de-hazing network which restores the color balance in a given varicolored hazy image and recovers the haze-free image. The proposed network comprises of 1) Haze color correction (HCC) module and 2) Visibility improvement (VI) module. The proposed HCC module provides required attention to each color channel and generates a color balanced hazy image. While the proposed VI module processes the color balanced hazy image through novel inception attention block to recover the haze-free image. We also propose a novel approach to generate a large-scale varicolored synthetic hazy image database. An ablation study has been carried out to demonstrate the effect of different factors on the performance of the proposed network for image de-hazing. Three benchmark synthetic datasets have been used for quantitative analysis of the proposed network. Visual results on a set of real-world hazy images captured in different weather conditions demonstrate the effectiveness of the proposed approach for varicolored image de-hazing.",./data/pdfs/CVPR2020/Varicolored Image De-Hazing.pdf,./data/imgs/CVPR2020/Varicolored Image De-Hazing.png,361c1dc4036d47a7f02196b29a5de44f325f4f16,44.0,"{2025: 1, 2024: 3, 2023: 2, 2022: 20, 2021: 17, 2020: 1}",27.0,"{2024: 1, 2023: 5, 2022: 11, 2021: 10}"
2433,StructEdit: Learning Structural Shape Variations,CVPR,2020,"['Kaichun Mo', 'Paul Guerrero', 'Li Yi', 'Hao Su', 'Peter Wonka', 'Niloy J. Mitra', 'Leonidas Guibas']","Learning to encode differences in the geometry and (topological) structure of the shapes of ordinary objects is key to generating semantically plausible variations of a given shape, transferring edits from one shape to another, and for many other applications in 3D content creation. The common approach of encoding shapes as points in a high-dimensional latent feature space suggests treating shape differences as vectors in that space. Instead, we treat shape differences as primary objects in their own right and propose to encode them in their own latent space. In a setting where the shapes themselves are encoded in terms of fine-grained part hierarchies, we demonstrate that a separate encoding of shape deltas or differences provides a principled way to deal with inhomogeneities in the shape space due to different combinatorial part structures, while also allowing for compactness in the representation, as well as edit abstraction and transfer. Our approach is based on a conditional variational autoencoder for encoding and decoding shape deltas, conditioned on a source shape. We demonstrate the effectiveness and robustness of our approach in multiple shape modification and generation tasks, and provide comparison and ablation studies on the PartNet dataset, one of the largest publicly available 3D datasets.",./data/pdfs/CVPR2020/StructEdit: Learning Structural Shape Variations.pdf,./data/imgs/CVPR2020/StructEdit: Learning Structural Shape Variations.png,e46ae504bcd878d8c9aa16252baee3d3560f6da1,35.0,"{2024: 5, 2023: 5, 2022: 5, 2021: 11, 2020: 8}",39.0,"{2024: 2, 2023: 6, 2022: 9, 2021: 10, 2020: 11, 2019: 1}"
1983,STINet: Spatio-Temporal-Interactive Network for Pedestrian Detection and Trajectory Prediction,CVPR,2020,"['Zhishuai Zhang', 'Jiyang Gao', 'Junhua Mao', 'Yukai Liu', 'Dragomir Anguelov', 'Congcong Li']","Detecting pedestrians and predicting future trajectories for them are critical tasks for numerous applications, such as autonomous driving. Previous methods either treat the detection and prediction as separate tasks or simply add a trajectory regression head on top of a detector. In this work, we present a novel end-to-end two-stage network: Spatio-Temporal-Interactive Network (STINet). In addition to 3D geometry modeling of pedestrians, we model the temporal information for each of the pedestrians. To do so, our method predicts both current and past locations in the first stage, so that each pedestrian can be linked across frames and the comprehensive spatio-temporal information can be captured in the second stage. Also, we model the interaction among objects with an interaction graph, to gather the information among the neighboring objects. Comprehensive experiments on the Lyft Dataset and the recently released large-scale Waymo Open Dataset for both object detection and future trajectory prediction validate the effectiveness of the proposed method. For the Waymo Open Dataset, we achieve a bird-eyes-view (BEV) detection AP of 80.73 and trajectory prediction average displacement error (ADE) of 33.67cm for pedestrians, which establish the state-of-the-art for both tasks.",./data/pdfs/CVPR2020/STINet: Spatio-Temporal-Interactive Network for Pedestrian Detection and Trajectory Prediction.pdf,./data/imgs/CVPR2020/STINet: Spatio-Temporal-Interactive Network for Pedestrian Detection and Trajectory Prediction.png,230c8e0c7b89310078f9eeaaaa0bae326d7f6e6f,65.0,"{2025: 1, 2024: 5, 2023: 12, 2022: 18, 2021: 24, 2020: 5}",54.0,"{2024: 1, 2023: 12, 2022: 13, 2021: 17, 2020: 11}"
1828,Universal Weighting Metric Learning for Cross-Modal Matching,CVPR,2020,"['Jiwei Wei', 'Xing Xu', 'Yang Yang', 'Yanli Ji', 'Zheng Wang', 'Heng Tao Shen']","Cross-modal matching has been a highlighted research topic in both vision and language areas. Learning appropriate mining strategy to sample and weight informative pairs is crucial for the cross-modal matching performance. However, most existing metric learning methods are developed for unimodal matching, which is unsuitable for cross-modal matching on multimodal data with heterogeneous features. To address this problem, we propose a simple and interpretable universal weighting framework for cross-modal matching, which provides a tool to analyze the interpretability of various loss functions. Furthermore, we introduce a new polynomial loss under the universal weighting framework, which defines a weight function for the positive and negative informative pairs respectively. Experimental results on two image-text matching benchmarks and two video-text matching benchmarks validate the efficacy of the proposed method.",./data/pdfs/CVPR2020/Universal Weighting Metric Learning for Cross-Modal Matching.pdf,./data/imgs/CVPR2020/Universal Weighting Metric Learning for Cross-Modal Matching.png,dd860c3f8a195d06f64ecf36ef6a78397eb883bd,87.0,"{2025: 1, 2024: 13, 2023: 26, 2022: 17, 2021: 28, 2020: 2}",72.0,"{2024: 8, 2023: 20, 2022: 21, 2021: 21, 2020: 2}"
3205,C2FNAS: Coarse-to-Fine Neural Architecture Search for 3D Medical Image Segmentation,CVPR,2020,"['Qihang Yu', 'Dong Yang', 'Holger R. Roth', 'Yutong Bai', 'Yixiao Zhang', 'Alan Yuille', 'Daguang Xu']","3D convolution neural networks (CNN) have been proved very successful in parsing organs or tumours in 3D medical images, but it remains sophisticated and time-consuming to choose or design proper 3D networks given different task contexts. Recently, Neural Architecture Search (NAS) is proposed to solve this problem by searching for the best network architecture automatically. However, the inconsistency between search stage and deployment stage often exists in NAS algorithms due to memory constraints and large search space, which could become more serious when applying NAS to some memory and time-consuming tasks, such as 3D medical image segmentation. In this paper, we propose a coarse-to-fine neural architecture search (C2FNAS) to automatically search a 3D segmentation network from scratch without inconsistency on network size or input size. Specifically, we divide the search procedure into two stages: 1) the coarse stage, where we search the macro-level topology of the network, i.e. how each convolution module is connected to other modules; 2) the fine stage, where we search at micro-level for operations in each cell based on previous searched macro-level topology. The coarse-to-fine manner divides the search procedure into two consecutive stages and meanwhile resolves the inconsistency. We evaluate our method on 10 public datasets from Medical Segmentation Decalthon (MSD) challenge, and achieve state-of-the-art performance with the network searched using one dataset, which demonstrates the effectiveness and generalization of our searched models.",./data/pdfs/CVPR2020/C2FNAS: Coarse-to-Fine Neural Architecture Search for 3D Medical Image Segmentation.pdf,./data/imgs/CVPR2020/C2FNAS: Coarse-to-Fine Neural Architecture Search for 3D Medical Image Segmentation.png,eca36cc534f516db1c4ff94531ae458240141b9c,139.0,"{2024: 21, 2023: 27, 2022: 29, 2021: 49, 2020: 11}",93.0,"{2024: 3, 2023: 18, 2022: 32, 2021: 26, 2020: 12, 2019: 2}"
2511,Learning to Cartoonize Using White-Box Cartoon Representations,CVPR,2020,"['Xinrui Wang', 'Jinze Yu']","This paper presents an approach for image cartoonization. By observing the cartoon painting behavior and consulting artists, we propose to separately identify three white-box representations from images: the surface representation that contains smooth surface of cartoon images, the structure representation that refers to the sparse color-blocks and flatten global content in the celluloid style workflow, and the texture representation that reflects high-frequency texture, contours and details in cartoon images. A Generative Adversarial Network (GAN) framework is used to learn the extracted representations and to cartoonize images. The learning objectives of our method are separately based on each extracted representations, making our framework controllable and adjustable. This enables our approach to meet artists' requirements in different styles and diverse use cases. Qualitative comparisons and quantitative analyses, as well as user studies, have been conducted to validate the effectiveness of this approach, and our method outperforms previous methods in all comparisons. Finally, the ablation study demonstrates the influence of each component in our framework.",./data/pdfs/CVPR2020/Learning to Cartoonize Using White-Box Cartoon Representations.pdf,./data/imgs/CVPR2020/Learning to Cartoonize Using White-Box Cartoon Representations.png,0269b13ccbe3e3a81213e7ca2b049298d3a154f5,121.0,"{2025: 4, 2024: 27, 2023: 37, 2022: 30, 2021: 21, 2020: 2}",102.0,"{2024: 13, 2023: 29, 2022: 35, 2021: 22, 2020: 3}"
2634,Video Playback Rate Perception for Self-Supervised Spatio-Temporal Representation Learning,CVPR,2020,"['Yuan Yao', 'Chang Liu', 'Dezhao Luo', 'Yu Zhou', 'Qixiang Ye']","In self-supervised spatio-temporal representation learning, the temporal resolution and long-short term characteristics are not yet fully explored, which limits representation capabilities of learned models. In this paper, we propose a novel self-supervised method, referred to as video Playback Rate Perception (PRP), to learn spatio-temporal representation in a simple-yet-effective way. PRP roots in a dilated sampling strategy, which produces self-supervision signals about video playback rates for representation model learning. PRP is implemented with a feature encoder, a classification module, and a reconstructing decoder, to achieve spatio-temporal semantic retention in a collaborative discrimination-generation manner. The discriminative perception model follows a feature encoder to prefer perceiving low temporal resolution and long-term representation by classifying fast-forward rates. The generative perception model acts as a feature decoder to focus on comprehending high temporal resolution and short-term representation by introducing a motion-attention mechanism. PRP is applied on typical video target tasks including action recognition and video retrieval. Experiments show that PRP outperforms state-of-the-art self-supervised models with significant margins. Code is available at github.com/yuanyao366/PRP.",./data/pdfs/CVPR2020/Video Playback Rate Perception for Self-Supervised Spatio-Temporal Representation Learning.pdf,./data/imgs/CVPR2020/Video Playback Rate Perception for Self-Supervised Spatio-Temporal Representation Learning.png,15e8b0a6366e29c7093ecf844eb40d03d8004abb,177.0,"{2025: 1, 2024: 10, 2023: 33, 2022: 53, 2021: 67, 2020: 13}",155.0,"{2024: 7, 2023: 36, 2022: 46, 2021: 52, 2020: 14}"
2416,Towards Fairness in Visual Recognition: Effective Strategies for Bias Mitigation,CVPR,2020,"['Zeyu Wang', 'Klint Qinami', 'Ioannis Christos Karakozis', 'Kyle Genova', 'Prem Qu Nair', 'Kenji Hata', 'Olga Russakovsky']","Computer vision models learn to perform a task by capturing relevant statistics from training data. It has been shown that models learn spurious age, gender, and race correlations when trained for seemingly unrelated tasks like activity recognition or image captioning. Various mitigation techniques have been presented to prevent models from utilizing or learning such biases. However, there has been little systematic comparison between these techniques. We design a simple but surprisingly effective visual recognition benchmark for studying bias mitigation. Using this benchmark, we provide a thorough analysis of a wide range of techniques. We highlight the shortcomings of popular adversarial training approaches for bias mitigation, propose a simple but similarly effective alternative to the inference-time Reducing Bias Amplification method of Zhao et al., and design a domain-independent training technique that outperforms all other methods. Finally, we validate our findings on the attribute classification task in the CelebA dataset, where attribute presence is known to be correlated with the gender of people in the image, and demonstrate that the proposed technique is effective at mitigating real-world gender bias.",./data/pdfs/CVPR2020/Towards Fairness in Visual Recognition: Effective Strategies for Bias Mitigation.pdf,./data/imgs/CVPR2020/Towards Fairness in Visual Recognition: Effective Strategies for Bias Mitigation.png,17d50efa0d5ad863f5939eb586bb0a5436b0adf7,227.0,"{2025: 4, 2024: 60, 2023: 53, 2022: 50, 2021: 48, 2020: 10}",275.0,"{2024: 41, 2023: 84, 2022: 83, 2021: 54, 2020: 13}"
2276,PointASNL: Robust Point Clouds Processing Using Nonlocal Neural Networks With Adaptive Sampling,CVPR,2020,"['Yan Xu', 'Chaoda Zheng', 'Zhen Li', 'Sheng Wang', 'Shuguang Cui']","Raw point clouds data inevitably contains outliers or noise through acquisition from 3D sensors or reconstruction algorithms. In this paper, we present a novel end-to-end network for robust point clouds processing, named PointASNL, which can deal with point clouds with noise effectively. The key component in our approach is the adaptive sampling (AS) module. It first re-weights the neighbors around the initial sampled points from farthest point sampling (FPS), and then adaptively adjusts the sampled points beyond the entire point cloud. Our AS module can not only benefit the feature learning of point clouds, but also ease the biased effect of outliers. To further capture the neighbor and long-range dependencies of the sampled point, we proposed a local-nonlocal (L-NL) module inspired by the nonlocal operation. Such L-NL module enables the learning process insensitive to noise. Extensive experiments verify the robustness and superiority of our approach in point clouds processing tasks regardless of synthesis data, indoor data, and outdoor data with or without noise. Specifically, PointASNL achieves state-of-the-art robust performance for classification and segmentation tasks on all datasets, and significantly outperforms previous methods on real-world outdoor SemanticKITTI dataset with considerate noise.",./data/pdfs/CVPR2020/PointASNL: Robust Point Clouds Processing Using Nonlocal Neural Networks With Adaptive Sampling.pdf,./data/imgs/CVPR2020/PointASNL: Robust Point Clouds Processing Using Nonlocal Neural Networks With Adaptive Sampling.png,40b36071bd553b9fda2ea26c6d4e057f37c0e7e7,556.0,"{2025: 22, 2024: 126, 2023: 150, 2022: 118, 2021: 119, 2020: 19, 2019: 2}",439.0,"{2024: 46, 2023: 126, 2022: 135, 2021: 101, 2020: 29, 2019: 2}"
11257,Dynamic Probabilistic Graph Convolution for Facial Action Unit Intensity Estimation,CVPR,2021,"['Tengfei Song', 'Zijun Cui', 'Yuru Wang', 'Wenming Zheng', 'Qiang Ji']","Deep learning methods have been widely applied to automatic facial action unit (AU) intensity estimation and achieved the state-of-the-art performance. These methods, however, are mostly appearance-based and fail to exploit the underlying structural information among AUs. In this paper, we propose a novel dynamic probabilistic graph convolution (DPG) model to simultaneously exploit AU appearances, AU dynamics, and their semantic structural dependencies for AU intensity estimation. Firstly, we propose to use Bayesian Network to capture the inherent dependencies among AUs. Secondly, we introduce probabilistic graph convolution that allows to perform graph convolution on the distribution of Bayesian Network structure to extract AU structural features. Finally, we introduce a dynamic deep model based on LSTM to simultaneously combine AU appearance features, AU dynamic features, and AU structural features for AU intensity estimation. In experiments, our method achieves comparable and even better performance with the state-of-the-art methods on two benchmark facial AU intensity estimation databases, i.e., FERA 2015 and DISFA.",./data/pdfs/CVPR2021/Dynamic Probabilistic Graph Convolution for Facial Action Unit Intensity Estimation.pdf,./data/imgs/CVPR2021/Dynamic Probabilistic Graph Convolution for Facial Action Unit Intensity Estimation.png,c22060424a75f0e8888fbe2af80f3740b61d286a,10.0,"{2024: 2, 2023: 3, 2022: 3, 2021: 1}",13.0,"{2024: 2, 2023: 5, 2022: 5, 2021: 1}"
12161,How Privacy-Preserving Are Line Clouds? Recovering Scene Details From 3D Lines,CVPR,2021,"['Kunal Chelani', 'Fredrik Kahl', 'Torsten Sattler']","Visual localization is the problem of estimating the camera pose of a given image with respect to a known scene. Visual localization algorithms are a fundamental building block in advanced computer vision applications, including Mixed and Virtual Reality systems. Many algorithms used in practice represent the scene through a Structure-from-Motion (SfM) point cloud and use 2D-3D matches between a query image and the 3D points for camera pose estimation. As recently shown, image details can be accurately recovered from SfM point clouds by translating renderings of the sparse point clouds to images. To address the resulting potential privacy risks for user-generated content, it was recently proposed to lift point clouds to line clouds by replacing 3D points by randomly oriented 3D lines passing through these points. The resulting representation is unintelligible to humans and effectively prevents point cloud-to-image translation. This paper shows that a significant amount of information about the 3D scene geometry is preserved in these line clouds, allowing us to (approximately) recover the 3D point positions and thus to (approximately) recover image content. Our approach is based on the observation that the closest points between lines can yield a good approximation to the original 3D points. Code is available at https://github.com/kunalchelani/Line2Point.",./data/pdfs/CVPR2021/How Privacy-Preserving Are Line Clouds? Recovering Scene Details From 3D Lines.pdf,./data/imgs/CVPR2021/How Privacy-Preserving Are Line Clouds? Recovering Scene Details From 3D Lines.png,4de17fe5c301e90062c8edad26c91ab7867913df,19.0,"{2025: 1, 2024: 5, 2023: 8, 2022: 4}",17.0,"{2024: 1, 2023: 11, 2022: 3, 2021: 2}"
11439,Improved Image Matting via Real-Time User Clicks and Uncertainty Estimation,CVPR,2021,"['Tianyi Wei', 'Dongdong Chen', 'Wenbo Zhou', 'Jing Liao', 'Hanqing Zhao', 'Weiming Zhang', 'Nenghai Yu']","Image matting is a fundamental and challenging problem in computer vision and graphics. Most existing matting methods leverage a user-supplied trimap as an auxiliary input to produce good alpha matte. However, obtaining high-quality trimap itself is arduous, thus restricting the application of these methods. Recently, some trimap-free methods have emerged, however, the matting quality is still far behind the trimap-based methods. The main reason is that, without the trimap guidance in some cases, the target network is ambiguous about which is the foreground target. In fact, choosing the foreground is a subjective procedure and depends on the user's intention. To this end, this paper proposes an improved deep image matting framework which is trimap-free and only needs several user click interactions to eliminate the ambiguity. Moreover, we introduce a new uncertainty estimation module that can predict which parts need polishing and a following local refinement module. Based on the computation budget, users can choose how many local parts to improve with the uncertainty guidance. Quantitative and qualitative results show that our method performs better than existing trimap-free methods and comparably to state-of-the-art trimap-based methods with minimal user effort.",./data/pdfs/CVPR2021/Improved Image Matting via Real-Time User Clicks and Uncertainty Estimation.pdf,./data/imgs/CVPR2021/Improved Image Matting via Real-Time User Clicks and Uncertainty Estimation.png,aee8a56202088c2a51dc6f858c1bdd07c405e000,23.0,"{2025: 2, 2024: 6, 2023: 11, 2022: 3, 2021: 1}",24.0,"{2024: 4, 2023: 11, 2022: 6, 2021: 3}"
11330,Consistent Instance False Positive Improves Fairness in Face Recognition,CVPR,2021,"['Xingkun Xu', 'Yuge Huang', 'Pengcheng Shen', 'Shaoxin Li', 'Jilin Li', 'Feiyue Huang', 'Yong Li', 'Zhen Cui']","Demographic bias is a significant challenge in practical face recognition systems. Existing methods heavily rely on accurate demographic annotations. However, such annotations are usually unavailable in real scenarios. Moreover, these methods are typically designed for a specific demographic group and are not general enough. In this paper, we propose a false positive rate penalty loss, which mitigates face recognition bias by increasing the consistency of instance False Positive Rate (FPR). Specifically, we first define the instance FPR as the ratio between the number of the non-target similarities above a unified threshold and the total number of the non-target similarities. The unified threshold is estimated for a given total FPR. Then, an additional penalty term, which is in proportion to the ratio of instance FPR overall FPR, is introduced into the denominator of the softmax-based loss. The larger the instance FPR, the larger the penalty. By such unequal penalties, the instance FPRs are supposed to be consistent. Compared with the previous debiasing methods, our method requires no demographic annotations. Thus, it can mitigate the bias among demographic groups divided by various attributes, and these attributes are not needed to be previously predefined during training. Extensive experimental results on popular benchmarks demonstrate the superiority of our method over state-of-the-art competitors. Code and pre-trained models are available at https://github.com/xkx0430/FairnessFR.",./data/pdfs/CVPR2021/Consistent Instance False Positive Improves Fairness in Face Recognition.pdf,./data/imgs/CVPR2021/Consistent Instance False Positive Improves Fairness in Face Recognition.png,37cab88678e3e0139d6110f3fcb51145e61e3c90,37.0,"{2024: 8, 2023: 14, 2022: 14, 2021: 1}",40.0,"{2024: 3, 2023: 12, 2022: 19, 2021: 5, 2020: 1}"
11441,Radar-Camera Pixel Depth Association for Depth Completion,CVPR,2021,"['Y. F. Long', 'Daniel Morris', 'Xiaoming Liu', 'Marcos Castro', 'Punarjay Chakravarty', 'P. J. Narayanan']","While radar and video data can be readily fused at the detection level, fusing them at the pixel level is potentially more beneficial. This is also more challenging in part due to the sparsity of radar, but also because automotive radar beams are much wider than a typical pixel combined with a large baseline between camera and radar, which results in poor association between radar pixels and color pixel. A consequence is that depth completion methods designed for LiDAR and video fare poorly for radar and video. Here we propose a radar-to-pixel association stage which learns a mapping from radar returns to pixels. This mapping also serves to densify radar returns. Using this as a first stage, followed by a more traditional depth completion method, we are able to achieve image-guided depth completion with radar and video. We demonstrate performance superior to camera and radar alone on the nuScenes dataset. Our source code is available at https://github.com/longyunf/rc-pda.",./data/pdfs/CVPR2021/Radar-Camera Pixel Depth Association for Depth Completion.pdf,./data/imgs/CVPR2021/Radar-Camera Pixel Depth Association for Depth Completion.png,e4aef4816ea7de92889dcd8e8af88c1d7058dfbe,47.0,"{2025: 1, 2024: 15, 2023: 20, 2022: 8, 2021: 3}",49.0,"{2024: 10, 2023: 19, 2022: 15, 2021: 5}"
12278,Navigating the GAN Parameter Space for Semantic Image Editing,CVPR,2021,"['Anton Cherepkov', 'Andrey Voynov', 'Artem Babenko']","Generative Adversarial Networks (GANs) are currently an indispensable tool for visual editing, being a standard component of image-to-image translation and image restoration pipelines. Furthermore, GANs are especially advantageous for controllable generation since their latent spaces contain a wide range of interpretable directions, well suited for semantic editing operations. By gradually changing latent codes along these directions, one can produce impressive visual effects, unattainable without GANs.In this paper, we significantly expand the range of visual effects achievable with the state-of-the-art models, like StyleGAN2. In contrast to existing works, which mostly operate by latent codes, we discover interpretable directions in the space of the generator parameters. By several simple methods, we explore this space and demonstrate that it also contains a plethora of interpretable directions, which are an excellent source of non-trivial semantic manipulations. The discovered manipulations cannot be achieved by transforming the latent codes and can be used to edit both synthetic and real images. We release our code and models and hope they will serve as a handy tool for further efforts on GAN-based image editing.",./data/pdfs/CVPR2021/Navigating the GAN Parameter Space for Semantic Image Editing.pdf,./data/imgs/CVPR2021/Navigating the GAN Parameter Space for Semantic Image Editing.png,7a14cf615f30feb0a650a7ffa1c0a3d650736899,49.0,"{2025: 1, 2024: 9, 2023: 17, 2022: 16, 2021: 5}",58.0,"{2024: 5, 2023: 22, 2022: 24, 2021: 7}"
11610,Diversifying Sample Generation for Accurate Data-Free Quantization,CVPR,2021,"['Xiangguo Zhang', 'Haotong Qin', 'Yifu Ding', 'Ruihao Gong', 'Qinghua Yan', 'Renshuai Tao', 'Yuhang Li', 'Fengwei Yu', 'Xianglong Liu']","Quantization has emerged as one of the most prevalent approaches to compress and accelerate neural networks. Recently, data-free quantization has been widely studied as a practical and promising solution. It synthesizes data for calibrating the quantized model according to the batch normalization (BN) statistics of FP32 ones and significantly relieves the heavy dependency on real training data in traditional quantization methods. Unfortunately, we find that in practice, the synthetic data identically constrained by BN statistics suffers serious homogenization at both distribution level and sample level and further causes a significant performance drop of the quantized model. We propose Diverse Sample Generation (DSG) scheme to mitigate the adverse effects caused by homogenization. Specifically, we slack the alignment of feature statistics in the BN layer to relax the constraint at the distribution level and design a layerwise enhancement to reinforce specific layers for different data samples. Our DSG scheme is versatile and even able to be applied to the state-of-the-art post-training quantization method like AdaRound. We evaluate the DSG scheme on the large-scale image classification task and consistently obtain significant improvements over various network architectures and quantization methods, especially when quantized to lower bits (e.g., up to 22% improvement on W4A4). Moreover, benefiting from the enhanced diversity, models calibrated with synthetic data perform close to those calibrated with real data and even outperform them on W4A4.",./data/pdfs/CVPR2021/Diversifying Sample Generation for Accurate Data-Free Quantization.pdf,./data/imgs/CVPR2021/Diversifying Sample Generation for Accurate Data-Free Quantization.png,99eeeca965c4384fa242d706b7db790111e0f233,65.0,"{2025: 1, 2024: 9, 2023: 27, 2022: 16, 2021: 10}",68.0,"{2024: 5, 2023: 25, 2022: 26, 2021: 10, 2020: 2}"
11903,TextOCR: Towards Large-Scale End-to-End Reasoning for Arbitrary-Shaped Scene Text,CVPR,2021,"['Amanpreet Singh', 'Guan Pang', 'Mandy Toh', 'Jing Huang', 'Wojciech Galuba', 'Tal Hassner']","A crucial component for the scene text based reasoning required for TextVQA and TextCaps datasets involve detecting and recognizing text present in the images using an optical character recognition (OCR) system. The current systems are crippled by the unavailability of ground truth text annotations for these datasets as well as lack of scene text detection and recognition datasets on real images disallowing the progress in the field of OCR and evaluation of scene text based reasoning in isolation from OCR systems. In this work, we propose TextOCR, an arbitrary-shaped scene text detection and recognition with 900k annotated words collected on real images from TextVQA dataset. We show that current state-of-the-art text-recognition (OCR) models fail to perform well on TextOCR and that training on TextOCR helps achieve state-of-the-art performance on multiple other OCR datasets as well. We use a TextOCR trained OCR model to create PixelM4C model which can do scene text based reasoning on an image in an end-to-end fashion, allowing us to revisit several design choices to achieve new state-of-the-art performance on TextVQA dataset.",./data/pdfs/CVPR2021/TextOCR: Towards Large-Scale End-to-End Reasoning for Arbitrary-Shaped Scene Text.pdf,./data/imgs/CVPR2021/TextOCR: Towards Large-Scale End-to-End Reasoning for Arbitrary-Shaped Scene Text.png,f09826d6ce8b11cf425793853a708b6d107bb79d,97.0,"{2025: 5, 2024: 33, 2023: 33, 2022: 18, 2021: 6, 2020: 1}",97.0,"{2024: 22, 2023: 37, 2022: 30, 2021: 7, 2020: 1}"
11361,PLOP: Learning Without Forgetting for Continual Semantic Segmentation,CVPR,2021,"['Arthur Douillard', 'Yifu Chen', 'Arnaud Dapogny', 'Matthieu Cord']","Deep learning approaches are nowadays ubiquitously used to tackle computer vision tasks such as semantic segmentation, requiring large datasets and substantial computational power. Continual learning for semantic segmentation (CSS) is an emerging trend that consists in updating an old model by sequentially adding new classes. However, continual learning methods are usually prone to catastrophic forgetting. This issue is further aggravated in CSS where, at each step, old classes from previous iterations are collapsed into the background. In this paper, we propose Local POD, a multi-scale pooling distillation scheme that preserves long- and short-range spatial relationships at feature level. Furthermore, we design an entropy-based pseudo-labelling of the background w.r.t. classes predicted by the old model to deal with background shift and avoid catastrophic forgetting of the old classes. Our approach, called PLOP, significantly outperforms state-of-the-art methods in existing CSS scenarios, as well as in newly proposed challenging benchmarks.",./data/pdfs/CVPR2021/PLOP: Learning Without Forgetting for Continual Semantic Segmentation.pdf,./data/imgs/CVPR2021/PLOP: Learning Without Forgetting for Continual Semantic Segmentation.png,389ee0934b5da420e51d1c77a12834c24ceec060,166.0,"{2025: 5, 2024: 52, 2023: 62, 2022: 35, 2021: 11, 2020: 1}",170.0,"{2024: 21, 2023: 77, 2022: 46, 2021: 23, 2020: 3}"
12120,Transformer Interpretability Beyond Attention Visualization,CVPR,2021,"['Hila Chefer', 'Shir Gur', 'Lior Wolf']","Self-attention techniques, and specifically Transformers, are dominating the field of text processing and are becoming increasingly popular in computer vision classification tasks. In order to visualize the parts of the image that led to a certain classification, existing methods either rely on the obtained attention maps or employ heuristic propagation along the attention graph. In this work, we propose a novel way to compute relevancy for Transformer networks. The method assigns local relevance based on the Deep Taylor Decomposition principle and then propagates these relevancy scores through the layers. This propagation involves attention layers and skip connections, which challenge existing methods. Our solution is based on a specific formulation that is shown to maintain the total relevancy across layers. We benchmark our method on very recent visual Transformer networks, as well as on a text classification problem, and demonstrate a clear advantage over the existing explainability methods. Our code is available at: https://github.com/hila-chefer/Transformer-Explainability.",./data/pdfs/CVPR2021/Transformer Interpretability Beyond Attention Visualization.pdf,./data/imgs/CVPR2021/Transformer Interpretability Beyond Attention Visualization.png,0acd7ff5817d29839b40197f7a4b600b7fba24e4,480.0,"{2025: 27, 2024: 153, 2023: 174, 2022: 86, 2021: 31, 2020: 2}",450.0,"{2024: 73, 2023: 195, 2022: 133, 2021: 46, 2020: 3}"
10963,UNIST: Unpaired Neural Implicit Shape Translation Network,CVPR,2022,"['Qimin Chen', 'Johannes Merz', 'Aditya Sanghi', 'Hooman Shayani', 'Ali Mahdavi‐Amiri', 'Hao Zhang']","We introduce UNIST, the first deep neural implicit model for general-purpose, unpaired shape-to-shape translation, in both 2D and 3D domains. Our model is built on autoencoding implicit fields, rather than point clouds which represents the state of the art. Furthermore, our translation network is trained to perform the task over a latent grid representation which combines the merits of both latent-space processing and position awareness, to not only enable drastic shape transforms but also well preserve spatial features and fine local details for natural shape translations. With the same network architecture and only dictated by the input domain pairs, our model can learn both style-preserving content alteration and content-preserving style transfer. We demonstrate the generality and quality of the translation results, and compare them to well-known baselines. Code is available at https://qiminchen.github.io/unist/.",./data/pdfs/CVPR2022/UNIST: Unpaired Neural Implicit Shape Translation Network.pdf,./data/imgs/CVPR2022/UNIST: Unpaired Neural Implicit Shape Translation Network.png,a08c992c99d3253fbf99265eee4b0287e84a173b,7.0,"{2024: 2, 2023: 2, 2022: 3}",4.0,"{2023: 2, 2022: 2}"
9243,SOMSI: Spherical Novel View Synthesis With Soft Occlusion Multi-Sphere Images,CVPR,2022,"['Tewodros Habtegebrial', 'Christiano Gava', 'Marcel Rogge', 'Didier Stricker', 'Varun Jampani']","Spherical novel view synthesis (SNVS) is the task of estimating 360 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">○</sup> views at dynamic novel views given a set of 360 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">○</sup> input views. Prior arts learn multi-sphere image (MSI) representations that enable fast rendering times but are only limited to modelling low-dimensional color values. Modelling high-dimensional appearance features in MSI can result in better view synthesis, but it is not feasible to represent high-dimensional features in a large number (> 64) of MSI spheres. We propose a novel MSI representation called Soft Occlusion MSI (SOMSI) that enables modelling high-dimensional appearance features in MSI while retaining the fast rendering times of a standard MSI. Our key insight is to model appearance features in a smaller set (e.g. 3) of occlusion levels instead of larger number of MSI levels. Experiments on both synthetic and real-world scenes demonstrate that using SOMSI can provide a good balance between accuracy and run-time. SOMSI can produce considerably better results compared to MSI based MODS [1], while having similar fast rendering time. SOMSI view synthesis quality is on-par with state-of-the-art NeRF [24] like model while being 2 orders of magnitude faster. For code, additional results and data, please visit https://tedyhabtegebrial.github.io/somsi.",./data/pdfs/CVPR2022/SOMSI: Spherical Novel View Synthesis With Soft Occlusion Multi-Sphere Images.pdf,./data/imgs/CVPR2022/SOMSI: Spherical Novel View Synthesis With Soft Occlusion Multi-Sphere Images.png,72b47fe215e1a623cd35e345f66be9845c9e783e,5.0,"{2024: 2, 2023: 3}",7.0,"{2024: 1, 2023: 5, 2022: 1}"
11052,Arch-Graph: Acyclic Architecture Relation Predictor for Task-Transferable Neural Architecture Search,CVPR,2022,"['Minbin Huang', 'Zhijian Huang', 'Changlin Li', 'Xin Chen', 'Hang Xu', 'Zhenguo Li', 'Xiaodan Liang']","Neural Architecture Search (NAS) aims to find efficient models for multiple tasks. Beyond seeking solutions for a single task, there are surging interests in transferring network design knowledge across multiple tasks. In this line of research, effectively modeling task correlations is vital yet highly neglected. Therefore, we propose Arch-Graph, a transferable NAS method that predicts task-specific optimal architectures with respect to given task embeddings. It leverages correlations across multiple tasks by using their embeddings as a part of the predictor's input for fast adaptation. We also formulate NAS as an architecture relation graph prediction problem, with the relational graph constructed by treating candidate architectures as nodes and their pairwise relations as edges. To enforce some basic properties such as acyclicity in the relational graph, we add additional constraints to the optimization process, converting NAS into the problem of finding a Maximal Weighted Acyclic Subgraph (MWAS). Our algorithm then strives to eliminate cycles and only establish edges in the graph if the rank results can be trusted. Through MWAS, Arch-Graph can effectively rank candidate models for each task with only a small budget to finetune the predictor. With extensive experiments on TransNAS-Bench-101, we show Arch-Graph's transferability and high sample efficiency across numerous tasks, beating many NAS methods designed for both single-task and multi-task search. It is able to find top 0.16% and 0.29% architectures on average on two search spaces under the budget of only 50 models. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> Code: https://github.com/Centaurus982034/Arch-Graph",./data/pdfs/CVPR2022/Arch-Graph: Acyclic Architecture Relation Predictor for Task-Transferable Neural Architecture Search.pdf,./data/imgs/CVPR2022/Arch-Graph: Acyclic Architecture Relation Predictor for Task-Transferable Neural Architecture Search.png,c930a4e013ce738919663a31e3cd2554de4083c5,17.0,"{2025: 1, 2024: 6, 2023: 5, 2022: 4}",14.0,"{2024: 2, 2023: 6, 2022: 6}"
10176,Context-Aware Video Reconstruction for Rolling Shutter Cameras,CVPR,2022,"['Bin Fan', 'Yuchao Dai', 'Zhiyuan Zhang', 'Qi Liu', 'Mingyi He']","With the ubiquity of rolling shutter (RS) cameras, it is becoming increasingly attractive to recover the latent global shutter (GS) video from two consecutive RS frames, which also places a higher demand on realism. Existing solutions, using deep neural networks or optimization, achieve promising performance. However, these methods generate intermediate GS frames through image warping based on the RS model, which inevitably result in black holes and noticeable motion artifacts. In this paper, we alleviate these issues by proposing a context-aware GS video reconstruction architecture. It facilitates the advantages such as occlusion reasoning, motion compensation, and temporal abstraction. Specifically, we first estimate the bilateral motion field so that the pixels of the two RS frames are warped to a common GS frame accordingly. Then, a refinement scheme is proposed to guide the GS frame synthesis along with bilateral occlusion masks to produce high-fidelity GS video frames at arbitrary times. Furthermore, we derive an approximated bilateral motion field model, which can serve as an alternative to provide a simple but effective GS frame initialization for related tasks. Experiments on synthetic and real data show that our approach achieves superior performance over state-of-the-art methods in terms of objective metrics and subjective visual quality. Code is available at https://github.com/GitCVfb/CVR.",./data/pdfs/CVPR2022/Context-Aware Video Reconstruction for Rolling Shutter Cameras.pdf,./data/imgs/CVPR2022/Context-Aware Video Reconstruction for Rolling Shutter Cameras.png,cee8e5a56366d024574a8b52cbbb836e115e366f,18.0,"{2025: 1, 2024: 11, 2023: 6}",17.0,"{2024: 5, 2023: 12}"
9500,M5Product: Self-Harmonized Contrastive Learning for E-Commercial Multi-Modal Pretraining,CVPR,2022,"['Xiao Dong', 'Xunlin Zhan', 'Yangxin Wu', 'Yunchao Wei', 'Michael Kampffmeyer', 'Xiao-Yong Wei', 'Minlong Lu', 'Yaowei Wang', 'Xiaodan Liang']","Despite the potential of multi-modal pre-training to learn highly discriminative feature representations from complementary data modalities, current progress is being slowed by the lack of large-scale modality-diverse datasets. By leveraging the natural suitability of E-commerce, where different modalities capture complementary semantic information, we contribute a large-scale multi-modal pretraining dataset M5Product. The dataset comprises 5 modalities (image, text, table, video, and audio), covers over 6,000 categories and 5,000 attributes, and is 500× larger than the largest publicly available dataset with a similar number of modalities. Furthermore, M5Product contains incomplete modality pairs and noise while also having a long-tailed distribution, resembling most real-world problems. We further propose Self-harmonized ContrAstive LEarning (SCALE), a novel pretraining framework that integrates the different modalities into a unified model through an adaptive feature fusion mechanism, where the importance of each modality is learned directly from the modality embeddings and impacts the inter-modality contrastive learning and masked tasks within a multi-modal transformer model. We evaluate the current multi-modal pre-training state-of-the-art approaches and benchmark their ability to learn from unlabeled data when faced with the large number of modalities in the M5Product dataset. We conduct extensive experiments on four downstream tasks and demonstrate the superiority of our SCALE model, providing insights into the importance of dataset scale and diversity. Dataset and codes are available at <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> https://xiaodongsuper.github.io/M5Product_dataset/.",./data/pdfs/CVPR2022/M5Product: Self-Harmonized Contrastive Learning for E-Commercial Multi-Modal Pretraining.pdf,./data/imgs/CVPR2022/M5Product: Self-Harmonized Contrastive Learning for E-Commercial Multi-Modal Pretraining.png,59ba12da3b9ee2d9ee829145cf6e061d772805ac,20.0,"{2025: 2, 2024: 7, 2023: 10, 2022: 1}",23.0,"{2024: 3, 2023: 15, 2022: 5}"
10953,FocusCut: Diving Into a Focus View in Interactive Segmentation,CVPR,2022,"['Lin Zheng', 'Zheng-Peng Duan', 'Zhao Zhang', 'Chunle Guo', 'Ming‐Ming Cheng']","Interactive image segmentation is an essential tool in pixel-level annotation and image editing. To obtain a high-precision binary segmentation mask, users tend to add interaction clicks around the object details, such as edges and holes, for efficient refinement. Current methods regard these repair clicks as the guidance to jointly determine the global prediction. However, the global view makes the model lose focus from later clicks, and is not in line with user intentions. In this paper, we dive into the view of clicks' eyes to endow them with the decisive role in object details again. To verify the necessity of focus view, we design a simple yet effective pipeline, named FocusCut, which integrates the functions of object segmentation and local refinement. After obtaining the global prediction, it crops click-centered patches from the original image with adaptive scopes to refine the local predictions progressively. Without user perception and parameters increase, our method has achieved state-of-the-art results. Extensive experiments and visualized results demonstrate that FocusCut makes hyper-fine segmentation possible for interactive image segmentation.",./data/pdfs/CVPR2022/FocusCut: Diving Into a Focus View in Interactive Segmentation.pdf,./data/imgs/CVPR2022/FocusCut: Diving Into a Focus View in Interactive Segmentation.png,39274c406be06b2bf77bb0bc82972d55772092cd,44.0,"{2025: 2, 2024: 21, 2023: 17, 2022: 4}",31.0,"{2024: 6, 2023: 20, 2022: 4, 2021: 1}"
9757,Coupled Iterative Refinement for 6D Multi-Object Pose Estimation,CVPR,2022,"['Lahav Lipson', 'Zachary Teed', 'Ankit Goyal', 'Jia Deng']","We address the task of 6D multi-object pose: given a set of known 3D objects and an RGB or RGB-D input image, we detect and estimate the 6D pose of each object. We propose a new approach to 6D object pose estimation which consists of an end-to-end differentiable architecture that makes use of geometric knowledge. Our approach iteratively refines both pose and correspondence in a tightly coupled manner, allowing us to dynamically remove outliers to improve accuracy. We use a novel differentiable layer to perform pose refinement by solving an optimization problem we refer to as Bidirectional Depth-Augmented Perspective-N-Point (BD-PnP). Our method achieves state-of-the-art accuracy on standard 6D Object Pose benchmarks. Code is available at https://github.com/princeton-vl/Coupled-Iterative-Refinement.",./data/pdfs/CVPR2022/Coupled Iterative Refinement for 6D Multi-Object Pose Estimation.pdf,./data/imgs/CVPR2022/Coupled Iterative Refinement for 6D Multi-Object Pose Estimation.png,cf6a902bb51fc8c75e29db3cb91b04f12782e066,47.0,"{2025: 3, 2024: 22, 2023: 20, 2022: 2}",40.0,"{2024: 11, 2023: 25, 2022: 4}"
9908,BppAttack: Stealthy and Efficient Trojan Attacks Against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning,CVPR,2022,"['Zhenting Wang', 'Juan Zhai', 'Shiqing Ma']","Deep neural networks are vulnerable to Trojan attacks. Existing attacks use visible patterns (e.g., a patch or image transformations) as triggers, which are vulnerable to human inspection. In this paper, we propose stealthy and efficient Trojan attacks, BppAttack. Based on existing biology literature on human visual systems, we propose to use image quantization and dithering as the Trojan trigger, making imperceptible changes. It is a stealthy and efficient attack without training auxiliary models. Due to the small changes made to images, it is hard to inject such triggers during training. To alleviate this problem, we propose a contrastive learning based approach that leverages adversarial attacks to generate negative sample pairs so that the learned trigger is precise and accurate. The proposed method achieves high attack success rates on four benchmark datasets, including MNIST, CIFAR-10, GTSRB, and CelebA. It also effectively bypasses existing Trojan defenses and human inspection. Our code can be found in https://github.com/RU-System-Software-and-Security/BppAttack.",./data/pdfs/CVPR2022/BppAttack: Stealthy and Efficient Trojan Attacks Against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning.pdf,./data/imgs/CVPR2022/BppAttack: Stealthy and Efficient Trojan Attacks Against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning.png,052d4787586a7e918638bd82c3034089648d62e7,49.0,"{2025: 3, 2024: 26, 2023: 13, 2022: 3}",60.0,"{2024: 17, 2023: 32, 2022: 11}"
11037,Uni-Perceiver: Pre-Training Unified Architecture for Generic Perception for Zero-Shot and Few-Shot Tasks,CVPR,2022,"['Xizhou Zhu', 'Jinguo Zhu', 'Hao Li', 'Xiaoshi Wu', 'Hongsheng Li', 'Xiaohua Wang', 'Jifeng Dai']","Biological intelligence systems of animals perceive the world by integrating information in different modalities and processing simultaneously for various tasks. In contrast, current machine learning research follows a task-specific paradigm, leading to inefficient collaboration between tasks and high marginal costs of developing perception models for new tasks. In this paper, we present a generic perception architecture named Uni-Perceiver, which processes a variety of modalities and tasks with unified modeling and shared parameters. Specifically, Uni-Perceiver encodes different task inputs and targets from arbitrary modalities into a unified representation space with a modality-agnostic Transformer encoder and lightweight modality-specific tokenizers. Different perception tasks are modeled as the same formulation, that is, finding the maximum likelihood target for each input through the similarity of their representations. The model is pre-trained on several uni-modal and multi-modal tasks, and evaluated on a variety of downstream tasks, including novel tasks that did not appear in the pre-training stage. Results show that our pre-trained model without any tuning can achieve reasonable performance even on novel tasks. The performance can be improved to a level close to state-of-the-art methods by conducting prompt tuning on 1% of downstream task data. Full-data fine-tuning further delivers results on par with or better than state-of-the-art results. Code and pre-trained weights shall be released.",./data/pdfs/CVPR2022/Uni-Perceiver: Pre-Training Unified Architecture for Generic Perception for Zero-Shot and Few-Shot Tasks.pdf,./data/imgs/CVPR2022/Uni-Perceiver: Pre-Training Unified Architecture for Generic Perception for Zero-Shot and Few-Shot Tasks.png,91dc75f94da13452a54ad5c03fab2c5fda87e9ba,50.0,"{2025: 3, 2024: 20, 2023: 23, 2022: 3}",101.0,"{2024: 17, 2023: 58, 2022: 26}"
11138,CRIS: CLIP-Driven Referring Image Segmentation,CVPR,2022,"['Zhaoqing Wang', 'Yu Lu', 'Qiang Li', 'Xunqiang Tao', 'Yandong Guo', 'Mingming Gong', 'Tongliang Liu']","Referring image segmentation aims to segment a referent via a natural linguistic expression. Due to the distinct data properties between text and image, it is challenging for a network to well align text and pixel-level features. Existing approaches use pretrained models to facilitate learning, yet separately transfer the language/vision knowledge from pretrained models, ignoring the multi-modal corresponding information. Inspired by the recent advance in Contrastive Language-Image Pretraining (CLIP), in this paper, we propose an end-to-end CLIP-Driven Referring Image Segmen-tation framework (CRIS). To transfer the multi-modal knowledge effectively, CRIS resorts to vision-language decoding and contrastive learning for achieving the text-to-pixel alignment. More specifically, we design a vision-language decoder to propagate fine-grained semantic information from textual representations to each pixel-level activation, which promotes consistency between the two modalities. In addition, we present text-to-pixel contrastive learning to explicitly enforce the text feature similar to the related pixel-level features and dissimilar to the irrelevances. The experimental results on three benchmark datasets demonstrate that our proposed framework significantly outperforms the state-of-the-art performance without any post-processing.",./data/pdfs/CVPR2022/CRIS: CLIP-Driven Referring Image Segmentation.pdf,./data/imgs/CVPR2022/CRIS: CLIP-Driven Referring Image Segmentation.png,76a2b197b5427ffd1d3470c6d3ea026588eb5d0a,196.0,"{2025: 16, 2024: 106, 2023: 64, 2022: 6}",226.0,"{2024: 57, 2023: 148, 2022: 19, 2021: 2}"
5185,Light Source Separation and Intrinsic Image Decomposition Under AC Illumination,CVPR,2023,"['Yusaku Yoshida', 'Ryo Kawahara', 'Takahiro Okabe']","Artificial light sources are often powered by an electric grid, and then their intensities rapidly oscillate in response to the grid's alternating current (AC). Interestingly, the flickers of scene radiance values due to AC illumination are useful for extracting rich information on a scene of interest. In this paper, we show that the flickers due to AC illumination is useful for intrinsic image decomposition (IID). Our proposed method conducts the light source separation (LSS) followed by the IID under AC illumination. In particular, we reveal the ambiguity in the blind LSS via matrix factorization and the ambiguity in the IID assuming the diffuse reflection model, and then show why and how those ambiguities can be resolved via a physics-based approach. We experimentally confirmed that our method can recover the colors of the light sources, the diffuse reflectance values, and the diffuse and specular intensities (shadings) under each of the light sources, and that the IID under AC illumination is effective for application to auto white balancing.",./data/pdfs/CVPR2023/Light Source Separation and Intrinsic Image Decomposition Under AC Illumination.pdf,./data/imgs/CVPR2023/Light Source Separation and Intrinsic Image Decomposition Under AC Illumination.png,ff0ed8c833d9e172003878a4fd8f91ac1dde4da4,0.0,{},0.0,{}
3454,Federated Learning With Data-Agnostic Distribution Fusion,CVPR,2023,"['Jian-Hui Duan', 'Wenzhong Li', 'Derun Zou', 'Ruichen Li', 'Sanglu Lu']","Federated learning has emerged as a promising distributed machine learning paradigm to preserve data privacy. One of the fundamental challenges of federated learning is that data samples across clients are usually not independent and identically distributed (non-IID), leading to slow convergence and severe performance drop of the aggregated global model. To facilitate model aggregation on non-IID data, it is desirable to infer the unknown global distributions without violating privacy protection policy. In this paper, we propose a novel data-agnostic distribution fusion based model aggregation method called FedFusion to optimize federated learning with non-IID local datasets, based on which the heterogeneous clients' data distributions can be represented by a global distribution of several virtual fusion components with different parameters and weights. We develop a Variational AutoEncoder (VAE) method to learn the optimal parameters of the distribution fusion components based on limited statistical information extracted from the local models, and apply the derived distribution fusion model to optimize federated model aggregation with non-IID data. Extensive experiments based on various federated learning scenarios with real-world datasets show that FedFusion achieves significant performance improvement compared to the state-of-the-art.",./data/pdfs/CVPR2023/Federated Learning With Data-Agnostic Distribution Fusion.pdf,./data/imgs/CVPR2023/Federated Learning With Data-Agnostic Distribution Fusion.png,56a6afca80d0a784b6acd1c463bfdd4d2b303e3b,8.0,"{2025: 1, 2024: 6, 2023: 1}",4.0,"{2024: 1, 2023: 3}"
4324,DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium,CVPR,2023,"['Antyanta Bangunharcana', 'Ahmed Magd', 'Kyung-Soo Kim']","Self-supervised multi-frame depth estimation achieves high accuracy by computing matching costs of pixel correspondences between adjacent frames, injecting geometric information into the network. These pixel-correspondence candidates are computed based on the relative pose estimates between the frames. Accurate pose predictions are essential for precise matching cost computation as they influence the epipolar geometry. Furthermore, improved depth estimates can, in turn, be used to align pose estimates. Inspired by traditional structure-from-motion (SfM) principles, we propose the DualRefine model, which tightly couples depth and pose estimation through a feedback loop. Our novel update pipeline uses a deep equilibrium model framework to iteratively refine depth estimates and a hidden state of feature maps by computing local matching costs based on epipolar geometry. Importantly, we used the refined depth estimates and feature maps to compute pose updates at each step. This update in the pose estimates slowly alters the epipolar geometry during the refinement process. Experimental results on the KITTI dataset demonstrate competitive depth prediction and odometry prediction performance surpassing published self-supervised baselines <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> https://github.com/antabangun/DualRefine.",./data/pdfs/CVPR2023/DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium.pdf,./data/imgs/CVPR2023/DualRefine: Self-Supervised Depth and Pose Estimation Through Iterative Epipolar Sampling and Refinement Toward Equilibrium.png,334b3273918ff1029ec848d0b66e0d8c96e0b00f,15.0,"{2025: 3, 2024: 10, 2023: 1}",5.0,"{2024: 3, 2023: 1, 2022: 1}"
4628,StructVPR: Distill Structural Knowledge With Weighting Samples for Visual Place Recognition,CVPR,2023,"['Yanqing Shen', 'Sanping Zhou', 'Jingwen Fu', 'Ruotong Wang', 'Shitao Chen', 'Nanning Zheng']","Visual place recognition (VPR) is usually considered as a specific image retrieval problem. Limited by existing training frameworks, most deep learning-based works cannot extract sufficiently stable global features from RGB images and rely on a time-consuming re-ranking step to exploit spatial structural information for better performance. In this paper, we propose StructVPR, a novel training architecture for VPR, to enhance structural knowledge in RGB global features and thus improve feature stability in a constantly changing environment. Specifically, StructVPR uses segmentation images as a more definitive source of structural knowledge input into a CNN network and applies knowledge distillation to avoid online segmentation and inference of seg-branch in testing. Considering that not all samples contain high-quality and helpful knowledge, and some even hurt the performance of distillation, we partition samples and weigh each sample's distillation loss to enhance the expected knowledge precisely. Finally, StructVPR achieves impressive performance on several benchmarks using only global retrieval and even outperforms many two-stage approaches by a large margin. After adding additional re-ranking, ours achieves state-of-the-art performance while maintaining a low computational cost.",./data/pdfs/CVPR2023/StructVPR: Distill Structural Knowledge With Weighting Samples for Visual Place Recognition.pdf,./data/imgs/CVPR2023/StructVPR: Distill Structural Knowledge With Weighting Samples for Visual Place Recognition.png,a246b823232d744e0391e74928e2336e7fa50eb4,9.0,"{2025: 1, 2024: 7, 2023: 1}",7.0,"{2024: 5, 2023: 2}"
3951,Text2Scene: Text-Driven Indoor Scene Stylization With Part-Aware Details,CVPR,2023,"['Inwoo Hwang', 'Hyeonwoo Kim', 'Young‐Min Kim']","We propose Text2Scene, a method to automatically create realistic textures for virtual scenes composed of multiple objects. Guided by a reference image and text descriptions, our pipeline adds detailed texture on labeled 3D geometries in the room such that the generated colors respect the hierarchical structure or semantic parts that are often composed of similar materials. Instead of applying flat stylization on the entire scene at a single step, we obtain weak semantic cues from geometric segmentation, which are further clarified by assigning initial colors to segmented parts. Then we add texture details for individual objects such that their projections on image space exhibit feature embedding aligned with the embedding of the input. The decomposition makes the entire pipeline tractable to a moderate amount of computation resources and memory. As our framework utilizes the existing resources of image and text embedding, it does not require dedicated datasets with high-quality textures designed by skillful artists. To the best of our knowledge, it is the first practical and scalable approach that can create detailed and realistic textures of the desired style that maintain structural context for scenes with multiple objects.",./data/pdfs/CVPR2023/Text2Scene: Text-Driven Indoor Scene Stylization With Part-Aware Details.pdf,./data/imgs/CVPR2023/Text2Scene: Text-Driven Indoor Scene Stylization With Part-Aware Details.png,4259aaadf3c5a846c0d1b1c45b954fc76c072597,9.0,"{2025: 3, 2024: 6}",8.0,"{2024: 7, 2023: 1}"
3679,NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and Animation,CVPR,2023,"['Ziyan Wang', 'Giljoo Nam', 'Tuur Stuyck', 'Stephen Lombardi', 'Chen Cao', 'Jason Saragih', 'Michael Zollhöfer', 'Jessica K. Hodgins', 'Christoph Lassner']","The capture and animation of human hair are two of the major challenges in the creation of realistic avatars for the virtual reality. Both problems are highly challenging, because hair has complex geometry and appearance and exhibits challenging motion. In this paper, we present a two-stage approach that models hair independently of the head to address these challenges in a data-driven manner. The first stage, state compression, learns a low-dimensional latent space of 3D hair states including motion and appearance via a novel autoencoder-as-a-tracker strategy. To better disentangle the hair and head in appearance learning, we employ multi-view hair segmentation masks in combination with a differentiable volumetric renderer. The second stage optimizes a novel hair dynamics model that performs temporal hair transfer based on the discovered latent codes. To enforce higher stability while driving our dynamics model, we employ the 3D point-cloud autoencoder from the compression stage for denoising of the hair state. Our model outperforms the state of the art in novel view synthesis and is capable of creating novel hair animations without relying on hair observations as a driving signal. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">†</sup> <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">†</sup> Project page at https://ziyanwl.github.io/neuwigs/.",./data/pdfs/CVPR2023/NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and Animation.pdf,./data/imgs/CVPR2023/NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and Animation.png,6a7a186cd241032c8d4343e05c0bbb5870caa08c,11.0,"{2024: 6, 2023: 4}",12.0,"{2024: 6, 2023: 6}"
3826,Deep Incomplete Multi-View Clustering With Cross-View Partial Sample and Prototype Alignment,CVPR,2023,"['Jiaqi Jin', 'Siwei Wang', 'Zhibin Dong', 'Xinwang Liu', 'En Zhu']","The success of existing multi-view clustering relies on the assumption of sample integrity across multiple views. However, in real-world scenarios, samples of multi-view are partially available due to data corruption or sensor failure, which leads to incomplete multi-view clustering study (IMVC). Although several attempts have been proposed to address IMVC, they suffer from the following draw-backs: i) Existing methods mainly adopt cross-view contrastive learning forcing the representations of each sample across views to be exactly the same, which might ignore view discrepancy and flexibility in representations; ii) Due to the absence of non-observed samples across multiple views, the obtained prototypes of clusters might be unaligned and biased, leading to incorrect fusion. To address the above issues, we propose a Cross-view Partial Sample and Prototype Alignment Network (CPSPAN) for Deep Incomplete Multi-view Clustering. Firstly, unlike existing contrastive-based methods, we adopt pair-observed data alignment as 'proxy supervised signals' to guide instance-to-instance correspondence construction among views. Then, regarding of the shifted prototypes in IMVC, we further propose a prototype alignment module to achieve incomplete distribution calibration across views. Extensive experimental results showcase the effectiveness of our proposed modules, attaining noteworthy performance improvements when compared to existing IMVC competitors on benchmark datasets.",./data/pdfs/CVPR2023/Deep Incomplete Multi-View Clustering With Cross-View Partial Sample and Prototype Alignment.pdf,./data/imgs/CVPR2023/Deep Incomplete Multi-View Clustering With Cross-View Partial Sample and Prototype Alignment.png,6e1dcc4cf626a3d920314f7eafafbde2453a8dd0,30.0,"{2025: 2, 2024: 24, 2023: 3}",14.0,"{2024: 7, 2023: 7}"
3851,Learning With Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning,CVPR,2023,"['Zeyin Song', 'Yifan Zhao', 'Yujun Shi', 'Peixi Peng', 'Li Yuan', 'Yonghong Tian']","Few-shot class-incremental learning (FSCIL) aims at learning to classify new classes continually from limited samples without forgetting the old classes. The mainstream framework tackling FSCIL is first to adopt the cross-entropy (CE) loss for training at the base session, then freeze the feature extractor to adapt to new classes. However, in this work, we find that the CE loss is not ideal for the base session training as it suffers poor class separation in terms of representations, which further degrades generalization to novel classes. One tempting method to mitigate this problem is to apply an additional naïve supervised contrastive learning (SCL) in the base session. Unfortunately, we find that although SCL can create a slightly better representation separation among different base classes, it still struggles to separate base classes and new classes. Inspired by the observations made, we propose Semantic-Aware Virtual Contrastive model (SAVC), a novel method that facilitates separation between new classes and base classes by introducing virtual classes to SCL. These virtual classes, which are generated via pre-defined transformations, not only act as placeholders for unseen classes in the representation space, but also provide diverse semantic information. By learning to recognize and contrast in the fantasy space fostered by virtual classes, our SAVC significantly boosts base class separation and novel class generalization, achieving new state-of-the-art performance on the three widely-used FSCIL benchmark datasets. Code is available at: https://github.com/zysong0113/SAVC.",./data/pdfs/CVPR2023/Learning With Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning.pdf,./data/imgs/CVPR2023/Learning With Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning.png,45d613442ca75eb515de49178cd5bee6ac90fd88,39.0,"{2025: 8, 2024: 27, 2023: 4}",20.0,"{2024: 13, 2023: 7}"
3614,Affordance Diffusion: Synthesizing Hand-Object Interactions,CVPR,2023,"['Yufei Ye', 'Xueting Li', 'Abhinav Gupta', 'Shalini De Mellon', 'Stan Birchfield', 'Jiaming Song', 'Shubham Tulsiani', 'Sifei Liu']","Recent successes in image synthesis are powered by large-scale diffusion models. However, most methods are currently limited to either text- or image-conditioned generation for synthesizing an entire image, texture transfer or inserting objects into a user-specified region. In contrast, in this work we focus on synthesizing complex interactions (i.e., an articulated hand) with a given object. Given an RGB image of an object, we aim to hallucinate plausible images of a human hand interacting with it. We propose a two-step generative approach: a LayoutNet that samples an articulation-agnostic hand-object-interaction layout, and a ContentNet that synthesizes images of a hand grasping the object given the predicted layout. Both are built on top of a large-scale pretrained diffusion model to make use of its latent representation. Compared to baselines, the proposed method is shown to generalize better to novel objects and perform surprisingly well on out-of-distribution in-the-wild scenes of portable-sized objects. The resulting system allows us to predict descriptive affordance information, such as hand articulation and approaching orientation.",./data/pdfs/CVPR2023/Affordance Diffusion: Synthesizing Hand-Object Interactions.pdf,./data/imgs/CVPR2023/Affordance Diffusion: Synthesizing Hand-Object Interactions.png,6aca520f9a226235062649eeb12928b51c69b90a,31.0,"{2025: 2, 2024: 27, 2023: 2}",37.0,"{2024: 21, 2023: 16}"
3704,CRAFT: Concept Recursive Activation FacTorization for Explainability,CVPR,2023,"['Thomas Fel', 'Agustin Martin Picard', 'Louis Béthune', 'Thibaut Boissin', 'David Vigouroux', 'Julien Colin', 'Rémi Cadénc', 'T. Serre']","Attribution methods, which employ heatmaps to identify the most influential regions of an image that impact model decisions, have gained widespread popularity as a type of explainability method. However, recent research has exposed the limited practical value of these methods, attributed in part to their narrow focus on the most prominent regions of an image - revealing ""where"" the model looks, but failing to elucidate ""what"" the model sees in those areas. In this work, we try to fill in this gap with CRAFT - a novel approach to identify both ""what"" and ""where"" by generating concept-based explanations. We introduce 3 new ingredients to the automatic concept extraction literature: (i) a recursive strategy to detect and decompose concepts across layers, (ii) a novel method for a more faithful estimation of concept importance using Sobol indices, and (iii) the use of implicit differentiation to unlock Concept Attribution Maps. We conduct both human and computer vision experiments to demonstrate the benefits of the proposed approach. We show that the proposed concept importance estimation technique is more faithful to the model than previous methods. When evaluating the usefulness of the method for human experimenters on a human-centered utility benchmark, we find that our approach significantly improves on two of the three test scenarios.",./data/pdfs/CVPR2023/CRAFT: Concept Recursive Activation FacTorization for Explainability.pdf,./data/imgs/CVPR2023/CRAFT: Concept Recursive Activation FacTorization for Explainability.png,8185c2ac7a4ea46ccccaa76a3b8c02fcf7354b47,35.0,"{2025: 2, 2024: 22, 2023: 8}",45.0,"{2024: 21, 2023: 22, 2022: 2}"
18265,Automatic Domain Partitioning for Multi-Domain Learning,EMNLP,2013,"['Di Wang', 'Chenyan Xiong', 'William Yang Wang']","Multi-Domain learning (MDL) assumes that the domain labels in the dataset are known. However, when there are multiple metadata attributes available, it is not always straightforward to select a single best attribute for domain partition, and it is possible that combining more than one metadata attributes (including continuous attributes) can lead to better MDL performance. In this work, we propose an automatic domain partitioning approach that aims at providing better domain identities for MDL. We use a supervised clustering approach that learns the domain distance between data instances , and then cluster the data into better domains for MDL. Our experiment on real multi-domain datasets shows that using our automatically generated domain partition improves over popular MDL methods.",./data/pdfs/EMNLP2013/Automatic Domain Partitioning for Multi-Domain Learning.pdf,./data/imgs/EMNLP2013/Automatic Domain Partitioning for Multi-Domain Learning.png,25287595444b22a5f03076bfa7349a75d6f875e7,1.0,{2015: 1},1.0,{2015: 1}
18286,Collective Personal Profile Summarization with Social Networks,EMNLP,2013,"['Zhongqing Wang', 'Shoushan Li', 'Fang Kong', 'Guodong Zhou']","Personal profile information on social media like LinkedIn.com and Facebook.com is at the core of many interesting applications, such as talent recommendation and contextual advertising. However, personal profiles usually lack organization confronted with the large amount of available information. Therefore, it is always a challenge for people to find desired information from them. In this paper, we address the task of personal profile summarization by leveraging both personal profile textual information and social networks. Here, using social networks is motivated by the intuition that, people with similar academic, business or social connections (e.g. co-major, co-university, and cocorporation) tend to have similar experience and summaries. To achieve the learning process, we propose a collective factor graph (CoFG) model to incorporate all these resources of knowledge to summarize personal profiles with local textual attribute functions and social connection factors. Extensive evaluation on a large-scale dataset from LinkedIn.com demonstrates the effectiveness of the proposed approach. *",./data/pdfs/EMNLP2013/Collective Personal Profile Summarization with Social Networks.pdf,./data/imgs/EMNLP2013/Collective Personal Profile Summarization with Social Networks.png,ad825000bb191e1cf705a2f83e55c128bfd1d160,3.0,"{2016: 1, 2015: 1, 2014: 1}",6.0,"{2019: 1, 2017: 1, 2016: 3, 2014: 1}"
18333,Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics,EMNLP,2013,"['Eva Maria Vecchi', 'Roberto Zamparelli', 'Marco Baroni']","In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Specifically, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics.",./data/pdfs/EMNLP2013/Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics.pdf,./data/imgs/EMNLP2013/Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics.png,dc62719c10b8856ede59e665bd781667ec055f63,10.0,"{2021: 1, 2020: 4, 2016: 1, 2015: 2, 2014: 1, 2013: 1}",10.0,"{2022: 1, 2021: 2, 2020: 3, 2017: 1, 2015: 1, 2014: 1, 2013: 1}"
18219,Predicting the Resolution of Referring Expressions from User Behavior,EMNLP,2013,"['Nikos Engonopoulos', 'Martín Villalba', 'Ivan Titov', 'Alexander Koller']","We present a statistical model for predicting how the user of an interactive, situated NLP system resolved a referring expression. The model makes an initial prediction based on the meaning of the utterance, and revises it continuously based on the user’s behavior. The combined model outperforms its components in predicting reference resolution and when to give feedback.",./data/pdfs/EMNLP2013/Predicting the Resolution of Referring Expressions from User Behavior.pdf,./data/imgs/EMNLP2013/Predicting the Resolution of Referring Expressions from User Behavior.png,8d4ffdc4c7bc2052dfde218f58b9a69cd148e1f1,16.0,"{2023: 3, 2021: 1, 2019: 1, 2018: 2, 2017: 1, 2016: 2, 2015: 3, 2014: 3}",19.0,"{2023: 2, 2022: 1, 2021: 1, 2019: 3, 2018: 2, 2017: 3, 2016: 2, 2015: 3, 2014: 2}"
18323,Exploiting Zero Pronouns to Improve Chinese Coreference Resolution,EMNLP,2013,"['Fang Kong', 'Hwee Tou Ng']","Coreference resolution plays a critical role in discourse analysis. This paper focuses on exploiting zero pronouns to improve Chinese coreference resolution. In particular, a simplified semantic role labeling framework is proposed to identify clauses and to detect zero pronouns effectively, and two effective methods (refining syntactic parser and refining learning example generation) are employed to exploit zero pronouns for Chinese coreference resolution. Evaluation on the CoNLL-2012 shared task data set shows that zero pronouns can significantly improve Chinese coreference resolution.",./data/pdfs/EMNLP2013/Exploiting Zero Pronouns to Improve Chinese Coreference Resolution.pdf,./data/imgs/EMNLP2013/Exploiting Zero Pronouns to Improve Chinese Coreference Resolution.png,08ec59ce3692c046c9c23ef85b0073f6b8c5e397,17.0,"{2023: 3, 2022: 1, 2021: 2, 2020: 1, 2019: 1, 2018: 1, 2017: 1, 2015: 5, 2014: 2}",20.0,"{2023: 2, 2022: 2, 2021: 2, 2020: 1, 2019: 2, 2017: 4, 2015: 5, 2014: 2}"
18259,Japanese Zero Reference Resolution Considering Exophora and Author Reader Mentions,EMNLP,2013,"['Masatsugu Hangyo', 'Daisuke Kawahara', 'Sadao Kurohashi']","In Japanese, zero references often occur and many of them are categorized into zero exophora, in which a referent is not mentioned in the document. However, previous studies have focused on only zero endophora, in which a referent explicitly appears. We present a zero reference resolution model considering zero exophora and author/reader of a document. To deal with zero exophora, our model adds pseudo entities corresponding to zero exophora to candidate referents of zero pronouns. In addition, we automatically detect mentions that refer to the author and reader of a document by using lexico-syntactic patterns. We represent their particular behavior in a discourse as a feature vector of a machine learning model. The experimental results demonstrate the effectiveness of our model for not only zero exophora but also zero endophora.",./data/pdfs/EMNLP2013/Japanese Zero Reference Resolution Considering Exophora and Author Reader Mentions.pdf,./data/imgs/EMNLP2013/Japanese Zero Reference Resolution Considering Exophora and Author Reader Mentions.png,4d47cc69c22a703cc0c3660db3273b123cf6db84,29.0,"{2022: 1, 2021: 9, 2020: 2, 2019: 3, 2018: 5, 2017: 2, 2016: 4, 2015: 2, 2014: 1}",28.0,"{2021: 5, 2020: 3, 2019: 4, 2018: 5, 2017: 3, 2016: 4, 2015: 3, 2014: 1}"
18252,Tree Kernel-based Negation and Speculation Scope Detection with Structured Syntactic Parse Features,EMNLP,2013,"['Bowei Zou', 'Guodong Zhou', 'Qiaoming Zhu']","Scope detection is a key task in information extraction. This paper proposes a new approach for tree kernel-based scope detection by using the structured syntactic parse information. In addition, we have explored the way of selecting compatible features for different part-of-speech cues. Experiments on the BioScope corpus show that both constituent and dependency structured syntactic parse features have the advantage in capturing the potential relationships between cues and their scopes. Compared with the state of the art scope detection systems, our system achieves substantial improvement.",./data/pdfs/EMNLP2013/Tree Kernel-based Negation and Speculation Scope Detection with Structured Syntactic Parse Features.pdf,./data/imgs/EMNLP2013/Tree Kernel-based Negation and Speculation Scope Detection with Structured Syntactic Parse Features.png,05e8f743a77aa487a42cac65935e7570f8b94b01,49.0,"{2020: 3, 2019: 18, 2018: 8, 2017: 9, 2016: 6, 2015: 5}",41.0,"{2022: 2, 2021: 1, 2020: 4, 2019: 5, 2018: 7, 2017: 10, 2016: 7, 2015: 5}"
18306,Document Summarization via Guided Sentence Compression,EMNLP,2013,"['Chen Li', 'Fei Liu', 'Fuliang Weng', 'Yang Liu']","Joint compression and summarization has been used recently to generate high quality summaries. However, such word-based joint optimization is computationally expensive. In this paper we adopt the ‘sentence compression + sentence selection’ pipeline approach for compressive summarization, but propose to perform summary guided compression, rather than generic sentence-based compression. To create an annotated corpus, the human annotators were asked to compress sentences while explicitly given the important summary words in the sentences. Using this corpus, we train a supervised sentence compression model using a set of word-, syntax-, and documentlevel features. During summarization, we use multiple compressed sentences in the integer linear programming framework to select salient summary sentences. Our results on the TAC 2008 and 2011 summarization data sets show that by incorporating the guided sentence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art.",./data/pdfs/EMNLP2013/Document Summarization via Guided Sentence Compression.pdf,./data/imgs/EMNLP2013/Document Summarization via Guided Sentence Compression.png,1b98ecc801d4435fd57c6ef87b4c24cf6b3b58ce,68.0,"{2021: 3, 2020: 3, 2019: 14, 2018: 12, 2017: 8, 2016: 8, 2015: 15, 2014: 4, 2013: 1}",66.0,"{2023: 2, 2022: 2, 2021: 2, 2020: 4, 2019: 8, 2018: 9, 2017: 9, 2016: 9, 2015: 16, 2014: 4, 2013: 1}"
18238,A Systematic Exploration of Diversity in Machine Translation,EMNLP,2013,"['Kevin Gimpel', 'Dhruv Batra', 'Chris Dyer', 'Gregory Shakhnarovich']","This paper addresses the problem of producing a diverse set of plausible translations. We present a simple procedure that can be used with any statistical machine translation (MT) system. We explore three ways of using diverse translations: (1) system combination, (2) discriminative reranking with rich features, and (3) a novel post-editing scenario in which multiple translations are presented to users. We find that diversity can improve performance on these tasks, especially for sentences that are difficult for MT.",./data/pdfs/EMNLP2013/A Systematic Exploration of Diversity in Machine Translation.pdf,./data/imgs/EMNLP2013/A Systematic Exploration of Diversity in Machine Translation.png,e8679859bf0ad6ca4253603d05462838957733fb,86.0,"{2024: 1, 2023: 2, 2022: 3, 2021: 10, 2020: 11, 2019: 11, 2018: 12, 2017: 10, 2016: 14, 2015: 8, 2014: 3, 2013: 1}",95.0,"{2024: 5, 2023: 6, 2022: 7, 2021: 8, 2020: 11, 2019: 10, 2018: 12, 2017: 12, 2016: 12, 2015: 7, 2014: 4, 2013: 1}"
18191,Scaling Semantic Parsers with On-the-Fly Ontology Matching,EMNLP,2013,"['Tom Kwiatkowski', 'Eunsol Choi', 'Yoav Artzi', 'Luke Zettlemoyer']","We consider the challenge of learning semantic parsers that scale to large, open-domain problems, such as question answering with Freebase. In such settings, the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to represent in a fixed target ontology. For example, even simple phrases such as ‘daughter’ and ‘number of people living in’ cannot be directly represented in Freebase, whose ontology instead encodes facts about gender, parenthood, and population. In this paper, we introduce a new semantic parsing approach that learns to resolve such ontological mismatches. The parser is learned from question-answer pairs, uses a probabilistic CCG to build linguistically motivated logicalform meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus.",./data/pdfs/EMNLP2013/Scaling Semantic Parsers with On-the-Fly Ontology Matching.pdf,./data/imgs/EMNLP2013/Scaling Semantic Parsers with On-the-Fly Ontology Matching.png,b2ac51e10a3510eadac5eac5e4fb828f086fab88,255.0,"{2024: 1, 2023: 3, 2022: 5, 2021: 17, 2020: 20, 2019: 32, 2018: 26, 2017: 35, 2016: 32, 2015: 41, 2014: 40, 2013: 3}",322.0,"{2024: 1, 2023: 4, 2022: 11, 2021: 18, 2020: 22, 2019: 40, 2018: 36, 2017: 43, 2016: 49, 2015: 50, 2014: 45, 2013: 3}"
18547,Reordering Model for Forest-to-String Machine Translation,EMNLP,2014,['Martin Čmejrek'],"In this paper, we present a novel extension of a forest-to-string machine translation system with a reordering model.We predict reordering probabilities for every pair of source words with a model using features observed from the input parse forest.Our approach naturally deals with the ambiguity present in the input parse forest, but, at the same time, takes into account only the parts of the input forest used by the current translation hypothesis.The method provides improvement from 0.6 up to 1.0 point measured by (Ter -Bleu)/2 metric.",./data/pdfs/EMNLP2014/Reordering Model for Forest-to-String Machine Translation.pdf,./data/imgs/EMNLP2014/Reordering Model for Forest-to-String Machine Translation.png,92e0bbecbafe7812d716b71a6662204280d9e951,0.0,{},1.0,{2016: 1}
18430,Predicting Dialect Variation in Immigrant Contexts Using Light Verb Constructions,EMNLP,2014,"['A. Seza Doğruöz', 'Preslav Nakov']","Languages spoken by immigrants change due to contact with the local languages. Capturing these changes is problematic for current language technologies, which are typically developed for speakers of the standard dialect only. Even when dialectal variants are available for such technologies, we still need to predict which dialect is being used. In this study, we distinguish between the immigrant and the standard dialect of Turkish by focusing on Light Verb Constructions. We experiment with a number of grammatical and contextual features, achieving over 84% accuracy (56% baseline).",./data/pdfs/EMNLP2014/Predicting Dialect Variation in Immigrant Contexts Using Light Verb Constructions.pdf,./data/imgs/EMNLP2014/Predicting Dialect Variation in Immigrant Contexts Using Light Verb Constructions.png,d2680d8817ff72e048f57b4438f0a1cfd630c5de,4.0,"{2023: 1, 2021: 1, 2016: 1, 2015: 1}",4.0,"{2023: 1, 2021: 1, 2017: 1, 2016: 1}"
18412,A convex relaxation for weakly supervised relation extraction,EMNLP,2014,['Édouard Grave'],"A promising approach to relation extraction, called weak or distant supervision, exploits an existing database of facts as training data, by aligning it to an unlabeled collection of text documents. Using this approach, the task of relation extraction can easily be scaled to hundreds of different relationships. However, distant supervision leads to a challenging multiple instance, multiple label learning problem. Most of the proposed solutions to this problem are based on non-convex formulations, and are thus prone to local minima. In this article, we propose a new approach to the problem of weakly supervised relation extraction, based on discriminative clustering and leading to a convex formulation. We demonstrate that our approach outperforms state-of-the-art methods on the challenging dataset introduced by Riedel et al. (2012).",./data/pdfs/EMNLP2014/A convex relaxation for weakly supervised relation extraction.pdf,./data/imgs/EMNLP2014/A convex relaxation for weakly supervised relation extraction.png,3e01260b55a37c7b391122a120c067350f6976cf,9.0,"{2020: 1, 2017: 1, 2016: 3, 2015: 3, 2014: 1}",10.0,"{2020: 1, 2018: 1, 2017: 2, 2016: 2, 2015: 3, 2014: 1}"
18564,Improve Statistical Machine Translation with Context-Sensitive Bilingual Semantic Embedding Model,EMNLP,2014,"['Haiyang Wu', 'Daxiang Dong', 'Xiaoguang Hu', 'Dianhai Yu', 'Wei He', 'Hua Wu', 'Haifeng Wang', 'Ting Liu']","We investigate how to improve bilingual embedding which has been successfully used as a feature in phrase-based statistical machine translation (SMT). Despite bilingual embedding’s success, the contextual information, which is of critical importance to translation quality, was ignored in previous work. To employ the contextual information, we propose a simple and memory-efficient model for learning bilingual embedding, taking both the source phrase and context around the phrase into account. Bilingual translation scores generated from our proposed bilingual embedding model are used as features in our SMT system. Experimental results show that the proposed method achieves significant improvements on large-scale Chinese-English translation task.",./data/pdfs/EMNLP2014/Improve Statistical Machine Translation with Context-Sensitive Bilingual Semantic Embedding Model.pdf,./data/imgs/EMNLP2014/Improve Statistical Machine Translation with Context-Sensitive Bilingual Semantic Embedding Model.png,58ad5b84b17fb70b353b26f41715197ecb77e5cf,18.0,"{2020: 1, 2019: 1, 2018: 4, 2017: 2, 2016: 3, 2015: 7}",15.0,"{2020: 1, 2019: 1, 2018: 4, 2017: 3, 2016: 2, 2015: 4}"
18359,Resolving Referring Expressions in Conversational Dialogs for Natural User Interfaces,EMNLP,2014,"['Aslı Çelikyılmaz', 'Zhaleh Feizollahi', 'Dilek Hakkani‐Tür', 'Ruhi Sarikaya']","Unlike traditional over-the-phone spoken dialog systems (SDSs), modern dialog systems tend to have visual rendering on the device screen as an additional modality to communicate the system's response to the user.Visual display of the system's response not only changes human behavior when interacting with devices, but also creates new research areas in SDSs.Onscreen item identification and resolution in utterances is one critical problem to achieve a natural and accurate humanmachine communication.We pose the problem as a classification task to correctly identify intended on-screen item(s) from user utterances.Using syntactic, semantic as well as context features from the display screen, our model can resolve different types of referring expressions with up to 90% accuracy.In the experiments we also show that the proposed model is robust to domain and screen layout changes.",./data/pdfs/EMNLP2014/Resolving Referring Expressions in Conversational Dialogs for Natural User Interfaces.pdf,./data/imgs/EMNLP2014/Resolving Referring Expressions in Conversational Dialogs for Natural User Interfaces.png,b635009a98e89485b501ee2e873e11f52ecb40ea,24.0,"{2023: 1, 2022: 1, 2020: 1, 2019: 4, 2018: 7, 2017: 2, 2016: 3, 2015: 3, 2014: 2}",24.0,"{2022: 2, 2020: 3, 2019: 2, 2018: 4, 2017: 3, 2016: 3, 2015: 5, 2014: 2}"
18419,Staying on Topic: An Indicator of Power in Political Debates,EMNLP,2014,"['Vinodkumar Prabhakaran', 'Ashima Arora', 'Owen Rambow']","We study the topic dynamics of interactions in political debates using the 2012 Republican presidential primary debates as data. We show that the tendency of candidates to shift topics changes over the course of the election campaign, and that it is correlated with their relative power. We also show that our topic shift features help predict candidates’ relative rankings.",./data/pdfs/EMNLP2014/Staying on Topic: An Indicator of Power in Political Debates.pdf,./data/imgs/EMNLP2014/Staying on Topic: An Indicator of Power in Political Debates.png,469a4152eeeda5ed2c4a9ef64c9d94ed881d57e8,26.0,"{2023: 5, 2022: 1, 2021: 1, 2020: 2, 2019: 2, 2018: 4, 2017: 3, 2016: 6, 2015: 2}",28.0,"{2023: 3, 2022: 1, 2021: 1, 2020: 3, 2019: 3, 2018: 2, 2017: 3, 2016: 9, 2015: 1, 2014: 2}"
18436,Language Transfer Hypotheses with Linear SVM Weights,EMNLP,2014,"['Shervin Malmasi', 'Mark Dras']","Language transfer, the characteristic second language usage patterns caused by native language interference, is investigated by Second Language Acquisition (SLA) researchers seeking to find overused and underused linguistic features.In this paper we develop and present a methodology for deriving ranked lists of such features.Using very large learner data, we show our method's ability to find relevant candidates using sophisticated linguistic features.To illustrate its applicability to SLA research, we formulate plausible language transfer hypotheses supported by current evidence.This is the first work to extend Native Language Identification to a broader linguistic interpretation of learner data and address the automatic extraction of underused features on a per-native language basis.",./data/pdfs/EMNLP2014/Language Transfer Hypotheses with Linear SVM Weights.pdf,./data/imgs/EMNLP2014/Language Transfer Hypotheses with Linear SVM Weights.png,96f052735470a90319b6393066c4532b7258f318,39.0,"{2023: 1, 2022: 1, 2020: 2, 2019: 1, 2018: 4, 2017: 10, 2016: 11, 2015: 5, 2014: 4}",37.0,"{2023: 1, 2022: 1, 2021: 1, 2020: 1, 2019: 1, 2018: 3, 2017: 8, 2016: 9, 2015: 8, 2014: 3, 2013: 1}"
18531,Explaining the Stars: Weighted Multiple-Instance Learning for Aspect-Based Sentiment Analysis,EMNLP,2014,"['Nikolaos Pappas', 'Andréi Popescu-Belis']","This paper introduces a model of multipleinstance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from usercontributed texts such as product reviews.Each variable-length text is represented by several independent feature vectors; one word vector per sentence or paragraph.For learning from texts with known aspect ratings, the model performs multipleinstance regression (MIR) and assigns importance weights to each of the sentences or paragraphs of a text, uncovering their contribution to the aspect ratings.Next, the model is used to predict aspect ratings in previously unseen texts, demonstrating interpretability and explanatory power for its predictions.We evaluate the model on seven multi-aspect sentiment analysis data sets, improving over four MIR baselines and two strong bag-of-words linear models, namely SVR and Lasso, by more than 10% relative in terms of MSE.",./data/pdfs/EMNLP2014/Explaining the Stars: Weighted Multiple-Instance Learning for Aspect-Based Sentiment Analysis.pdf,./data/imgs/EMNLP2014/Explaining the Stars: Weighted Multiple-Instance Learning for Aspect-Based Sentiment Analysis.png,661e8c58a60d8d609445a3114ae82c8386cbc838,74.0,"{2025: 2, 2024: 2, 2023: 4, 2022: 8, 2021: 11, 2020: 13, 2019: 12, 2018: 4, 2017: 9, 2016: 6, 2015: 2, 2014: 1}",63.0,"{2024: 2, 2023: 4, 2022: 6, 2021: 8, 2020: 10, 2019: 11, 2018: 3, 2017: 9, 2016: 7, 2015: 3}"
18471,A* CCG Parsing with a Supertag-factored Model,EMNLP,2014,"['Mike Lewis', 'Mark Steedman']","We introduce a new CCG parsing model which is factored on lexical category assignments.Parsing is then simply a deterministic search for the most probable category sequence that supports a CCG derivation.The parser is extremely simple, with a tiny feature set, no POS tagger, and no statistical model of the derivation or dependencies.Formulating the model in this way allows a highly effective heuristic for A * parsing, which makes parsing extremely fast.Compared to the standard C&C CCG parser, our model is more accurate out-of-domain, is four times faster, has higher coverage, and is greatly simplified.We also show that using our parser improves the performance of a state-ofthe-art question answering system.",./data/pdfs/EMNLP2014/A* CCG Parsing with a Supertag-factored Model.pdf,./data/imgs/EMNLP2014/A* CCG Parsing with a Supertag-factored Model.png,d4e87e2e64da8d72a9dc91a16a60e7fdaf28f00e,119.0,"{2023: 5, 2022: 1, 2021: 5, 2020: 13, 2019: 21, 2018: 16, 2017: 19, 2016: 20, 2015: 18, 2014: 1}",112.0,"{2024: 1, 2023: 5, 2022: 2, 2021: 2, 2020: 11, 2019: 16, 2018: 16, 2017: 21, 2016: 19, 2015: 18, 2014: 1}"
18421,Modeling Biological Processes for Reading Comprehension,EMNLP,2014,"['Jonathan Berant', 'Vivek Srikumar', 'Pei‐Chun Chen', 'Abby Vander Linden', 'Brittany Harding', 'Brad Huang', 'Peter E. Clark', 'Christopher D. Manning']","Jonathan Berant, Vivek Srikumar, Pei-Chun Chen, Abby Vander Linden, Brittany Harding, Brad Huang, Peter Clark, Christopher D. Manning. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014.",./data/pdfs/EMNLP2014/Modeling Biological Processes for Reading Comprehension.pdf,./data/imgs/EMNLP2014/Modeling Biological Processes for Reading Comprehension.png,6396ab37641d36be4c26420e58adeb8665914c3b,218.0,"{2024: 6, 2023: 20, 2022: 15, 2021: 25, 2020: 25, 2019: 25, 2018: 32, 2017: 19, 2016: 31, 2015: 19, 2014: 1}",211.0,"{2024: 8, 2023: 16, 2022: 25, 2021: 24, 2020: 20, 2019: 18, 2018: 23, 2017: 27, 2016: 29, 2015: 18, 2014: 3}"
18790,A Framework for Comparing Groups of Documents,EMNLP,2015,['Arun S. Maiya'],"We present a general framework for comparing multiple groups of documents.A bipartite graph model is proposed where document groups are represented as one node set and the comparison criteria are represented as the other node set.Using this model, we present basic algorithms to extract insights into similarities and differences among the document groups.Finally, we demonstrate the versatility of our framework through an analysis of NSF funding programs for basic research.",./data/pdfs/EMNLP2015/A Framework for Comparing Groups of Documents.pdf,./data/imgs/EMNLP2015/A Framework for Comparing Groups of Documents.png,a309564190350bf3b1f2aafb7a12f419d55dc351,2.0,"{2017: 1, 2015: 1}",1.0,{2015: 1}
18806,Sieve-Based Spatial Relation Extraction with Expanding Parse Trees,EMNLP,2015,"['Jennifer D’Souza', 'Vincent Ng']","A key challenge introduced by the recent SpaceEval shared task on spatial relation extraction is the identification of MOVELINKs, a type of spatial relation in which up to eight spatial elements can participate.To handle the complexity of extracting MOVELINKs, we combine two ideas that have been successfully applied to information extraction tasks, namely tree kernels and multi-pass sieves, proposing the use of an expanding parse tree as a novel structured feature for training MOVELINK classifiers.Our approach yields state-of-the-art results on two key tasks in SpaceEval.",./data/pdfs/EMNLP2015/Sieve-Based Spatial Relation Extraction with Expanding Parse Trees.pdf,./data/imgs/EMNLP2015/Sieve-Based Spatial Relation Extraction with Expanding Parse Trees.png,c5ec2968a37312b89244eb8139273cf870cab802,11.0,"{2024: 4, 2023: 1, 2021: 2, 2018: 1, 2017: 2, 2016: 1}",8.0,"{2023: 1, 2022: 1, 2021: 2, 2018: 1, 2017: 2, 2016: 1}"
18796,Summarizing Topical Contents from PubMed Documents Using a Thematic Analysis,EMNLP,2015,"['Sun Kim', 'Lana Yeganova', 'W. John Wilbur']","Improving the search and browsing experience in PubMed is a key component in helping users detect information of interest.In particular, when exploring a novel field, it is important to provide a comprehensive view for a specific subject.One solution for providing this panoramic picture is to find sub-topics from a set of documents.We propose a method that finds sub-topics that we refer to as themes and computes representative titles based on a set of documents in each theme.The method combines a thematic clustering algorithm and the Pool Adjacent Violators algorithm to induce significant themes.Then, for each theme, a title is computed using PubMed document titles and theme-dependent term scores.We tested our system on five disease sets from OMIM and evaluated the results based on normalized point-wise mutual information and MeSH terms.For both performance measures, the proposed approach outperformed LDA.The quality of theme titles were also evaluated by comparing them with manually created titles.",./data/pdfs/EMNLP2015/Summarizing Topical Contents from PubMed Documents Using a Thematic Analysis.pdf,./data/imgs/EMNLP2015/Summarizing Topical Contents from PubMed Documents Using a Thematic Analysis.png,01fdbd80c0cbf287039db96236aba6c145ba6ea0,6.0,"{2021: 1, 2018: 1, 2017: 1, 2016: 3}",9.0,"{2024: 1, 2021: 1, 2020: 1, 2019: 1, 2018: 1, 2017: 1, 2016: 3}"
18775,Incorporating Trustiness and Collective Synonym Contrastive Evidence into Taxonomy Construction,EMNLP,2015,"['Tuan Luu Anh', 'Jung‐Jae Kim', 'See Kiong Ng']","Taxonomy plays an important role in many applications by organizing domain knowledge into a hierarchy of is-a relations between terms.Previous works on the taxonomic relation identification from text corpora lack in two aspects: 1) They do not consider the trustiness of individual source texts, which is important to filter out incorrect relations from unreliable sources.2) They also do not consider collective evidence from synonyms and contrastive terms, where synonyms may provide additional supports to taxonomic relations, while contrastive terms may contradict them.In this paper, we present a method of taxonomic relation identification that incorporates the trustiness of source texts measured with such techniques as PageRank and knowledge-based trust, and the collective evidence of synonyms and contrastive terms identified by linguistic pattern matching and machine learning.The experimental results show that the proposed features can consistently improve performance up to 4%-10% of F-measure.",./data/pdfs/EMNLP2015/Incorporating Trustiness and Collective Synonym Contrastive Evidence into Taxonomy Construction.pdf,./data/imgs/EMNLP2015/Incorporating Trustiness and Collective Synonym Contrastive Evidence into Taxonomy Construction.png,aaad6e24bbd10c973aa7f576a368e2e796cb6261,14.0,"{2025: 1, 2023: 3, 2022: 1, 2020: 1, 2019: 1, 2018: 2, 2017: 1, 2016: 4}",13.0,"{2023: 3, 2020: 2, 2018: 2, 2017: 2, 2016: 4}"
18791,Multi-label Text Categorization with Joint Learning Predictions-as-Features Method,EMNLP,2015,"['Li Li', 'Houfeng Wang', 'Xu Sun', 'Baobao Chang', 'Shi Zhao', 'Lei Sha']","Multi-label text categorization is a type of text categorization, where each document is assigned to one or more categories.Recently, a series of methods have been developed, which train a classifier for each label, organize the classifiers in a partially ordered structure and take predictions produced by the former classifiers as the latter classifiers' features.These predictions-asfeatures style methods model high order label dependencies and obtain high performance.Nevertheless, the predictionsas-features methods suffer a drawback.When training a classifier for one label, the predictions-as-features methods can model dependencies between former labels and the current label, but they can't model dependencies between the current label and the latter labels.To address this problem, we propose a novel joint learning algorithm that allows the feedbacks to be propagated from the classifiers for latter labels to the classifier for the current label.We conduct experiments using real-world textual data sets, and these experiments illustrate the predictions-as-features models trained by our algorithm outperform the original models.",./data/pdfs/EMNLP2015/Multi-label Text Categorization with Joint Learning Predictions-as-Features Method.pdf,./data/imgs/EMNLP2015/Multi-label Text Categorization with Joint Learning Predictions-as-Features Method.png,029dde0e43d272a47aa8479831a867254574e7ba,25.0,"{2024: 3, 2023: 1, 2022: 1, 2021: 3, 2020: 1, 2019: 8, 2018: 7, 2017: 1}",25.0,"{2023: 3, 2022: 1, 2021: 3, 2020: 2, 2019: 9, 2018: 5, 2017: 1, 2014: 1}"
18631,Modeling Reportable Events as Turning Points in Narrative,EMNLP,2015,"['Jessica Ouyang', 'Kathleen McKeown']","We present novel experiments in modeling the rise and fall of story characteristics within narrative, leading up to the Most Reportable Event (MRE), the compelling event that is the nucleus of the story.We construct a corpus of personal narratives from the bulletin board website Reddit, using the organization of Reddit content into topic-specific communities to automatically identify narratives.Leveraging the structure of Reddit comment threads, we automatically label a large dataset of narratives.We present a change-based model of narrative that tracks changes in formality, affect, and other characteristics over the course of a story, and we use this model in distant supervision and selftraining experiments that achieve significant improvements over the baselines at the task of identifying MREs.",./data/pdfs/EMNLP2015/Modeling Reportable Events as Turning Points in Narrative.pdf,./data/imgs/EMNLP2015/Modeling Reportable Events as Turning Points in Narrative.png,d4bf8b2de7d9b8b7f001ac2da5295ecf2c7b674e,36.0,"{2024: 2, 2023: 6, 2022: 2, 2021: 2, 2020: 5, 2019: 4, 2018: 6, 2017: 7, 2016: 1, 2015: 1}",38.0,"{2024: 1, 2023: 9, 2022: 3, 2021: 4, 2020: 5, 2019: 2, 2018: 8, 2017: 4, 2016: 1, 2015: 1}"
18892,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,EMNLP,2015,[],,./data/pdfs/EMNLP2015/Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.pdf,./data/imgs/EMNLP2015/Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.png,2c293051d4d95fec88be9890072f722eba72c9f1,760.0,"{2025: 52, 2024: 148, 2023: 140, 2022: 121, 2021: 101, 2020: 73, 2019: 51, 2018: 28, 2017: 16, 2016: 11, 2015: 3}",53.0,"{2024: 2, 2023: 3, 2022: 5, 2021: 7, 2020: 6, 2019: 12, 2018: 8, 2017: 5, 2016: 5}"
18757,Experiments in Open Domain Deception Detection,EMNLP,2015,"['Verónica Pérez‐Rosas', 'Rada Mihalcea']","The widespread use of deception in online sources has motivated the need for methods to automatically profile and identify deceivers.This work explores deception, gender and age detection in short texts using a machine learning approach.First, we collect a new open domain deception dataset also containing demographic data such as gender and age.Second, we extract feature sets including n-grams, shallow and deep syntactic features, semantic features, and syntactic complexity and readability metrics.Third, we build classifiers that aim to predict deception, gender, and age.Our findings show that while deception detection can be performed in short texts even in the absence of a predetermined domain, gender and age prediction in deceptive texts is a challenging task.We further explore the linguistic differences in deceptive content that relate to deceivers gender and age and find evidence that both age and gender play an important role in people's word choices when fabricating lies.",./data/pdfs/EMNLP2015/Experiments in Open Domain Deception Detection.pdf,./data/imgs/EMNLP2015/Experiments in Open Domain Deception Detection.png,5d37364ceeb34010be818ad997746f1336356665,90.0,"{2025: 3, 2024: 3, 2023: 11, 2022: 12, 2021: 12, 2020: 15, 2019: 12, 2018: 12, 2017: 7, 2013: 1}",77.0,"{2024: 3, 2023: 7, 2022: 11, 2021: 9, 2020: 14, 2019: 12, 2018: 11, 2017: 10}"
18609,Discriminative Neural Sentence Modeling by Tree-Based Convolution,EMNLP,2015,"['Lili Mou', 'Hao Peng', 'Ge Li', 'Yan Xu', 'Lu Zhang', 'Zhi Jin']","This paper proposes a tree-based convolutional neural network (TBCNN) for discriminative sentence modeling.Our model leverages either constituency trees or dependency trees of sentences.The tree-based convolution process extracts sentences structural features, which are then aggregated by max pooling.Such architecture allows short propagation paths between the output layer and underlying feature detectors, enabling effective structural feature learning and extraction.We evaluate our models on two tasks: sentiment analysis and question classification.In both experiments, TBCNN outperforms previous state-of-the-art results, including existing neural networks and dedicated feature/rule engineering.We also make efforts to visualize the tree-based convolution process, shedding light on how our models work.",./data/pdfs/EMNLP2015/Discriminative Neural Sentence Modeling by Tree-Based Convolution.pdf,./data/imgs/EMNLP2015/Discriminative Neural Sentence Modeling by Tree-Based Convolution.png,bd32ebb9fac53a14202fb1a4f76ef96d1ff68c6c,85.0,"{2024: 1, 2023: 3, 2022: 4, 2021: 11, 2020: 11, 2019: 9, 2018: 16, 2017: 14, 2016: 14, 2015: 2}",124.0,"{2023: 1, 2022: 5, 2021: 4, 2020: 22, 2019: 10, 2018: 32, 2017: 17, 2016: 24, 2015: 8, 2014: 1}"
18734,Document Modeling with Gated Recurrent Neural Network for Sentiment Classification,EMNLP,2015,"['Duyu Tang', 'Bing Qin', 'Ting Liu']","Document level sentiment classification remains a challenge: encoding the intrinsic relations between sentences in the semantic meaning of a document. To address this, we introduce a neural network model to learn vector-based document representation in a unified, bottom-up fashion. The model first learns sentence representation with convolutional neural network or long short-term memory. Afterwards, semantics of sentences and their relations are adaptively encoded in document representation with gated recurrent neural network. We conduct document level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge. Experimental results show that: (1) our neural model shows superior performances over several state-of-the-art algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification. 1",./data/pdfs/EMNLP2015/Document Modeling with Gated Recurrent Neural Network for Sentiment Classification.pdf,./data/imgs/EMNLP2015/Document Modeling with Gated Recurrent Neural Network for Sentiment Classification.png,ecb5336bf7b54a62109f325e7152bb74c4c7f527,1469.0,"{2025: 14, 2024: 57, 2023: 113, 2022: 123, 2021: 168, 2020: 258, 2019: 267, 2018: 262, 2017: 135, 2016: 52, 2015: 14}",1354.0,"{2024: 17, 2023: 80, 2022: 92, 2021: 133, 2020: 239, 2019: 278, 2018: 273, 2017: 165, 2016: 66, 2015: 11}"
13000,Automatic Extraction of Implicit Interpretations from Modal Constructions,EMNLP,2016,"['Jordan Sanders', 'Eduardo Blanco']","This paper presents an approach to extract implicit interpretations from modal constructions.Importantly, our approach uses a deterministic procedure to normalize eventualities and generate potential interpretations.An annotation effort demonstrates that these interpretations are intuitive to humans and most modal constructions convey at least one interpretation.Experimental results show that the task is challenging but can be automated.",./data/pdfs/EMNLP2016/Automatic Extraction of Implicit Interpretations from Modal Constructions.pdf,./data/imgs/EMNLP2016/Automatic Extraction of Implicit Interpretations from Modal Constructions.png,813779418a613d1faecd7b1deb9b4456121a9b7e,1.0,{2018: 1},1.0,{2018: 1}
12891,The Effects of the Content of FOMC Communications on US Treasury Rates,EMNLP,2016,"['Chris Rohlfs', 'Sunandan Chakraborty', 'Lakshminarayanan Subramanian']",,./data/pdfs/EMNLP2016/The Effects of the Content of FOMC Communications on US Treasury Rates.pdf,./data/imgs/EMNLP2016/The Effects of the Content of FOMC Communications on US Treasury Rates.png,7983c187c6b42266a459c6de010b54482164258e,4.0,"{2023: 1, 2021: 2, 2019: 1}",9.0,"{2023: 3, 2022: 1, 2021: 1, 2019: 2, 2018: 2}"
12857,TweeTime : A Minimally Supervised Method for Recognizing and Normalizing Time Expressions in Twitter,EMNLP,2016,"['Jeniya Tabassum', 'Alan Ritter', 'Wei Xu']","We describe TweeTIME, a temporal tagger for recognizing and normalizing time expressions in Twitter.Most previous work in social media analysis has to rely on temporal resolvers that are designed for well-edited text, and therefore suffer from reduced performance due to domain mismatch.We present a minimally supervised method that learns from large quantities of unlabeled data and requires no hand-engineered rules or hand-annotated training corpora.TweeTIME achieves 0.68 F1 score on the end-to-end task of resolving date expressions, outperforming a broad range of state-of-the-art systems. 1",./data/pdfs/EMNLP2016/TweeTime : A Minimally Supervised Method for Recognizing and Normalizing Time Expressions in Twitter.pdf,./data/imgs/EMNLP2016/TweeTime : A Minimally Supervised Method for Recognizing and Normalizing Time Expressions in Twitter.png,0641bb2aa9820dc20befa1ff833331d702eb79e9,18.0,"{2023: 2, 2022: 1, 2021: 4, 2020: 1, 2019: 3, 2018: 4, 2017: 3}",16.0,"{2023: 2, 2022: 1, 2021: 1, 2019: 3, 2018: 4, 2017: 5}"
12906,BrainBench: A Brain-Image Test Suite for Distributional Semantic Models,EMNLP,2016,"['Haoyan Xu', 'Brian Murphy', 'Alona Fyshe']","The brain is the locus of our language ability, and so brain images can be used to ground linguistic theories.Here we introduce Brain-Bench, a lightweight system for testing distributional models of word semantics.We compare the performance of several models, and show that the performance on brain-image tasks differs from the performance on behavioral tasks.We release our benchmark test as part of a web service.",./data/pdfs/EMNLP2016/BrainBench: A Brain-Image Test Suite for Distributional Semantic Models.pdf,./data/imgs/EMNLP2016/BrainBench: A Brain-Image Test Suite for Distributional Semantic Models.png,95e0308c4dbc98f27c0645c7d87204c4d7be33af,33.0,"{2024: 1, 2023: 4, 2022: 2, 2021: 5, 2020: 1, 2019: 9, 2018: 10, 2017: 1}",27.0,"{2024: 3, 2023: 1, 2022: 1, 2021: 3, 2020: 4, 2019: 5, 2018: 7, 2017: 3}"
12895,AMR-to-text generation as a Traveling Salesman Problem,EMNLP,2016,"['Linfeng Song', 'Yue Zhang', 'Xiaochang Peng', 'Zhiguo Wang', 'Daniel Gildea']","The task of AMR-to-text generation is to generate grammatical text that sustains the semantic meaning for a given AMR graph.We attack the task by first partitioning the AMR graph into smaller fragments, and then generating the translation for each fragment, before finally deciding the order by solving an asymmetric generalized traveling salesman problem (AGTSP).A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution.The final model reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset.",./data/pdfs/EMNLP2016/AMR-to-text generation as a Traveling Salesman Problem.pdf,./data/imgs/EMNLP2016/AMR-to-text generation as a Traveling Salesman Problem.png,0e367d898c9701f09ec3205b39bb19aa677c751f,27.0,"{2023: 1, 2022: 2, 2021: 1, 2020: 3, 2019: 10, 2018: 5, 2017: 5}",32.0,"{2023: 1, 2022: 3, 2021: 1, 2020: 3, 2019: 8, 2018: 6, 2017: 10}"
12861,Global Neural CCG Parsing with Optimality Guarantees,EMNLP,2016,"['Kenton Lee', 'Mike Lewis', 'Luke Zettlemoyer']","We introduce the first global recursive neural parsing model with optimality guarantees during decoding. To support global features, we give up dynamic programs and instead search directly in the space of all possible subtrees. Although this space is exponentially large in the sentence length, we show it is possible to learn an efficient A* parser. We augment existing parsing models, which have informative bounds on the outside score, with a global model that has loose bounds but only needs to model non-local phenomena. The global model is trained with a new objective that encourages the parser to explore a tiny fraction of the search space. The approach is applied to CCG parsing, improving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal parse for 99.9% of held-out sentences, exploring on average only 190 subtrees.",./data/pdfs/EMNLP2016/Global Neural CCG Parsing with Optimality Guarantees.pdf,./data/imgs/EMNLP2016/Global Neural CCG Parsing with Optimality Guarantees.png,8602398403281dae0694b4e0488eb501d6db49ef,36.0,"{2023: 2, 2022: 1, 2021: 9, 2020: 2, 2019: 10, 2018: 4, 2017: 7, 2016: 1}",38.0,"{2024: 1, 2023: 1, 2021: 8, 2020: 2, 2019: 8, 2018: 6, 2017: 10, 2016: 2}"
12929,A Graph Degeneracy-based Approach to Keyword Extraction,EMNLP,2016,"['Antoine J.‐P. Tixier', 'Fragkiskos D. Malliaros', 'Michalis Vazirgiannis']","We operate a change of paradigm and hypothesize that keywords are more likely to be found among influential nodes of a graph-ofwords rather than among its nodes high on eigenvector-related centrality measures.To test this hypothesis, we introduce unsupervised techniques that capitalize on graph degeneracy.Our methods strongly and significantly outperform all baselines on two datasets (short and medium size documents), and reach best performance on the third one (long documents).",./data/pdfs/EMNLP2016/A Graph Degeneracy-based Approach to Keyword Extraction.pdf,./data/imgs/EMNLP2016/A Graph Degeneracy-based Approach to Keyword Extraction.png,0c22933a45594cec626678e828269e85e28294b0,84.0,"{2024: 4, 2023: 3, 2022: 12, 2021: 16, 2020: 11, 2019: 21, 2018: 8, 2017: 7, 2016: 2}",73.0,"{2023: 4, 2022: 7, 2021: 14, 2020: 15, 2019: 18, 2018: 9, 2017: 4, 2016: 2}"
12930,Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text,EMNLP,2016,"['Subhashini Venugopalan', 'Lisa Anne Hendricks', 'Raymond J. Mooney', 'Kate Saenko']","This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos.Specifically, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description.We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality.",./data/pdfs/EMNLP2016/Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text.pdf,./data/imgs/EMNLP2016/Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text.png,d1ffd519ff274517ec6fd014ae67af0d0c68a969,116.0,"{2025: 1, 2024: 1, 2023: 12, 2022: 8, 2021: 10, 2020: 16, 2019: 18, 2018: 21, 2017: 29}",116.0,"{2023: 5, 2022: 10, 2021: 11, 2020: 19, 2019: 15, 2018: 31, 2017: 18, 2016: 6, 2015: 1}"
13097,Multi-view Response Selection for Human-Computer Conversation,EMNLP,2016,"['Xiangyang Zhou', 'Daxiang Dong', 'Hua Wu', 'Shiqi Zhao', 'Dianhai Yu', 'Hao Tian', 'Xuan Liu', 'Rui Yan']",,./data/pdfs/EMNLP2016/Multi-view Response Selection for Human-Computer Conversation.pdf,./data/imgs/EMNLP2016/Multi-view Response Selection for Human-Computer Conversation.png,2e0bed618d023cad81eae218e69afce8bef8e4d6,251.0,"{2024: 9, 2023: 15, 2022: 24, 2021: 47, 2020: 48, 2019: 56, 2018: 38, 2017: 12, 2016: 2}",222.0,"{2024: 4, 2023: 16, 2022: 19, 2021: 37, 2020: 41, 2019: 51, 2018: 36, 2017: 15, 2016: 3}"
12956,Transfer Learning for Low-Resource Neural Machine Translation,EMNLP,2016,"['Barret Zoph', 'Deniz Yüret', 'Jonathan May', 'Kevin Knight']","The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages.We present a transfer learning method that significantly improves BLEU scores across a range of low-resource languages.Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training.Using our transfer learning method we improve baseline NMT models by an average of 5.6 BLEU on four low-resource language pairs.Ensembling and unknown word replacement add another 2 BLEU which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair.Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 BLEU, improving the state-of-the-art on low-resource machine translation.",./data/pdfs/EMNLP2016/Transfer Learning for Low-Resource Neural Machine Translation.pdf,./data/imgs/EMNLP2016/Transfer Learning for Low-Resource Neural Machine Translation.png,1cd7f2c74bd7ffb3a8b1527bec8795d0876a40b6,736.0,"{2025: 8, 2024: 38, 2023: 89, 2022: 58, 2021: 158, 2020: 159, 2019: 116, 2018: 70, 2017: 34, 2016: 5}",779.0,"{2024: 24, 2023: 99, 2022: 101, 2021: 144, 2020: 154, 2019: 107, 2018: 98, 2017: 42, 2016: 9, 2015: 1}"
17901,Dimensions of Interpersonal Relationships: Corpus and Experiments,EMNLP,2017,"['Farzana Rashid', 'Eduardo Blanco']","This paper presents a corpus and experiments to determine dimensions of interpersonal relationships. We define a set of dimensions heavily inspired by work in social science. We create a corpus by retrieving pairs of people, and then annotating dimensions for their relationships. A corpus analysis shows that dimensions can be annotated reliably. Experimental results show that given a pair of people, values to dimensions can be assigned automatically.",./data/pdfs/EMNLP2017/Dimensions of Interpersonal Relationships: Corpus and Experiments.pdf,./data/imgs/EMNLP2017/Dimensions of Interpersonal Relationships: Corpus and Experiments.png,c39b7105863a59b89e13561280d2889ba1c83ac7,7.0,"{2021: 4, 2020: 1, 2019: 1, 2018: 1}",7.0,"{2022: 1, 2021: 2, 2020: 1, 2019: 2, 2018: 1}"
17944,A Simple Language Model based on PMI Matrix Approximations,EMNLP,2017,"['Oren Melamud', 'Ido Dagan', 'Jacob Goldberger']","In this study, we introduce a new approach for learning language models by training them to estimate word-context pointwise mutual information (PMI), and then deriving the desired conditional probabilities from PMI at test time. Specifically, we show that with minor modifications to word2vec’s algorithm, we get principled language models that are closely related to the well-established Noise Contrastive Estimation (NCE) based language models. A compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in word2vec to learn word embeddings.",./data/pdfs/EMNLP2017/A Simple Language Model based on PMI Matrix Approximations.pdf,./data/imgs/EMNLP2017/A Simple Language Model based on PMI Matrix Approximations.png,9557aa7ce18555d21f238a90057c9295a5b796f6,8.0,"{2021: 2, 2020: 2, 2019: 3, 2018: 1}",10.0,"{2021: 2, 2020: 1, 2019: 1, 2018: 6}"
17912,A Factored Neural Network Model for Characterizing Online Discussions in Vector Space,EMNLP,2017,"['Hao Cheng', 'Hao Fang', 'Mari Ostendorf']","We develop a novel factored neural model that learns comment embeddings in an unsupervised way leveraging the structure of distributional context in online discussion forums. The model links different context with related language factors in the embedding space, providing a way to interpret the factored embeddings. Evaluated on a community endorsement prediction task using a large collection of topic-varying Reddit discussions, the factored embeddings consistently achieve improvement over other text representations. Qualitative analysis shows that the model captures community style and topic, as well as response trigger patterns.",./data/pdfs/EMNLP2017/A Factored Neural Network Model for Characterizing Online Discussions in Vector Space.pdf,./data/imgs/EMNLP2017/A Factored Neural Network Model for Characterizing Online Discussions in Vector Space.png,ee127ccc3c7f9865131694759bcac0950a99a620,17.0,"{2024: 1, 2023: 1, 2022: 1, 2021: 2, 2020: 2, 2019: 5, 2018: 5}",15.0,"{2023: 1, 2022: 1, 2021: 2, 2020: 2, 2019: 4, 2018: 5}"
17909,Bootstrapping incremental dialogue systems from minimal data: the generalisation power of dialogue grammars,EMNLP,2017,"['Arash Eshghi', 'Igor Shalyminov', 'Oliver Lemon']","We investigate an end-to-end method for automatically inducing task-based dialogue systems from small amounts of unannotated dialogue data. It combines an incremental semantic grammar - Dynamic Syntax and Type Theory with Records (DS-TTR) - with Reinforcement Learning (RL), where language generation and dialogue management are a joint decision problem. The systems thus produced are incremental: dialogues are processed word-by-word, shown previously to be essential in supporting natural, spontaneous dialogue. We hypothesised that the rich linguistic knowledge within the grammar should enable a combinatorially large number of dialogue variations to be processed, even when trained on very few dialogues. Our experiments show that our model can process 74% of the Facebook AI bAbI dataset even when trained on only 0.13% of the data (5 dialogues). It can in addition process 65% of bAbI+, a corpus we created by systematically adding incremental dialogue phenomena such as restarts and self-corrections to bAbI. We compare our model with a state-of-the-art retrieval model, MEMN2N. We find that, in terms of semantic accuracy, the MEMN2N model shows very poor robustness to the bAbI+ transformations even when trained on the full bAbI dataset.",./data/pdfs/EMNLP2017/Bootstrapping incremental dialogue systems from minimal data: the generalisation power of dialogue grammars.pdf,./data/imgs/EMNLP2017/Bootstrapping incremental dialogue systems from minimal data: the generalisation power of dialogue grammars.png,5b05e94804243071088d961e548819ca714f6b18,22.0,"{2023: 2, 2022: 1, 2021: 4, 2020: 7, 2019: 4, 2018: 4}",25.0,"{2023: 1, 2022: 1, 2021: 3, 2020: 9, 2019: 3, 2018: 7, 2017: 1}"
17974,CRF Autoencoder for Unsupervised Dependency Parsing,EMNLP,2017,"['Jiong Cai', 'Yong Jiang', 'Kewei Tu']","Unsupervised dependency parsing, which tries to discover linguistic dependency structures from unannotated data, is a very challenging task. Almost all previous work on this task focuses on learning generative models. In this paper, we develop an unsupervised dependency parsing model based on the CRF autoencoder. The encoder part of our model is discriminative and globally normalized which allows us to use rich features as well as universal linguistic priors. We propose an exact algorithm for parsing as well as a tractable learning algorithm. We evaluated the performance of our model on eight multilingual treebanks and found that our model achieved comparable performance with state-of-the-art approaches.",./data/pdfs/EMNLP2017/CRF Autoencoder for Unsupervised Dependency Parsing.pdf,./data/imgs/EMNLP2017/CRF Autoencoder for Unsupervised Dependency Parsing.png,a1733d564c3283199aa23cf68fdf9e944f0c5359,29.0,"{2024: 1, 2022: 2, 2021: 6, 2020: 11, 2019: 8, 2018: 1}",33.0,"{2022: 4, 2021: 5, 2020: 11, 2019: 6, 2018: 7}"
17998,Identifying attack and support argumentative relations using deep learning,EMNLP,2017,"['Oana Cocarascu', 'Francesca Toni']","We propose a deep learning architecture to capture argumentative relations of attack and support from one piece of text to another, of the kind that naturally occur in a debate. The architecture uses two (unidirectional or bidirectional) Long Short-Term Memory networks and (trained or non-trained) word embeddings, and allows to considerably improve upon existing techniques that use syntactic features and supervised classifiers for the same form of (relation-based) argument mining.",./data/pdfs/EMNLP2017/Identifying attack and support argumentative relations using deep learning.pdf,./data/imgs/EMNLP2017/Identifying attack and support argumentative relations using deep learning.png,4cec33808c7ff7194304c36f1a6fe22bca3a2815,57.0,"{2025: 2, 2024: 3, 2023: 4, 2022: 8, 2021: 5, 2020: 12, 2019: 14, 2018: 9}",54.0,"{2024: 2, 2023: 6, 2022: 10, 2021: 5, 2020: 11, 2019: 11, 2018: 9}"
17946,Reference-Aware Language Models,EMNLP,2017,"['Zichao Yang', 'Phil Blunsom', 'Chris Dyer', 'Ling Wang']","We propose a general class of language models that treat reference as discrete stochastic latent variables. This decision allows for the creation of entity mentions by accessing external databases of referents (required by, e.g., dialogue generation) or past internal state (required to explicitly model coreferentiality). Beyond simple copying, our coreference model can additionally refer to a referent using varied mention forms (e.g., a reference to ""Jane"" can be realized as ""she""), a characteristic feature of reference in natural languages. Experiments on three representative applications show our model variants outperform models based on deterministic attention and standard language modeling baselines.",./data/pdfs/EMNLP2017/Reference-Aware Language Models.pdf,./data/imgs/EMNLP2017/Reference-Aware Language Models.png,26e9eb44ed8065122d37b0c429a8d341bfeea9a5,79.0,"{2024: 1, 2023: 1, 2022: 4, 2021: 7, 2020: 15, 2019: 30, 2018: 12, 2017: 9}",79.0,"{2024: 3, 2023: 1, 2022: 6, 2021: 5, 2020: 11, 2019: 24, 2018: 15, 2017: 14}"
18094,"Magnets for Sarcasm: Making Sarcasm Detection Timely, Contextual and Very Personal",EMNLP,2017,"['Aniruddha Ghosh', 'Tony Veale']","Sarcasm is a pervasive phenomenon in social media, permitting the concise communication of meaning, affect and attitude. Concision requires wit to produce and wit to understand, which demands from each party knowledge of norms, context and a speaker’s mindset. Insight into a speaker’s psychological profile at the time of production is a valuable source of context for sarcasm detection. Using a neural architecture, we show significant gains in detection accuracy when knowledge of the speaker’s mood at the time of production can be inferred. Our focus is on sarcasm detection on Twitter, and show that the mood exhibited by a speaker over tweets leading up to a new post is as useful a cue for sarcasm as the topical context of the post itself. The work opens the door to an empirical exploration not just of sarcasm in text but of the sarcastic state of mind.","./data/pdfs/EMNLP2017/Magnets for Sarcasm: Making Sarcasm Detection Timely, Contextual and Very Personal.pdf","./data/imgs/EMNLP2017/Magnets for Sarcasm: Making Sarcasm Detection Timely, Contextual and Very Personal.png",a9dc6ce5e69b015aa7b0557ede03b5decfcdf778,143.0,"{2025: 2, 2024: 17, 2023: 10, 2022: 20, 2021: 23, 2020: 46, 2019: 10, 2018: 15}",91.0,"{2024: 8, 2023: 6, 2022: 16, 2021: 16, 2020: 22, 2019: 12, 2018: 11}"
17868,MinIE: Minimizing Facts in Open Information Extraction,EMNLP,2017,"['Kiril Gashteovski', 'Rainer Gemulla', 'Luciano Del Corro']","The goal of Open Information Extraction (OIE) is to extract surface relations and their arguments from natural-language text in an unsupervised, domain-independent manner. In this paper, we propose MinIE, an OIE system that aims to provide useful, compact extractions with high precision and recall. MinIE approaches these goals by (1) representing information about polarity, modality, attribution, and quantities with semantic annotations instead of in the actual extraction, and (2) identifying and removing parts that are considered overly specific. We conducted an experimental study with several real-world datasets and found that MinIE achieves competitive or higher precision and recall than most prior systems, while at the same time producing shorter, semantically enriched extractions.",./data/pdfs/EMNLP2017/MinIE: Minimizing Facts in Open Information Extraction.pdf,./data/imgs/EMNLP2017/MinIE: Minimizing Facts in Open Information Extraction.png,1e94f1f54597f7f65300712ae97035e50ff3bc9a,150.0,"{2024: 12, 2023: 22, 2022: 21, 2021: 33, 2020: 22, 2019: 23, 2018: 16, 2017: 1}",142.0,"{2024: 5, 2023: 22, 2022: 20, 2021: 32, 2020: 20, 2019: 20, 2018: 21, 2017: 2}"
17979,"Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",EMNLP,2017,"['Bjarke Felbo', 'Alan Mislove', 'Anders Søgaard', 'Iyad Rahwan', 'Sune Lehmann']","NLP tasks are often limited by scarcity of manually annotated data. In social media sentiment analysis and related tasks, researchers have therefore used binarized emoticons and specific hashtags as forms of distant supervision. Our paper shows that by extending the distant supervision to a more diverse set of noisy labels, the models can learn richer representations. Through emoji prediction on a dataset of 1246 million tweets containing one of 64 common emojis we obtain state-of-the-art performance on 8 benchmark datasets within sentiment, emotion and sarcasm detection using a single pretrained model. Our analyses confirm that the diversity of our emotional labels yield a performance improvement over previous distant supervision approaches.","./data/pdfs/EMNLP2017/Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm.pdf","./data/imgs/EMNLP2017/Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm.png",2a8b6f990a5ddf0122aae82a46359b03031f302b,766.0,"{2025: 10, 2024: 44, 2023: 68, 2022: 100, 2021: 134, 2020: 149, 2019: 144, 2018: 110, 2017: 3, 2016: 1}",679.0,"{2024: 19, 2023: 58, 2022: 93, 2021: 114, 2020: 143, 2019: 140, 2018: 107, 2017: 4, 2016: 1}"
17648,Disambiguated skip-gram model,EMNLP,2018,"['Karol Grzegorczyk', 'Marcin Kurdziel']","We present disambiguated skip-gram: a neural-probabilistic model for learning multi-sense distributed representations of words. Disambiguated skip-gram jointly estimates a skip-gram-like context word prediction model and a word sense disambiguation model. Unlike previous probabilistic models for learning multi-sense word embeddings, disambiguated skip-gram is end-to-end differentiable and can be interpreted as a simple feed-forward neural network. We also introduce an effective pruning strategy for the embeddings learned by disambiguated skip-gram. This allows us to control the granularity of representations learned by our model. In experimental evaluation disambiguated skip-gram improves state-of-the are results in several word sense induction benchmarks.",./data/pdfs/EMNLP2018/Disambiguated skip-gram model.pdf,./data/imgs/EMNLP2018/Disambiguated skip-gram model.png,d836d1c7c1a492ee000f71150c73a6d4f863cec4,3.0,"{2023: 1, 2021: 2}",2.0,{2023: 2}
17373,Automatic Pyramid Evaluation Exploiting EDU-based Extractive Reference Summaries,EMNLP,2018,"['Tsutomu Hirao', 'Hidetaka Kamigaito', 'Masaaki Nagata']","This paper tackles automation of the pyramid method, a reliable manual evaluation framework. To construct a pyramid, we transform human-made reference summaries into extractive reference summaries that consist of Elementary Discourse Units (EDUs) obtained from source documents and then weight every EDU by counting the number of extractive reference summaries that contain the EDU. A summary is scored by the correspondences between EDUs in the summary and those in the pyramid. Experiments on DUC and TAC data sets show that our methods strongly correlate with various manual evaluations.",./data/pdfs/EMNLP2018/Automatic Pyramid Evaluation Exploiting EDU-based Extractive Reference Summaries.pdf,./data/imgs/EMNLP2018/Automatic Pyramid Evaluation Exploiting EDU-based Extractive Reference Summaries.png,85dd5825592f7d152f98862df0f05cdab9057beb,15.0,"{2023: 1, 2022: 2, 2021: 3, 2020: 5, 2019: 4}",11.0,"{2024: 1, 2023: 1, 2021: 2, 2020: 5, 2019: 2}"
17367,A Nil-Aware Answer Extraction Framework for Question Answering,EMNLP,2018,"['Souvik Kundu', 'Hwee Tou Ng']","Recently, there has been a surge of interest in reading comprehension-based (RC) question answering (QA). However, current approaches suffer from an impractical assumption that every question has a valid answer in the associated passage. A practical QA system must possess the ability to determine whether a valid answer exists in a given text passage. In this paper, we focus on developing QA systems that can extract an answer for a question if and only if the associated passage contains an answer. If the associated passage does not contain any valid answer, the QA system will correctly return Nil. We propose a novel nil-aware answer span extraction framework that is capable of returning Nil or a text span from the associated passage as an answer in a single step. We show that our proposed framework can be easily integrated with several recently proposed QA models developed for reading comprehension and can be trained in an end-to-end fashion. Our proposed nil-aware answer extraction neural network decomposes pieces of evidence into relevant and irrelevant parts and then combines them to infer the existence of any answer. Experiments on the NewsQA dataset show that the integration of our proposed framework significantly outperforms several strong baseline systems that use pipeline or threshold-based approaches.",./data/pdfs/EMNLP2018/A Nil-Aware Answer Extraction Framework for Question Answering.pdf,./data/imgs/EMNLP2018/A Nil-Aware Answer Extraction Framework for Question Answering.png,b561e606a74885a1c0e768874a6436e5c995eea4,26.0,"{2024: 1, 2023: 3, 2022: 2, 2021: 1, 2020: 8, 2019: 10, 2018: 1}",18.0,"{2023: 2, 2022: 2, 2021: 1, 2020: 6, 2019: 6, 2018: 1}"
17721,Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence Models,EMNLP,2018,"['Daniel Watson', 'Nasser Zalmout', 'Nizar Habash']","Text normalization is an important enabling technology for several NLP tasks. Recently, neural-network-based approaches have outperformed well-established models in this task. However, in languages other than English, there has been little exploration in this direction. Both the scarcity of annotated data and the complexity of the language increase the difficulty of the problem. To address these challenges, we use a sequence-to-sequence model with character-based attention, which in addition to its self-learned character embeddings, uses word embeddings pre-trained with an approach that also models subword information. This provides the neural model with access to more linguistic information especially suitable for text normalization, without large parallel corpora. We show that providing the model with word-level features bridges the gap for the neural network approach to achieve a state-of-the-art F1 score on a standard Arabic language correction shared task dataset.",./data/pdfs/EMNLP2018/Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence Models.pdf,./data/imgs/EMNLP2018/Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence Models.png,a5599cd0ab66dc1b43c531d8a6ceef74c5374a22,23.0,"{2023: 5, 2022: 2, 2021: 2, 2020: 10, 2019: 4}",25.0,"{2023: 5, 2022: 1, 2021: 2, 2020: 10, 2019: 7}"
17818,Game-Based Video-Context Dialogue,EMNLP,2018,"['Ramakanth Pasunuru', 'Mohit Bansal']","Current dialogue systems focus more on textual and speech context knowledge and are usually based on two speakers. Some recent work has investigated static image-based dialogue. However, several real-world human interactions also involve dynamic visual context (similar to videos) as well as dialogue exchanges among multiple speakers. To move closer towards such multimodal conversational skills and visually-situated applications, we introduce a new video-context, many-speaker dialogue dataset based on live-broadcast soccer game videos and chats from Twitch.tv. This challenging testbed allows us to develop visually-grounded dialogue models that should generate relevant temporal and spatial event language from the live video, while also being relevant to the chat history. For strong baselines, we also present several discriminative and generative models, e.g., based on tridirectional attention flow (TriDAF). We evaluate these models via retrieval ranking-recall, automatic phrase-matching metrics, as well as human evaluation studies. We also present dataset analyses, model ablations, and visualizations to understand the contribution of different modalities and model components.",./data/pdfs/EMNLP2018/Game-Based Video-Context Dialogue.pdf,./data/imgs/EMNLP2018/Game-Based Video-Context Dialogue.png,667a6eea4c3039d4d1bde2ebf4f2fe8bcfa4af23,21.0,"{2024: 1, 2023: 4, 2022: 2, 2021: 6, 2020: 5, 2019: 3}",32.0,"{2024: 1, 2023: 3, 2022: 7, 2021: 9, 2020: 1, 2019: 7, 2018: 4}"
17772,NORMA: Neighborhood Sensitive Maps for Multilingual Word Embeddings,EMNLP,2018,['Ndapa Nakashole'],"Inducing multilingual word embeddings by learning a linear map between embedding spaces of different languages achieves remarkable accuracy on related languages. However, accuracy drops substantially when translating between distant languages. Given that languages exhibit differences in vocabulary, grammar, written form, or syntax, one would expect that embedding spaces of different languages have different structures especially for distant languages. With the goal of capturing such differences, we propose a method for learning neighborhood sensitive maps, NORMA. Our experiments show that NORMA outperforms current state-of-the-art methods for word translation between distant languages.",./data/pdfs/EMNLP2018/NORMA: Neighborhood Sensitive Maps for Multilingual Word Embeddings.pdf,./data/imgs/EMNLP2018/NORMA: Neighborhood Sensitive Maps for Multilingual Word Embeddings.png,5f6878bc8aeaec45108933dbde11384efbc13db1,58.0,"{2025: 1, 2024: 1, 2022: 1, 2021: 5, 2020: 32, 2019: 16, 2018: 2}",43.0,"{2024: 2, 2022: 1, 2021: 2, 2020: 23, 2019: 12, 2018: 2, 2017: 1}"
17690,Learning Sentiment Memories for Sentiment Modification without Parallel Data,EMNLP,2018,"['Yi Zhang', 'Jingjing Xu', 'Pengcheng Yang', 'Xu Sun']","The task of sentiment modification requires reversing the sentiment of the input and preserving the sentiment-independent content. However, aligned sentences with the same content but different sentiments are usually unavailable. Due to the lack of such parallel data, it is hard to extract sentiment independent content and reverse the sentiment in an unsupervised way. Previous work usually can not reconcile sentiment transformation and content preservation. In this paper, motivated by the fact the non-emotional context (e.g., ""staff"") provides strong cues for the occurrence of emotional words (e.g., ""friendly""), we propose a novel method that automatically extracts appropriate sentiment information from learned sentiment memories according to the specific context. Experiments show that our method substantially improves the content preservation degree and achieves the state-of-the-art performance.",./data/pdfs/EMNLP2018/Learning Sentiment Memories for Sentiment Modification without Parallel Data.pdf,./data/imgs/EMNLP2018/Learning Sentiment Memories for Sentiment Modification without Parallel Data.png,054c5e07cdef8fb077ee19ff582fef3689e5e3e9,58.0,"{2024: 4, 2023: 5, 2022: 2, 2021: 11, 2020: 14, 2019: 21, 2018: 1}",54.0,"{2024: 1, 2023: 5, 2022: 4, 2021: 11, 2020: 14, 2019: 18, 2018: 1}"
17287,The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification,EMNLP,2018,"['Chen Jing', 'Qingcai Chen', 'Xin Liu', 'Haijun Yang', 'Daohe Lu', 'Buzhou Tang']","This paper introduces the Bank Question (BQ) corpus, a Chinese corpus for sentence semantic equivalence identification (SSEI). The BQ corpus contains 120,000 question pairs from 1-year online bank custom service logs. To efficiently process and annotate questions from such a large scale of logs, this paper proposes a clustering based annotation method to achieve questions with the same intent. First, the deduplicated questions with the same answer are clustered into stacks by the Word Mover’s Distance (WMD) based Affinity Propagation (AP) algorithm. Then, the annotators are asked to assign the clustered questions into different intent categories. Finally, the positive and negative question pairs for SSEI are selected in the same intent category and between different intent categories respectively. We also present six SSEI benchmark performance on our corpus, including state-of-the-art algorithms. As the largest manually annotated public Chinese SSEI corpus in the bank domain, the BQ corpus is not only useful for Chinese question semantic matching research, but also a significant resource for cross-lingual and cross-domain SSEI research. The corpus is available in public.",./data/pdfs/EMNLP2018/The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification.pdf,./data/imgs/EMNLP2018/The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification.png,7afb83134d5b7914131e10b229d30dc2593266f6,106.0,"{2025: 2, 2024: 16, 2023: 23, 2022: 22, 2021: 24, 2020: 12, 2019: 7}",95.0,"{2024: 7, 2023: 24, 2022: 28, 2021: 20, 2020: 9, 2019: 7}"
17439,Contextual Inter-modal Attention for Multi-modal Sentiment Analysis,EMNLP,2018,"['Deepanway Ghosal', 'Md Shad Akhtar', 'Dushyant Singh Chauhan', 'Soujanya Poria', 'Asif Ekbal', 'Pushpak Bhattacharyya']","Multi-modal sentiment analysis offers various challenges, one being the effective combination of different input modalities, namely text, visual and acoustic. In this paper, we propose a recurrent neural network based multi-modal attention framework that leverages the contextual information for utterance-level sentiment prediction. The proposed approach applies attention on multi-modal multi-utterance representations and tries to learn the contributing features amongst them. We evaluate our proposed approach on two multi-modal sentiment analysis benchmark datasets, viz. CMU Multi-modal Opinion-level Sentiment Intensity (CMU-MOSI) corpus and the recently released CMU Multi-modal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) corpus. Evaluation results show the effectiveness of our proposed approach with the accuracies of 82.31% and 79.80% for the MOSI and MOSEI datasets, respectively. These are approximately 2 and 1 points performance improvement over the state-of-the-art models for the datasets.",./data/pdfs/EMNLP2018/Contextual Inter-modal Attention for Multi-modal Sentiment Analysis.pdf,./data/imgs/EMNLP2018/Contextual Inter-modal Attention for Multi-modal Sentiment Analysis.png,ec99eca7c86601b8c909147f8caa8c35975bc44e,154.0,"{2025: 6, 2024: 22, 2023: 43, 2022: 26, 2021: 18, 2020: 29, 2019: 9}",124.0,"{2024: 7, 2023: 37, 2022: 27, 2021: 19, 2020: 26, 2019: 8}"
17508,Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study,EMNLP,2018,"['Aditya Siddhant', 'Zachary C. Lipton']","Several recent papers investigate Active Learning (AL) for mitigating the data dependence of deep learning for natural language processing. However, the applicability of AL to real-world problems remains an open question. While in supervised learning, practitioners can try many different methods, evaluating each against a validation set before selecting a model, AL affords no such luxury. Over the course of one AL run, an agent annotates its dataset exhausting its labeling budget. Thus, given a new task, we have no opportunity to compare models and acquisition functions. This paper provides a large-scale empirical study of deep active learning, addressing multiple tasks and, for each, multiple datasets, multiple models, and a full suite of acquisition functions. We find that across all settings, Bayesian active learning by disagreement, using uncertainty estimates provided either by Dropout or Bayes-by-Backprop significantly improves over i.i.d. baselines and usually outperforms classic uncertainty sampling.",./data/pdfs/EMNLP2018/Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study.pdf,./data/imgs/EMNLP2018/Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study.png,838fbfd9066dbbac6c10059c5b183046fb1cd9d1,164.0,"{2025: 2, 2024: 20, 2023: 22, 2022: 21, 2021: 43, 2020: 33, 2019: 20, 2018: 1}",186.0,"{2024: 13, 2023: 35, 2022: 31, 2021: 46, 2020: 31, 2019: 24, 2018: 6}"
16911,Don’t Just Scratch the Surface: Enhancing Word Representations for Korean with Hanja,EMNLP,2019,"['Kang Min Yoo', 'Taeuk Kim', 'Sang‐Goo Lee']","Kang Min Yoo, Taeuk Kim, Sang-goo Lee. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",./data/pdfs/EMNLP2019/Don’t Just Scratch the Surface: Enhancing Word Representations for Korean with Hanja.pdf,./data/imgs/EMNLP2019/Don’t Just Scratch the Surface: Enhancing Word Representations for Korean with Hanja.png,016a18626ca037f24359ad628504c54f0477bdd9,1.0,{2023: 1},2.0,"{2023: 1, 2020: 1}"
16923,The Challenges of Optimizing Machine Translation for Low Resource Cross-Language Information Retrieval,EMNLP,2019,"['Constantine Lignos', 'Daniel Cohen', 'Yen-Chieh Lien', 'Pratik Mehta', 'W. Bruce Croft', 'S.L. Miller']","Constantine Lignos, Daniel Cohen, Yen-Chieh Lien, Pratik Mehta, W. Bruce Croft, Scott Miller. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",./data/pdfs/EMNLP2019/The Challenges of Optimizing Machine Translation for Low Resource Cross-Language Information Retrieval.pdf,./data/imgs/EMNLP2019/The Challenges of Optimizing Machine Translation for Low Resource Cross-Language Information Retrieval.png,89026a78f53f7cc7470ac7caebf395efa0243ccb,3.0,"{2023: 1, 2022: 1, 2021: 1}",7.0,"{2023: 2, 2022: 2, 2021: 1, 2020: 2}"
16747,Multiplex Word Embeddings for Selectional Preference Acquisition,EMNLP,2019,"['Hongming Zhang', 'Jiaxin Bai', 'Yan Song', 'Kun Xu', 'Changlong Yu', 'Yangqiu Song', 'Wilfred Ng', 'Dong Yu']","Hongming Zhang, Jiaxin Bai, Yan Song, Kun Xu, Changlong Yu, Yangqiu Song, Wilfred Ng, Dong Yu. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",./data/pdfs/EMNLP2019/Multiplex Word Embeddings for Selectional Preference Acquisition.pdf,./data/imgs/EMNLP2019/Multiplex Word Embeddings for Selectional Preference Acquisition.png,61c789d5e61df832fc86005c6ede14fd86b556cc,13.0,"{2023: 4, 2022: 1, 2021: 3, 2020: 5}",16.0,"{2023: 4, 2022: 3, 2021: 3, 2020: 6}"
16779,Leveraging Medical Literature for Section Prediction in Electronic Health Records,EMNLP,2019,"['Sara Rosenthal', 'Ken Barker', 'Zhicheng Liang']","Sara Rosenthal, Ken Barker, Zhicheng Liang. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",./data/pdfs/EMNLP2019/Leveraging Medical Literature for Section Prediction in Electronic Health Records.pdf,./data/imgs/EMNLP2019/Leveraging Medical Literature for Section Prediction in Electronic Health Records.png,64f10b37681250110227e8953459e0db55f15a98,21.0,"{2025: 1, 2024: 2, 2023: 4, 2022: 3, 2021: 8, 2020: 3}",22.0,"{2024: 1, 2023: 10, 2022: 1, 2021: 8, 2020: 2}"
17257,Improving Relation Extraction with Knowledge-attention,EMNLP,2019,"['Pengfei Li', 'Kezhi Mao', 'Xuefeng Yang', 'Qi Li']","Pengfei Li, Kezhi Mao, Xuefeng Yang, Qi Li. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",./data/pdfs/EMNLP2019/Improving Relation Extraction with Knowledge-attention.pdf,./data/imgs/EMNLP2019/Improving Relation Extraction with Knowledge-attention.png,1dea10566757f53f32583a3a442c71759b292895,42.0,"{2024: 4, 2023: 6, 2022: 4, 2021: 19, 2020: 9}",30.0,"{2024: 2, 2023: 3, 2022: 5, 2021: 9, 2020: 11}"
17198,Modeling Multi-mapping Relations for Precise Cross-lingual Entity Alignment,EMNLP,2019,"['Xiaofei Shi', 'Yanghua Xiao']","Xiaofei Shi, Yanghua Xiao. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",./data/pdfs/EMNLP2019/Modeling Multi-mapping Relations for Precise Cross-lingual Entity Alignment.pdf,./data/imgs/EMNLP2019/Modeling Multi-mapping Relations for Precise Cross-lingual Entity Alignment.png,67d8d17d06fa49760dfdd4112207b39fa52c243e,40.0,"{2025: 1, 2024: 4, 2023: 5, 2022: 7, 2021: 17, 2020: 6}",42.0,"{2024: 6, 2023: 3, 2022: 10, 2021: 14, 2020: 9}"
16910,Out-of-Domain Detection for Low-Resource Text Classification Tasks,EMNLP,2019,"['Ming Tan', 'Yang Yu', 'Haoyu Wang', 'Dakuo Wang', 'Saloni Potdar', 'Shiyu Chang', 'Mo Yu']","Ming Tan, Yang Yu, Haoyu Wang, Dakuo Wang, Saloni Potdar, Shiyu Chang, Mo Yu. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",./data/pdfs/EMNLP2019/Out-of-Domain Detection for Low-Resource Text Classification Tasks.pdf,./data/imgs/EMNLP2019/Out-of-Domain Detection for Low-Resource Text Classification Tasks.png,7e870b2feda462bc4d11ddbb78fab911d02d61dc,40.0,"{2025: 1, 2024: 2, 2023: 7, 2022: 9, 2021: 16, 2020: 5}",52.0,"{2024: 5, 2023: 12, 2022: 10, 2021: 16, 2020: 9}"
16700,Simple and Effective Noisy Channel Modeling for Neural Machine Translation,EMNLP,2019,"['Kyra Yee', 'Yann Dauphin', 'Michael Auli']","Kyra Yee, Yann Dauphin, Michael Auli. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",./data/pdfs/EMNLP2019/Simple and Effective Noisy Channel Modeling for Neural Machine Translation.pdf,./data/imgs/EMNLP2019/Simple and Effective Noisy Channel Modeling for Neural Machine Translation.png,c7b361427a54b309caea8ef69ed59b50cd54ac67,54.0,"{2024: 2, 2023: 6, 2022: 9, 2021: 10, 2020: 20, 2019: 7}",73.0,"{2024: 5, 2023: 9, 2022: 18, 2021: 17, 2020: 19, 2019: 5}"
16988,A Discrete Hard EM Approach for Weakly Supervised Question Answering,EMNLP,2019,"['Sewon Min', 'Danqi Chen', 'Hannaneh Hajishirzi', 'Luke Zettlemoyer']","Sewon Min, Danqi Chen, Hannaneh Hajishirzi, Luke Zettlemoyer. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",./data/pdfs/EMNLP2019/A Discrete Hard EM Approach for Weakly Supervised Question Answering.pdf,./data/imgs/EMNLP2019/A Discrete Hard EM Approach for Weakly Supervised Question Answering.png,30eff53e981695c7296d258b8dc44b4c7b482a0c,160.0,"{2024: 4, 2023: 19, 2022: 23, 2021: 61, 2020: 51, 2019: 2}",153.0,"{2024: 6, 2023: 22, 2022: 29, 2021: 47, 2020: 43, 2019: 6}"
16886,PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification,EMNLP,2019,"['Yinfei Yang', 'Yuan Zhang', 'Chris Tar', 'Jason Baldridge']","Yinfei Yang, Yuan Zhang, Chris Tar, Jason Baldridge. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",./data/pdfs/EMNLP2019/PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification.pdf,./data/imgs/EMNLP2019/PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification.png,04a7021fe6be6bddcfae476493fcc7571e7c613c,225.0,"{2025: 3, 2024: 12, 2023: 46, 2022: 47, 2021: 68, 2020: 44, 2019: 3}",308.0,"{2024: 50, 2023: 70, 2022: 82, 2021: 65, 2020: 38, 2019: 3}"
16219,Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT,EMNLP,2020,"['Rik van Noord', 'Antonio Toral', 'Johan Bos']","We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing. Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a fully separate encoder, with improvements that are robust to different language models, languages and data sets. For English, these improvements are larger than adding individual sources of linguistic information or adding non-contextual embeddings. A new method of analysis based on semantic tags demonstrates that the character-level representations improve performance across a subset of selected semantic phenomena.",./data/pdfs/EMNLP2020/Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT.pdf,./data/imgs/EMNLP2020/Character-level Representations Improve DRS-based Semantic Parsing Even in the Age of BERT.png,,17.0,"{2025: 1, 2024: 1, 2023: 4, 2022: 5, 2021: 5, 2020: 1}",0.0,{}
16057,Multi-resolution Annotations for Emoji Prediction,EMNLP,2020,"['Weicheng Ma', 'Ruibo Liu', 'Lili Wang', 'Soroush Vosoughi']","Emojis are able to express various linguistic components, including emotions, sentiments, events, etc. Predicting the proper emojis associated with text provides a way to summarize the text accurately, and it has been proven to be a good auxiliary task to many Natural Language Understanding (NLU) tasks. Labels in existing emoji prediction datasets are all passage-based and are usually under the multi-class classification setting. However, in many cases, one single emoji cannot fully cover the theme of a piece of text. It is thus useful to infer the part of text related to each emoji. The lack of multi-label and aspect-level emoji prediction datasets is one of the bottlenecks for this task. This paper annotates an emoji prediction dataset with passage-level multi-class/multi-label, and aspect-level multi-class annotations. We also present a novel annotation method with which we generate the aspect-level annotations. The annotations are generated heuristically, taking advantage of the self-attention mechanism in Transformer networks. We validate the annotations both automatically and manually to ensure their quality. We also benchmark the dataset with a pre-trained BERT model.",./data/pdfs/EMNLP2020/Multi-resolution Annotations for Emoji Prediction.pdf,./data/imgs/EMNLP2020/Multi-resolution Annotations for Emoji Prediction.png,641250e235d81e5b5c0023c32e71731aa0b0027c,6.0,"{2024: 2, 2022: 2, 2021: 2}",6.0,"{2024: 1, 2023: 1, 2022: 2, 2021: 2}"
16215,Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze,EMNLP,2020,"['Ece Takmaz', 'Sandro Pezzelle', 'Lisa Beinborn', 'Raquel Fernández']","When speakers describe an image, they tend to look at objects before mentioning them. In this paper, we investigate such sequential cross-modal alignment by modelling the image description generation process computationally. We take as our starting point a state-of-the-art image captioning system and develop several model variants that exploit information from human gaze patterns recorded during language production. In particular, we propose the first approach to image description generation where visual processing is modelled sequentially. Our experiments and analyses confirm that better descriptions can be obtained by exploiting gaze-driven attention and shed light on human cognitive processes by comparing different ways of aligning the gaze modality with language production. We find that processing gaze data sequentially leads to descriptions that are better aligned to those produced by speakers, more diverse, and more natural—particularly when gaze is encoded with a dedicated recurrent component.",./data/pdfs/EMNLP2020/Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze.pdf,./data/imgs/EMNLP2020/Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze.png,306ec4956aa2bb4e29a0b5c8b52d1c0e6007a32b,15.0,"{2023: 6, 2022: 1, 2021: 7}",17.0,"{2024: 4, 2023: 6, 2022: 4, 2021: 3}"
16544,ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization,EMNLP,2020,"['Shiyue Zhang', 'Benjamin Frey', 'Mohit Bansal']","Cherokee is a highly endangered Native American language spoken by the Cherokee people. The Cherokee culture is deeply embedded in its language. However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the world and the number is declining every year. To help save this endangered language, we introduce ChrEn, a Cherokee-English parallel dataset, to facilitate machine translation research between Cherokee and English. Compared to some popular machine translation language pairs, ChrEn is extremely low-resource, only containing 14k sentence pairs in total. We split our parallel data in ways that facilitate both in-domain and out-of-domain evaluation. We also collect 5k Cherokee monolingual data to enable semi-supervised learning. Besides these datasets, we propose several Cherokee-English and English-Cherokee machine translation systems. We compare SMT (phrase-based) versus NMT (RNN-based and Transformer-based) systems; supervised versus semi-supervised (via language model, back-translation, and BERT/Multilingual-BERT) methods; as well as transfer learning versus multilingual joint training with 4 other languages. Our best results are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/EnChr translations, respectively; and we hope that our dataset and systems will encourage future work by the community for Cherokee language revitalization.",./data/pdfs/EMNLP2020/ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization.pdf,./data/imgs/EMNLP2020/ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization.png,556f4ff1027ba5d13aed421a08b322b166973ecb,19.0,"{2023: 6, 2022: 2, 2021: 9, 2020: 2}",20.0,"{2023: 5, 2022: 4, 2021: 10, 2020: 1}"
16376,Span-based discontinuous constituency parsing: a family of exact chart-based algorithms with time complexities from O(nˆ6) down to O(nˆ3),EMNLP,2020,['Caio Corro'],"We introduce a novel chart-based algorithm for span-based parsing of discontinuous constituency trees of block degree two, including ill-nested structures. In particular, we show that we can build variants of our parser with smaller search spaces and time complexities ranging from O(nˆ6) down to O(nˆ3). The cubic time variant covers 98% of constituents observed in linguistic treebanks while having the same complexity as continuous constituency parsers. We evaluate our approach on German and English treebanks (Negra, Tiger, and DPTB) and report state-of-the-art results in the fully supervised setting. We also experiment with pre-trained word embeddings and Bert-based neural networks.",./data/pdfs/EMNLP2020/Span-based discontinuous constituency parsing: a family of exact chart-based algorithms with time complexities from O(nˆ6) down to O(nˆ3).pdf,./data/imgs/EMNLP2020/Span-based discontinuous constituency parsing: a family of exact chart-based algorithms with time complexities from O(nˆ6) down to O(nˆ3).png,04edbe5f52f97ca2349e189431cda013194121c2,22.0,"{2023: 7, 2022: 3, 2021: 6, 2020: 6}",22.0,"{2024: 1, 2023: 5, 2022: 5, 2021: 6, 2020: 5}"
15869,Introducing Syntactic Structures into Target Opinion Word Extraction with Deep Learning,EMNLP,2020,"['Amir Pouran Ben Veyseh', 'Nasim Nouri', 'Franck Dernoncourt', 'Dejing Dou', 'Thien Huu Nguyen']","Targeted opinion word extraction (TOWE) is a sub-task of aspect based sentiment analysis (ABSA) which aims to find the opinion words for a given aspect-term in a sentence. Despite their success for TOWE, the current deep learning models fail to exploit the syntactic information of the sentences that have been proved to be useful for TOWE in the prior research. In this work, we propose to incorporate the syntactic structures of the sentences into the deep learning models for TOWE, leveraging the syntax-based opinion possibility scores and the syntactic connections between the words. We also introduce a novel regularization technique to improve the performance of the deep learning models based on the representation distinctions between the words in TOWE. The proposed model is extensively analyzed and achieves the state-of-the-art performance on four benchmark datasets.",./data/pdfs/EMNLP2020/Introducing Syntactic Structures into Target Opinion Word Extraction with Deep Learning.pdf,./data/imgs/EMNLP2020/Introducing Syntactic Structures into Target Opinion Word Extraction with Deep Learning.png,832962037a3aedafeea842b6c2d18c6c1ef09461,35.0,"{2024: 4, 2023: 8, 2022: 12, 2021: 11}",36.0,"{2024: 2, 2023: 8, 2022: 12, 2021: 13, 2018: 1}"
16462,Table Fact Verification with Structure-Aware Transformer,EMNLP,2020,"['Hongzhi Zhang', 'Yingyao Wang', 'Sirui Wang', 'Xuezhi Cao', 'Fuzheng Zhang', 'Zhongyuan Wang']","Verifying fact on semi-structured evidence like tables requires the ability to encode structural information and perform symbolic reasoning. Pre-trained language models trained on natural language could not be directly applied to encode tables, because simply linearizing tables into sequences will lose the cell alignment information. To better utilize pre-trained transformers for table representation, we propose a Structure-Aware Transformer (SAT), which injects the table structural information into the mask of the self-attention layer. A method to combine symbolic and linguistic reasoning is also explored for this task. Our method outperforms baseline with 4.93% on TabFact, a large scale table verification dataset.",./data/pdfs/EMNLP2020/Table Fact Verification with Structure-Aware Transformer.pdf,./data/imgs/EMNLP2020/Table Fact Verification with Structure-Aware Transformer.png,38b3c835e272a25fca4fe523dad627feb6552bd3,51.0,"{2024: 7, 2023: 9, 2022: 18, 2021: 17}",50.0,"{2024: 6, 2023: 11, 2022: 22, 2021: 11}"
15940,Intrinsic Evaluation of Summarization Datasets,EMNLP,2020,"['Rishi Bommasani', 'Claire Cardie']","High quality data forms the bedrock for building meaningful statistical models in NLP. Consequently, data quality must be evaluated either during dataset construction or *post hoc*. Almost all popular summarization datasets are drawn from natural sources and do not come with inherent quality assurance guarantees. In spite of this, data quality has gone largely unquestioned for many of these recent datasets. We perform the first large-scale evaluation of summarization datasets by introducing 5 intrinsic metrics and applying them to 10 popular datasets. We find that data usage in recent summarization research is sometimes inconsistent with the underlying properties of the data. Further, we discover that our metrics can serve the additional purpose of being inexpensive heuristics for detecting generically low quality examples.",./data/pdfs/EMNLP2020/Intrinsic Evaluation of Summarization Datasets.pdf,./data/imgs/EMNLP2020/Intrinsic Evaluation of Summarization Datasets.png,fc8b5dc80c45855c594f3f2b7906a9d2f52bc50f,51.0,"{2024: 3, 2023: 19, 2022: 13, 2021: 16}",55.0,"{2024: 6, 2023: 14, 2022: 20, 2021: 14, 2020: 1}"
16082,Factual Error Correction for Abstractive Summarization Models,EMNLP,2020,"['Meng Cao', 'Yue Dong', 'Jiapeng Wu', 'Jackie Chi Kit Cheung']","Neural abstractive summarization systems have achieved promising progress, thanks to the availability of large-scale datasets and models pre-trained with self-supervised methods. However, ensuring the factual consistency of the generated summaries for abstractive summarization systems is a challenge. We propose a post-editing corrector module to address this issue by identifying and correcting factual errors in generated summaries. The neural corrector model is pre-trained on artificial examples that are created by applying a series of heuristic transformations on reference summaries. These transformations are inspired by the error analysis of state-of-the-art summarization model outputs. Experimental results show that our model is able to correct factual errors in summaries generated by other neural summarization models and outperforms previous models on factual consistency evaluation on the CNN/DailyMail dataset. We also find that transferring from artificial error correction to downstream settings is still very challenging.",./data/pdfs/EMNLP2020/Factual Error Correction for Abstractive Summarization Models.pdf,./data/imgs/EMNLP2020/Factual Error Correction for Abstractive Summarization Models.png,4c6f53097829872734aa11de5ba6788fd992ce50,115.0,"{2025: 3, 2024: 7, 2023: 42, 2022: 28, 2021: 29, 2020: 3}",128.0,"{2024: 13, 2023: 52, 2022: 33, 2021: 26, 2020: 4}"
15971,Cold-start Active Learning through Self-supervised Language Modeling,EMNLP,2020,"['Michelle Yuan', 'Hsuan-Tien Lin', 'Jordan Boyd‐Graber']","Active learning strives to reduce annotation costs by choosing the most critical examples to label. Typically, the active learning strategy is contingent on the classification model. For instance, uncertainty sampling depends on poorly calibrated model confidence scores. In the cold-start setting, active learning is impractical because of model instability and data scarcity. Fortunately, modern NLP provides an additional source of information: pre-trained language models. The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning. Therefore, we treat the language modeling loss as a proxy for classification uncertainty. With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification. Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.",./data/pdfs/EMNLP2020/Cold-start Active Learning through Self-supervised Language Modeling.pdf,./data/imgs/EMNLP2020/Cold-start Active Learning through Self-supervised Language Modeling.png,2e06b7f72270900544284e0898aac2bb564ff58b,117.0,"{2025: 5, 2024: 22, 2023: 44, 2022: 29, 2021: 15, 2020: 1, 2019: 1}",148.0,"{2024: 21, 2023: 59, 2022: 45, 2021: 21, 2019: 1, 2018: 1}"
13133,A New Representation for Span-based CCG Parsing,EMNLP,2021,"['Yoshihide Kato', 'Shigeki Matsubara']","This paper proposes a new representation for CCG derivations. CCG derivations are represented as trees whose nodes are labeled with categories strictly restricted by CCG rule schemata. This characteristic is not suitable for span-based parsing models because they predict node labels independently. In other words, span-based models may generate invalid CCG derivations that violate the rule schemata. Our proposed representation decomposes CCG derivations into several independent pieces and prevents the span-based parsing models from violating the schemata. Our experimental result shows that an off-the-shelf span-based parser with our representation is comparable with previous CCG parsers.",./data/pdfs/EMNLP2021/A New Representation for Span-based CCG Parsing.pdf,./data/imgs/EMNLP2021/A New Representation for Span-based CCG Parsing.png,3712be27993ee340920659e422a4b98581e2e094,1.0,{2022: 1},2.0,"{2022: 1, 2021: 1}"
13725,Adversarial Mixing Policy for Relaxing Locally Linear Constraints in Mixup,EMNLP,2021,"['Guang Liu', 'Yuzhao Mao', 'Hailong Huang', 'Weiguo Gao', 'Xuan Li']","Mixup is a recent regularizer for current deep classification networks. Through training a neural network on convex combinations of pairs of examples and their labels, it imposes locally linear constraints on the model's input space. However, such strict linear constraints often lead to under-fitting which degrades the effects of regularization. Noticeably, this issue is getting more serious when the resource is extremely limited. To address these issues, we propose the Adversarial Mixing Policy (AMP), organized in a ""min-max-rand"" formulation, to relax the Locally Linear Constraints in Mixup. Specifically, AMP adds a small adversarial perturbation to the mixing coefficients rather than the examples. Thus, slight non-linearity is injected in-between the synthetic examples and synthetic labels. By training on these data, the deep networks are further regularized, and thus achieve a lower predictive error rate. Experiments on five text classification benchmarks and five backbone models have empirically shown that our methods reduce the error rate over Mixup variants in a significant margin (up to 31.3%), especially in low-resource conditions (up to 17.5%).",./data/pdfs/EMNLP2021/Adversarial Mixing Policy for Relaxing Locally Linear Constraints in Mixup.pdf,./data/imgs/EMNLP2021/Adversarial Mixing Policy for Relaxing Locally Linear Constraints in Mixup.png,72d7a20623d5f760df36d4467aa68f5002b8e92a,3.0,"{2024: 1, 2023: 2}",4.0,"{2024: 1, 2023: 2, 2022: 1}"
13446,Sequential Randomized Smoothing for Adversarially Robust Speech Recognition,EMNLP,2021,"['Raphaël Olivier', 'Bhiksha Raj']","While Automatic Speech Recognition has been shown to be vulnerable to adversarial attacks, defenses against these attacks are still lagging. Existing, naive defenses can be partially broken with an adaptive attack. In classification tasks, the Randomized Smoothing paradigm has been shown to be effective at defending models. However, it is difficult to apply this paradigm to ASR tasks, due to their complexity and the sequential nature of their outputs. Our paper overcomes some of these challenges by leveraging speech-specific tools like enhancement and ROVER voting to design an ASR model that is robust to perturbations. We apply adaptive versions of state-of-the-art attacks, such as the Imperceptible ASR attack, to our model, and show that our strongest defense is robust to all attacks that use inaudible noise, and can only be broken with very high distortion.",./data/pdfs/EMNLP2021/Sequential Randomized Smoothing for Adversarially Robust Speech Recognition.pdf,./data/imgs/EMNLP2021/Sequential Randomized Smoothing for Adversarially Robust Speech Recognition.png,b0754c0a3ac2a18ff6d3f721ab53401a790075e1,2.0,"{2023: 1, 2022: 1}",8.0,"{2024: 1, 2023: 2, 2022: 4, 2020: 1}"
13751,Structure-Augmented Keyphrase Generation,EMNLP,2021,"['Jihyuk Kim', 'Myeongho Jeong', 'Seungtaek Choi', 'Seung-won Hwang']","This paper studies the keyphrase generation (KG) task for scenarios where structure plays an important role. For example, a scientific publication consists of a short title and a long body, where the title can be used for de-emphasizing unimportant details in the body. Similarly, for short social media posts (, tweets), scarce context can be augmented from titles, though often missing. Our contribution is generating/augmenting structure then injecting these information in the encoding, using existing keyphrases of other documents, complementing missing/incomplete titles. We propose novel structure-augmented document encoding approaches that consist of the following two phases: The first phase, generating structure, extends the given document with related but absent keyphrases, augmenting missing context. The second phase, encoding structure, builds a graph of keyphrases and the given document to obtain the structure-aware representation of the augmented text. Our empirical results validate that our proposed structure augmentation and augmentation-aware encoding/decoding can improve KG for both scenarios, outperforming the state-of-the-art.",./data/pdfs/EMNLP2021/Structure-Augmented Keyphrase Generation.pdf,./data/imgs/EMNLP2021/Structure-Augmented Keyphrase Generation.png,b5356c79d6e8f9279a4f384a34702e7f096cfc50,9.0,"{2024: 3, 2023: 2, 2022: 4}",11.0,"{2024: 1, 2023: 4, 2022: 5, 2021: 1}"
13924,Multitask Semi-Supervised Learning for Class-Imbalanced Discourse Classification,EMNLP,2021,"['Alexander Spangher', 'Jonathan May', 'Sz-Rung Shiang', 'Lingjia Deng']","As labeling schemas evolve over time, small differences can render datasets following older schemas unusable. This prevents researchers from building on top of previous annotation work and results in the existence, in discourse learning in particular, of many small class-imbalanced datasets. In this work, we show that a multitask learning approach can combine discourse datasets from similar and diverse domains to improve discourse classification. We show an improvement of 4.9% Micro F1-score over current state-of-the-art benchmarks on the NewsDiscourse dataset, one of the largest discourse datasets recently published, due in part to label correlations across tasks, which improve performance for underrepresented classes. We also offer an extensive review of additional techniques proposed to address resource-poor problems in NLP, and show that none of these approaches can improve classification accuracy in our setting.",./data/pdfs/EMNLP2021/Multitask Semi-Supervised Learning for Class-Imbalanced Discourse Classification.pdf,./data/imgs/EMNLP2021/Multitask Semi-Supervised Learning for Class-Imbalanced Discourse Classification.png,8d555bb74cb9fc9b778dd6b1116fb3b0141cb9ba,13.0,"{2024: 2, 2023: 6, 2022: 5}",16.0,"{2024: 2, 2023: 7, 2022: 6, 2021: 1}"
13656,FiD-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation,EMNLP,2021,"['Kushal Lakhotia', 'Bhargavi Paranjape', 'Asish Ghoshal', 'Scott Yih', 'Yashar Mehdad', 'Srini Iyer']","Natural language (NL) explanations of model predictions are gaining popularity as a means to understand and verify decisions made by large black-box pre-trained models, for tasks such as Question Answering (QA) and Fact Verification. Recently, pre-trained sequence to sequence (seq2seq) models have proven to be very effective in jointly making predictions, as well as generating NL explanations. However, these models have many shortcomings; they can fabricate explanations even for incorrect predictions, they are difficult to adapt to long input documents, and their training requires a large amount of labeled data. In this paper, we develop FiD-Ex, which addresses these shortcomings for seq2seq models by: 1) introducing sentence markers to eliminate explanation fabrication by encouraging extractive generation, 2) using the fusion-in-decoder architecture to handle long input contexts, and 3) intermediate fine-tuning on re-structured open domain QA datasets to improve few-shot performance. FiD-Ex significantly improves over prior work in terms of explanation metrics and task accuracy on five tasks from the ERASER explainability benchmark in both fully supervised and few-shot settings.",./data/pdfs/EMNLP2021/FiD-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation.pdf,./data/imgs/EMNLP2021/FiD-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation.png,17d4681b29b79c4ee5029ae39acabfdf9946bd77,15.0,"{2024: 6, 2023: 6, 2022: 3}",21.0,"{2024: 6, 2023: 7, 2022: 7, 2021: 1}"
13710,STANKER: Stacking Network based on Level-grained Attention-masked BERT for Rumor Detection on Social Media,EMNLP,2021,"['Dongning Rao', 'Xin Miao', 'Zhihua Jiang', 'Ran Li']","Rumor detection on social media puts pre-trained language models (LMs), such as BERT, and auxiliary features, such as comments, into use. However, on the one hand, rumor detection datasets in Chinese companies with comments are rare; on the other hand, intensive interaction of attention on Transformer-based models like BERT may hinder performance improvement. To alleviate these problems, we build a new Chinese microblog dataset named Weibo20 by collecting posts and associated comments from Sina Weibo and propose a new ensemble named STANKER (Stacking neTwork bAsed-on atteNtion-masKed BERT). STANKER adopts two level-grained attention-masked BERT (LGAM-BERT) models as base encoders. Unlike the original BERT, our new LGAM-BERT model takes comments as important auxiliary features and masks co-attention between posts and comments on lower-layers. Experiments on Weibo20 and three existing social media datasets showed that STANKER outperformed all compared models, especially beating the old state-of-the-art on Weibo dataset.",./data/pdfs/EMNLP2021/STANKER: Stacking Network based on Level-grained Attention-masked BERT for Rumor Detection on Social Media.pdf,./data/imgs/EMNLP2021/STANKER: Stacking Network based on Level-grained Attention-masked BERT for Rumor Detection on Social Media.png,76da4c6e58e8f2a0158ea62d9568c74a91bfd96f,28.0,"{2025: 1, 2024: 10, 2023: 12, 2022: 4}",25.0,"{2024: 5, 2023: 13, 2022: 6, 2021: 1}"
13842,Sorting through the noise: Testing robustness of information processing in pre-trained language models,EMNLP,2021,"['Lalchand Pandia', 'Allyson Ettinger']","Pre-trained LMs have shown impressive performance on downstream NLP tasks, but we have yet to establish a clear understanding of their sophistication when it comes to processing, retaining, and applying information presented in their input. In this paper we tackle a component of this question by examining robustness of models' ability to deploy relevant context information in the face of distracting content. We present models with cloze tasks requiring use of critical context information, and introduce distracting content to test how robustly the models retain and use that critical information for prediction. We also systematically manipulate the nature of these distractors, to shed light on dynamics of models' use of contextual cues. We find that although models appear in simple contexts to make predictions based on understanding and applying relevant facts from prior context, the presence of distracting but irrelevant content has clear impact in confusing model predictions. In particular, models appear particularly susceptible to factors of semantic similarity and word position. The findings are consistent with the conclusion that LM predictions are driven in large part by superficial contextual cues, rather than by robust representations of context meaning.",./data/pdfs/EMNLP2021/Sorting through the noise: Testing robustness of information processing in pre-trained language models.pdf,./data/imgs/EMNLP2021/Sorting through the noise: Testing robustness of information processing in pre-trained language models.png,0981ce872d31a665882e7677d608351ff5f1de6b,19.0,"{2024: 2, 2023: 14, 2022: 2, 2021: 1}",31.0,"{2024: 5, 2023: 16, 2022: 10}"
13118,Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration,EMNLP,2021,"['Shufan Wang', 'Laure Thompson', 'Mohit Iyyer']","Phrase representations derived from BERT often do not exhibit complex phrasal compositionality, as the model relies instead on lexical similarity to determine semantic relatedness. In this paper, we propose a contrastive fine-tuning objective that enables BERT to produce more powerful phrase embeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal paraphrases, which is automatically generated using a paraphrase generation model, as well as a large-scale dataset of phrases in context mined from the Books3 corpus. Phrase-BERT outperforms baselines across a variety of phrase-level similarity tasks, while also demonstrating increased lexical diversity between nearest neighbors in the vector space. Finally, as a case study, we show that Phrase-BERT embeddings can be easily integrated with a simple autoencoder to build a phrase-based neural topic model that interprets topics as mixtures of words and phrases by performing a nearest neighbor search in the embedding space. Crowdsourced evaluations demonstrate that this phrase-based topic model produces more coherent and meaningful topics than baseline word and phrase-level topic models, further validating the utility of Phrase-BERT.",./data/pdfs/EMNLP2021/Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration.pdf,./data/imgs/EMNLP2021/Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration.png,46ed42e4318e1363a0ec3dde195422cdfecf2017,48.0,"{2025: 3, 2024: 9, 2023: 18, 2022: 18}",51.0,"{2024: 5, 2023: 19, 2022: 23, 2021: 3, 2020: 1}"
13863,The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation,EMNLP,2021,"['Marzena Karpinska', 'Nader Akoury', 'Mohit Iyyer']","Recent text generation research has increasingly focused on open-ended domains such as story and poetry generation. Because models built for such tasks are difficult to evaluate automatically, most researchers in the space justify their modeling choices by collecting crowdsourced human judgments of text quality (e.g., Likert scores of coherence or grammaticality) from Amazon Mechanical Turk (AMT). In this paper, we first conduct a survey of 45 open-ended text generation papers and find that the vast majority of them fail to report crucial details about their AMT tasks, hindering reproducibility. We then run a series of story evaluation experiments with both AMT workers and English teachers and discover that even with strict qualification filters, AMT workers (unlike teachers) fail to distinguish between model-generated text and human-generated references. We show that AMT worker judgments improve when they are shown model-generated output alongside human-generated references, which enables the workers to better calibrate their ratings. Finally, interviews with the English teachers provide deeper insights into the challenges of the evaluation process, particularly when rating model-generated text.",./data/pdfs/EMNLP2021/The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation.pdf,./data/imgs/EMNLP2021/The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation.png,d706645fbbc6edfad5fb642b1dfc3019fcabbd99,43.0,"{2025: 1, 2024: 5, 2023: 21, 2022: 15, 2021: 1}",80.0,"{2024: 21, 2023: 36, 2022: 19, 2021: 4}"
15126,“Covid vaccine is against Covid but Oxford vaccine is made at Oxford!” Semantic Interpretation of Proper Noun Compounds,EMNLP,2022,"['Keshav Kolluru', 'Gabriel Stanovsky', 'Mausam Mausam']","Proper noun compounds, e.g., “Covid vaccine”, convey information in a succinct manner (a “Covid vaccine” is a “vaccine that immunizes against the Covid disease”). These are commonly used in short-form domains, such as news headlines, but are largely ignored in information-seeking applications. To address this limitation, we release a new manually annotated dataset, ProNCI, consisting of 22.5K proper noun compounds along with their free-form semantic interpretations. ProNCI is 60 times larger than prior noun compound datasets and also includes non-compositional examples, which have not been previously explored. We experiment with various neural models for automatically generating the semantic interpretations from proper noun compounds, ranging from few-shot prompting to supervised learning, with varying degrees of knowledge about the constituent nouns. We find that adding targeted knowledge, particularly about the common noun, results in performance gains of upto 2.8%. Finally, we integrate our model generated interpretations with an existing Open IE system and observe an 7.5% increase in yield at a precision of 85%. The dataset and code are available at https://github.com/dair-iitd/pronci.",./data/pdfs/EMNLP2022/“Covid vaccine is against Covid but Oxford vaccine is made at Oxford!” Semantic Interpretation of Proper Noun Compounds.pdf,./data/imgs/EMNLP2022/“Covid vaccine is against Covid but Oxford vaccine is made at Oxford!” Semantic Interpretation of Proper Noun Compounds.png,33285e02758788b681754d283df20971fef6e31f,0.0,{},1.0,{2023: 1}
15039,Interventional Training for Out-Of-Distribution Natural Language Understanding,EMNLP,2022,"['Sicheng Yu', 'Jing Jiang', 'Hao Zhang', 'Yulei Niu', 'Qianru Sun', 'Lidong Bing']","Out-of-distribution (OOD) settings are used to measure a model’s performance when the distribution of the test data is different from that of the training data. NLU models are known to suffer in OOD. We study this issue from the perspective of causality, which sees confounding bias as the reason for models to learn spurious correlations. While a common solution is to perform intervention, existing methods handle only known and single confounder, but in many NLU tasks the confounders can be both unknown and multifactorial. In this paper, we propose a novel interventional training method called Bottom-up Automatic Intervention (BAI) that performs multi-granular intervention with identified multifactorial confounders. Our experiments on three NLU tasks, namely, natural language inference, fact verification and paraphrase identification, show the effectiveness of BAI for tackling OOD settings.",./data/pdfs/EMNLP2022/Interventional Training for Out-Of-Distribution Natural Language Understanding.pdf,./data/imgs/EMNLP2022/Interventional Training for Out-Of-Distribution Natural Language Understanding.png,4468974fc2974f2fcb1c19fec56579065f8c5dcc,3.0,{2024: 3},2.0,"{2024: 1, 2023: 1}"
15094,Faithful Knowledge Graph Explanations in Commonsense Question Answering,EMNLP,2022,"['Guy Aglionby', 'Simone Teufel']","Knowledge graphs are commonly used as sources of information in commonsense question answering, and can also be used to express explanations for the model's answer choice. A common way of incorporating facts from the graph is to encode them separately from the question, and then combine the two representations to select an answer. In this paper, we argue that highly faithful graph-based explanations cannot be extracted from existing models of this type. Such explanations will not include reasoning done by the transformer encoding the question, so will be incomplete. We confirm this theory with a novel proxy measure for faithfulness and propose two architecture changes to address the problem. Our findings suggest a path forward for developing architectures for faithful graph-based explanations.",./data/pdfs/EMNLP2022/Faithful Knowledge Graph Explanations in Commonsense Question Answering.pdf,./data/imgs/EMNLP2022/Faithful Knowledge Graph Explanations in Commonsense Question Answering.png,f5f1004674a2b5bd882f8fe125d71c9fdbfd9b5b,3.0,"{2024: 2, 2023: 1}",4.0,"{2024: 2, 2023: 2}"
15056,Agent-Specific Deontic Modality Detection in Legal Language,EMNLP,2022,"['Abhilasha Sancheti', 'Aparna Garimella', 'B. Srinivasan', 'Rachel Rudinger']","Legal documents are typically long and written in legalese, which makes it particularly difficult for laypeople to understand their rights and duties. While natural language understanding technologies can be valuable in supporting such understanding in the legal domain, the limited availability of datasets annotated for deontic modalities in the legal domain, due to the cost of hiring experts and privacy issues, is a bottleneck. To this end, we introduce, LEXDEMOD, a corpus of English contracts annotatedwith deontic modality expressed with respect to a contracting party or agent along with the modal triggers. We benchmark this dataset on two tasks: (i) agent-specific multi-label deontic modality classification, and (ii) agent-specific deontic modality and trigger span detection using Transformer-based (Vaswani et al., 2017) language models. Transfer learning experiments show that the linguistic diversity of modal expressions in LEXDEMOD generalizes reasonably from lease to employment andrental agreements. A small case study indicates that a model trained on LEXDEMOD can detect red flags with high recall. We believe our work offers a new research direction for deontic modality detection in the legal domain.",./data/pdfs/EMNLP2022/Agent-Specific Deontic Modality Detection in Legal Language.pdf,./data/imgs/EMNLP2022/Agent-Specific Deontic Modality Detection in Legal Language.png,3eb4d4ec27accd8023a1eecfe2bd49d2e18dfa3d,2.0,{2023: 2},5.0,"{2024: 1, 2023: 2, 2022: 2}"
15534,MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure,EMNLP,2022,"['Yinya Huang', 'Hongming Zhang', 'Ruixin Hong', 'Xiaodan Liang', 'Changshui Zhang', 'Dong Yu']","In this paper, we propose a comprehensive benchmark to investigate models' logical reasoning capabilities in complex real-life scenarios. Current explanation datasets often employ synthetic data with simple reasoning structures. Therefore, it cannot express more complex reasoning processes, such as the rebuttal to a reasoning step and the degree of certainty of the evidence. To this end, we propose a comprehensive logical reasoning explanation form. Based on the multi-hop chain of reasoning, the explanation form includes three main components: (1) The condition of rebuttal that the reasoning node can be challenged; (2) Logical formulae that uncover the internal texture of reasoning nodes; (3) Reasoning strength indicated by degrees of certainty. The fine-grained structure conforms to the real logical reasoning scenario, better fitting the human cognitive process but, simultaneously, is more challenging for the current models. We evaluate the current best models' performance on this new explanation form. The experimental results show that generating reasoning graphs remains a challenging task for current models, even with the help of giant pre-trained language models.",./data/pdfs/EMNLP2022/MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure.pdf,./data/imgs/EMNLP2022/MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure.png,2b22a3acb3ba1581d320b70b02343d4a0f356e3e,3.0,"{2024: 1, 2023: 2}",6.0,"{2024: 1, 2023: 5}"
15382,Increasing Visual Awareness in Multimodal Neural Machine Translation from an Information Theoretic Perspective,EMNLP,2022,"['Baijun Ji', 'Tong Zhang', 'Yicheng Zou', 'Bojie Hu', 'Si Shen']","Multimodal machine translation (MMT) aims to improve translation quality by equipping the source sentence with its corresponding image. Despite the promising performance, MMT models still suffer the problem of input degradation: models focus more on textual information while visual information is generally overlooked. In this paper, we endeavor to improve MMT performance by increasing visual awareness from an information theoretic perspective. In detail, we decompose the informative visual signals into two parts: source-specific information and target-specific information. We use mutual information to quantify them and propose two methods for objective optimization to better leverage visual signals. Experiments on two datasets demonstrate that our approach can effectively enhance the visual awareness of MMT model and achieve superior results against strong baselines.",./data/pdfs/EMNLP2022/Increasing Visual Awareness in Multimodal Neural Machine Translation from an Information Theoretic Perspective.pdf,./data/imgs/EMNLP2022/Increasing Visual Awareness in Multimodal Neural Machine Translation from an Information Theoretic Perspective.png,8e1b951309ca6a5925d056d5b2a89a4f678480c0,7.0,"{2024: 3, 2023: 4}",9.0,"{2024: 3, 2023: 5, 2022: 1}"
15221,Detecting Label Errors by Using Pre-Trained Language Models,EMNLP,2022,"['Derek Chong', 'Jenny Hong', 'Christopher D. Manning']","We show that large pre-trained language models are inherently highly capable of identifying label errors in natural language datasets: simply examining out-of-sample data points in descending order of fine-tuned task loss significantly outperforms more complex error-detection mechanisms proposed in previous work. To this end, we contribute a novel method for introducing realistic, human-originated label noise into existing crowdsourced datasets such as SNLI and TweetNLP. We show that this noise has similar properties to real, hand-verified label errors, and is harder to detect than existing synthetic noise, creating challenges for model robustness.We argue that human-originated noise is a better standard for evaluation than synthetic noise. Finally, we use crowdsourced verification to evaluate the detection of real errors on IMDB, Amazon Reviews, and Recon, and confirm that pre-trained models perform at a 9–36% higher absolute Area Under the Precision-Recall Curve than existing models.",./data/pdfs/EMNLP2022/Detecting Label Errors by Using Pre-Trained Language Models.pdf,./data/imgs/EMNLP2022/Detecting Label Errors by Using Pre-Trained Language Models.png,ed6957836b262c4604b96e8bf175123ea59827e9,7.0,"{2024: 4, 2023: 2}",13.0,"{2024: 4, 2023: 9}"
15140,Saving Dense Retriever from Shortcut Dependency in Conversational Search,EMNLP,2022,"['Sungdong Kim', 'Gangwoo Kim']","Conversational search (CS) needs a holistic understanding of conversational inputs to retrieve relevant passages. In this paper, we demonstrate the existence of a retrieval shortcut in CS, which causes models to retrieve passages solely relying on partial history while disregarding the latest question. With in-depth analysis, we first show that naively trained dense retrievers heavily exploit the shortcut and hence perform poorly when asked to answer history-independent questions. To build more robust models against shortcut dependency, we explore various hard negative mining strategies. Experimental results show that training with the model-based hard negatives effectively mitigates the dependency on the shortcut, significantly improving dense retrievers on recent CS benchmarks. In particular, our retriever outperforms the previous state-of-the-art model by 11.0 in Recall@10 on QReCC.",./data/pdfs/EMNLP2022/Saving Dense Retriever from Shortcut Dependency in Conversational Search.pdf,./data/imgs/EMNLP2022/Saving Dense Retriever from Shortcut Dependency in Conversational Search.png,c2f9a27ab32bff87573e31594c97742af90f11b2,10.0,"{2024: 2, 2023: 6, 2022: 2}",20.0,"{2024: 7, 2023: 9, 2022: 3, 2021: 1}"
15676,Towards Climate Awareness in NLP Research,EMNLP,2022,"['Daniel Hershcovich', 'Nicolas Webersinke', 'Mathias Kraus', 'Julia Bingler', 'Markus Leippold']","The climate impact of AI, and NLP research in particular, has become a serious issue given the enormous amount of energy that is increasingly being used for training and running computational models. Consequently, increasing focus is placed on efficient NLP. However, this important initiative lacks simple guidelines that would allow for systematic climate reporting of NLP research. We argue that this deficiency is one of the reasons why very few publications in NLP report key figures that would allow a more thorough examination of environmental impact, and present a quantitative survey to demonstrate this. As a remedy, we propose a climate performance model card with the primary purpose of being practically usable with only limited information about experiments and the underlying computer hardware. We describe why this step is essential to increase awareness about the environmental impact of NLP research and, thereby, paving the way for more thorough discussions.",./data/pdfs/EMNLP2022/Towards Climate Awareness in NLP Research.pdf,./data/imgs/EMNLP2022/Towards Climate Awareness in NLP Research.png,8d0f755dea90f35f4b126a01fa3cce96b3bdd344,20.0,"{2024: 5, 2023: 8, 2022: 6}",26.0,"{2024: 5, 2023: 11, 2022: 8, 2021: 2}"
15777,Information-Transport-based Policy for Simultaneous Translation,EMNLP,2022,"['Shaolei Zhang', 'Yan Feng']","Simultaneous translation (ST) outputs translation while receiving the source inputs, and hence requires a policy to determine whether to translate a target token or wait for the next source token. The major challenge of ST is that each target token can only be translated based on the current received source tokens, where the received source information will directly affect the translation quality. So naturally, how much source information is received for the translation of the current target token is supposed to be the pivotal evidence for the ST policy to decide between translating and waiting. In this paper, we treat the translation as information transport from source to target and accordingly propose an Information-Transport-based Simultaneous Translation (ITST). ITST quantifies the transported information weight from each source token to the current target token, and then decides whether to translate the target token according to its accumulated received information. Experiments on both text-to-text ST and speech-to-text ST (a.k.a., streaming speech translation) tasks show that ITST outperforms strong baselines and achieves state-of-the-art performance.",./data/pdfs/EMNLP2022/Information-Transport-based Policy for Simultaneous Translation.pdf,./data/imgs/EMNLP2022/Information-Transport-based Policy for Simultaneous Translation.png,7dc02a2ecbcd2c5f22595b736e2c7a1b35ac9267,24.0,"{2024: 6, 2023: 15, 2022: 3}",34.0,"{2024: 12, 2023: 18, 2022: 4}"
14123,Text Representation Distillation via Information Bottleneck Principle,EMNLP,2023,"['Yanzhao Zhang', 'Dingkun Long', 'Zehan Li', 'Pengjun Xie']","Pre-trained language models (PLMs) have recently shown great success in text representation field. However, the high computational cost and high-dimensional representation of PLMs pose significant challenges for practical applications. To make models more accessible, an effective method is to distill large models into smaller representation models. In order to relieve the issue of performance degradation after distillation, we propose a novel Knowledge Distillation method called IBKD. This approach is motivated by the Information Bottleneck principle and aims to maximize the mutual information between the final representation of the teacher and student model, while simultaneously reducing the mutual information between the student model’s representation and the input data. This enables the student model to preserve important learned information while avoiding unnecessary information, thus reducing the risk of over-fitting. Empirical studies on two main downstream applications of text representation (Semantic Textual Similarity and Dense Retrieval tasks) demonstrate the effectiveness of our proposed approach.",./data/pdfs/EMNLP2023/Text Representation Distillation via Information Bottleneck Principle.pdf,./data/imgs/EMNLP2023/Text Representation Distillation via Information Bottleneck Principle.png,ab71258b6bbd39056da7fcf7fbde8e317ac8778a,1.0,{2025: 1},0.0,{}
14761,Are All Steps Equally Important? Benchmarking Essentiality Detection in Event Processes,EMNLP,2023,"['Haoyu Wang', 'Hongming Zhang', 'Yueguan Wang', 'Yuqian Deng', 'Muhao Chen', 'Dan Roth']","Natural language often describes events in different granularities, such that more coarse-grained (goal) events can often be decomposed into fine-grained sequences of (step) events. A critical but overlooked challenge in understanding an event process lies in the fact that the step events are not equally important to the central goal. In this paper, we seek to fill this gap by studying how well current models can understand the essentiality of different step events towards a goal event. As discussed by cognitive studies, such an ability enables the machine to mimic human’s commonsense reasoning about preconditions and necessary efforts of daily-life tasks. Our work contributes with a high-quality corpus of (goal, step) pairs from a community guideline website WikiHow, where the steps are manually annotated with their essentiality w.r.t. the goal. The high IAA indicates that humans have a consistent understanding of the events. Despite evaluating various statistical and massive pre-trained NLU models, we observe that existing SOTA models all perform drastically behind humans, indicating the need for future investigation of this crucial yet challenging task.",./data/pdfs/EMNLP2023/Are All Steps Equally Important? Benchmarking Essentiality Detection in Event Processes.pdf,./data/imgs/EMNLP2023/Are All Steps Equally Important? Benchmarking Essentiality Detection in Event Processes.png,a67dd6a030cd552ae4eb8bf9cf191a18630d615c,2.0,{2023: 2},0.0,{}
14291,From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base,EMNLP,2023,"['Wangzhen Guo', 'Linyin Luo', 'Hanjiang Lai', 'Jian Yin']","Parsing questions into executable logical forms has showed impressive results for knowledge-base question answering (KBQA). However, complex KBQA is a more challenging task that requires to perform complex multi-step reasoning. Recently, a new semantic parser called KoPL has been proposed to explicitly model the reasoning processes, which achieved the state-of-the-art on complex KBQA. In this paper, we further explore how to unlock the reasoning ability of semantic parsers by a simple proposed parse-execute-refine paradigm. We refine and improve the KoPL parser by demonstrating the executed intermediate reasoning steps to the KBQA model. We show that such simple strategy can significantly improve the ability of complex reasoning. Specifically, we propose three components: a parsing stage, an execution stage and a refinement stage, to enhance the ability of complex reasoning. The parser uses the KoPL to generate the transparent logical forms. Then, the execution stage aligns and executes the logical forms over knowledge base to obtain intermediate reasoning processes. Finally, the intermediate step-by-step reasoning processes are demonstrated to the KBQA model in the refinement stage. With the explicit reasoning processes, it is much easier to answer the complex questions. Experiments on benchmark dataset shows that the proposed PER-KBQA performs significantly better than the stage-of-the-art baselines on the complex KBQA.",./data/pdfs/EMNLP2023/From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base.pdf,./data/imgs/EMNLP2023/From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base.png,ad55fef5ecf6f75912cb4dbb12408c0a10ca490b,4.0,"{2025: 1, 2024: 1, 2023: 2}",1.0,{2024: 1}
14773,mRedditSum: A Multimodal Abstractive Summarization Dataset of Reddit Threads with Images,EMNLP,2023,"['Keighley Overbay', 'Jaewoo Ahn', 'Fatemeh Pesaran zadeh', 'Joonsuk Park', 'Gunhee Kim']","The growing number of multimodal online discussions necessitates automatic summarization to save time and reduce content overload. However, existing summarization datasets are not suitable for this purpose, as they either do not cover discussions, multiple modalities, or both. To this end, we present mRedditSum, the first multimodal discussion summarization dataset. It consists of 3,033 discussion threads where a post solicits advice regarding an issue described with an image and text, and respective comments express diverse opinions. We annotate each thread with a human-written summary that captures both the essential information from the text, as well as the details available only in the image. Experiments show that popular summarization models—GPT-3.5, BART, and T5—consistently improve in performance when visual information is incorporated. We also introduce a novel method, cluster-based multi-stage summarization, that outperforms existing baselines and serves as a competitive baseline for future work.",./data/pdfs/EMNLP2023/mRedditSum: A Multimodal Abstractive Summarization Dataset of Reddit Threads with Images.pdf,./data/imgs/EMNLP2023/mRedditSum: A Multimodal Abstractive Summarization Dataset of Reddit Threads with Images.png,9e52e82773f2f5be5c7239068bf4f6029d71f844,3.0,{2024: 3},1.0,{2024: 1}
14380,GazeVQA: A Video Question Answering Dataset for Multiview Eye-Gaze Task-Oriented Collaborations,EMNLP,2023,"['Muhammet Furkan Ilaslan', 'Chenan Song', 'Joya Chen', 'Difei Gao', 'Weixian Lei', 'Qianli Xu', 'Joo Weon Lim', 'Mike Zheng Shou']","The usage of exocentric and egocentric videos in Video Question Answering (VQA) is a new endeavor in human-robot interaction and collaboration studies. Particularly for egocentric videos, one may leverage eye-gaze information to understand human intentions during the task. In this paper, we build a novel task-oriented VQA dataset, called GazeVQA, for collaborative tasks where gaze information is captured during the task process. GazeVQA is designed with a novel QA format that covers thirteen different reasoning types to capture multiple aspects of task information and user intent. For each participant, GazeVQA consists of more than 1,100 textual questions and more than 500 labeled images that were annotated with the assistance of the Segment Anything Model. In total, 2,967 video clips, 12,491 labeled images, and 25,040 questions from 22 participants were included in the dataset. Additionally, inspired by the assisting models and common ground theory for industrial task collaboration, we propose a new AI model called AssistGaze that is designed to answer the questions with three different answer types, namely textual, image, and video. AssistGaze can effectively ground the perceptual input into semantic information while reducing ambiguities. We conduct comprehensive experiments to demonstrate the challenges of GazeVQA and the effectiveness of AssistGaze.",./data/pdfs/EMNLP2023/GazeVQA: A Video Question Answering Dataset for Multiview Eye-Gaze Task-Oriented Collaborations.pdf,./data/imgs/EMNLP2023/GazeVQA: A Video Question Answering Dataset for Multiview Eye-Gaze Task-Oriented Collaborations.png,0c5859fbd6c0f3188cd1d264afe2296a54112b8d,3.0,"{2025: 1, 2024: 1}",2.0,{2024: 2}
14841,Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation,EMNLP,2023,"['Wenyu Guo', 'Qingkai Fang', 'Dong Yu', 'Yang Feng']","Multimodal machine translation (MMT) simultaneously takes the source sentence and a relevant image as input for translation.Since there is no paired image available for the input sentence in most cases, recent studies suggest utilizing powerful text-to-image generation models to provide image inputs.Nevertheless, synthetic images generated by these models often follow different distributions compared to authentic images.Consequently, using authentic images for training and synthetic images for inference can introduce a distribution shift, resulting in performance degradation during inference.To tackle this challenge, in this paper, we feed synthetic and authentic images to the MMT model, respectively.Then we minimize the gap between the synthetic and authentic images by drawing close the input image representations of the Transformer Encoder and the output distributions of the Transformer Decoder.Therefore, we mitigate the distribution disparity introduced by the synthetic images during inference, thereby freeing the authentic images from the inference process.Experimental results show that our approach achieves state-ofthe-art performance on the Multi30K En-De and En-Fr datasets, while remaining independent of authentic images during inference.",./data/pdfs/EMNLP2023/Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation.pdf,./data/imgs/EMNLP2023/Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation.png,7f265fd9a036b2f049a430a4d0f44ee200a68deb,1.0,{2024: 1},3.0,{2024: 3}
14190,BLESS: Benchmarking Large Language Models on Sentence Simplification,EMNLP,2023,"['Tannon Kew', 'Alison Chi', 'Laura Vásquez-Rodríguez', 'Sweta Agrawal', 'Dennis Aumiller', 'Fernando Alva-Manchego', 'Matthew Shardlow']","Tannon Kew, Alison Chi, Laura Vásquez-Rodríguez, Sweta Agrawal, Dennis Aumiller, Fernando Alva-Manchego, Matthew Shardlow. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",./data/pdfs/EMNLP2023/BLESS: Benchmarking Large Language Models on Sentence Simplification.pdf,./data/imgs/EMNLP2023/BLESS: Benchmarking Large Language Models on Sentence Simplification.png,b37690747ba314455b8a992e794b25223e105d1c,6.0,"{2025: 2, 2024: 3, 2023: 1}",4.0,"{2024: 3, 2023: 1}"
14096,Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism,EMNLP,2023,"['M. H. Ye', 'Tatsuki Kuribayashi', 'Jun Suzuki', 'GORO KOBAYASHI', 'Hiroaki Funayama']","Large language models (LLMs) take advantage of step-by-step reasoning instructions, e.g., chain-of-thought (CoT) prompting. Building on this, their ability to perform CoT-style reasoning robustly is of interest from a probing perspective. In this study, we inspect the step-by-step reasoning ability of LLMs with a focus on negation, which is a core linguistic phenomenon that is difficult to process. In particular, we introduce several controlled settings (e.g., reasoning in case of fictional entities) to evaluate the logical reasoning abilities of the models. We observed that dozens of modern LLMs were not robust against lexical negation (e.g., plausible→implausible) when performing CoT-style reasoning, and the results highlight unique limitations in each LLM family.",./data/pdfs/EMNLP2023/Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism.pdf,./data/imgs/EMNLP2023/Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study on Syllogism.png,ed5d6c4ee7161f951f09c4bd16046010c4acb47e,1.0,{2025: 1},6.0,"{2024: 4, 2023: 2}"
14301,Crystal: Introspective Reasoners Reinforced with Self-Feedback,EMNLP,2023,"['Jiacheng Liu', 'Ramakanth Pasunuru', 'Hannaneh Hajishirzi', 'Yejin Choi', 'Aslı Çelikyılmaz']","Extensive work has shown that the performance and interpretability of commonsense reasoning can be improved via knowledge-augmented reasoning methods, where the knowledge that underpins the reasoning process is explicitly verbalized and utilized. However, existing implementations, including ""chain-of-thought"" and its variants, fall short in capturing the *introspective* nature of knowledge required in commonsense reasoning, and in accounting for the mutual adaptation between the generation and utilization of knowledge. We propose a novel method to develop an introspective commonsense reasoner, **Crystal**. To tackle commonsense problems, it first introspects for knowledge statements related to the given question, and subsequently makes an informed prediction that is grounded in the previously introspected knowledge. The knowledge introspection and knowledge-grounded reasoning modes of the model are tuned via reinforcement learning to mutually adapt, where the reward derives from the feedback given by the model itself. Experiments show that Crystal significantly outperforms both the standard supervised finetuning and chain-of-thought distilled methods, and enhances the transparency of the commonsense reasoning process. Our work ultimately validates the feasibility and potential of reinforcing a neural model with self-feedback.",./data/pdfs/EMNLP2023/Crystal: Introspective Reasoners Reinforced with Self-Feedback.pdf,./data/imgs/EMNLP2023/Crystal: Introspective Reasoners Reinforced with Self-Feedback.png,6db6e6e71cc54435265643e19fcbdc7f3ba4c772,3.0,{2024: 3},12.0,"{2024: 11, 2023: 1}"
14543,WiCE: Real-World Entailment for Claims in Wikipedia,EMNLP,2023,"['Ryo Kamoi', 'Tanya Goyal', 'Juan Rodríguez', 'Greg Durrett']","Textual entailment models are increasingly applied in settings like fact-checking, presupposition verification in question answering, or summary evaluation. However, these represent a significant domain shift from existing entailment datasets, and models underperform as a result. We propose WiCE, a new fine-grained textual entailment dataset built on natural claim and evidence pairs extracted from Wikipedia. In addition to standard claim-level entailment, WiCE provides entailment judgments over sub-sentence units of the claim, and a minimal subset of evidence sentences that support each subclaim. To support this, we propose an automatic claim decomposition strategy using GPT-3.5 which we show is also effective at improving entailment models' performance on multiple datasets at test time. Finally, we show that real claims in our dataset involve challenging verification and retrieval problems that existing models fail to address.",./data/pdfs/EMNLP2023/WiCE: Real-World Entailment for Claims in Wikipedia.pdf,./data/imgs/EMNLP2023/WiCE: Real-World Entailment for Claims in Wikipedia.png,f7ec0e9237fbf960ffd20e2223e07287edb4f26a,10.0,"{2025: 3, 2024: 4, 2023: 3}",41.0,"{2024: 19, 2023: 21, 2021: 1}"
26352,That was fast! Speeding up NN search of high dimensional distributions,ICML,2013,"['Emanuele Coviello', 'Adeel Mumtaz', 'Antoni B. Chan', 'Gert Lanckriet']","We present a data structure for fast nearest neighbor retrieval of generative models of documents based on Kullback-Leibler (KL) divergence. Our data structure, which shares some similarity with Bregman Ball Trees, consists of a hierarchical partition of a database, and uses a novel branch and bound methodology for search. The main technical contribution of the paper is a novel and efficient algorithm for deciding whether to explore nodes during backtracking, based on a variational approximation. This reduces the number of computations per node, and overcomes the limitations of Bregman Ball Trees on high dimensional data. In addition, our strategy is applicable also to probability distributions with hidden state variables, and is not limited to regular exponential family distributions.

Experiments demonstrate substantial speedups over both Bregman Ball Trees and over brute force search, on both moderate and high dimensional histogram data. In addition, experiments on linear dynamical systems demonstrate the flexibility of our approach to latent variable models.",./data/pdfs/ICML2013/That was fast! Speeding up NN search of high dimensional distributions.pdf,./data/imgs/ICML2013/That was fast! Speeding up NN search of high dimensional distributions.png,7ddcef290b7ab20c009db241a882a0c87a216b9a,5.0,"{2020: 1, 2018: 2, 2017: 1, 2014: 1}",3.0,"{2020: 1, 2018: 1, 2015: 1}"
26427,Precision-recall space to correct external indices for biclustering,ICML,2013,"['Blaise Hanczar', 'Mohamed Nadif']","Biclustering is a major tool of data mining in many domains and many algorithms have emerged in recent years. All these algorithms aim to obtain coherent biclusters and it is crucial to have a reliable procedure for their validation. We point out the problem of size bias in biclustering evaluation and show how it can lead to wrong conclusions in a comparative study. We present the theoretical corrections for all of the most popular measures in order to remove this bias. We introduce the corrected precision-recall space that combines the advantages of corrected measures, the ease of interpretation and visualization of uncorrected measures. Numerical experiments demonstrate the interest of our approach.",./data/pdfs/ICML2013/Precision-recall space to correct external indices for biclustering.pdf,./data/imgs/ICML2013/Precision-recall space to correct external indices for biclustering.png,3aba5fe42c6ca2eed62fbed41c28aa5e33497790,7.0,"{2021: 1, 2019: 3, 2018: 1, 2016: 1, 2013: 1}",11.0,"{2023: 1, 2022: 1, 2021: 1, 2020: 1, 2019: 2, 2018: 2, 2016: 1, 2015: 1, 2013: 1}"
26245,Fast algorithms for sparse principal component analysis based on Rayleigh quotient iteration,ICML,2013,['Volodymyr Kuleshov'],"We introduce new algorithms for sparse principal component analysis (sPCA), a variation of PCA which aims to represent data in a sparse low-dimensional basis. Our algorithms possess a cubic rate of convergence and can compute principal components with k non-zero elements at a cost of O(nk + k3) flops per iteration. We observe in numerical experiments that these components are of equal or greater quality than ones obtained from current state-of-the-art techniques, but require between one and two orders of magnitude fewer flops to be computed. Conceptually, our approach generalizes the Rayleigh quotient iteration algorithm for computing eigenvectors, and can be viewed as a second-order optimization method. We demonstrate the applicability of our algorithms on several datasets, including the STL-10 machine vision dataset and gene expression data.",./data/pdfs/ICML2013/Fast algorithms for sparse principal component analysis based on Rayleigh quotient iteration.pdf,./data/imgs/ICML2013/Fast algorithms for sparse principal component analysis based on Rayleigh quotient iteration.png,0e07200748ce87676bcbd8b7eef0ae23966b4b25,18.0,"{2022: 1, 2021: 1, 2020: 2, 2019: 4, 2017: 1, 2016: 5, 2015: 2, 2013: 2}",25.0,"{2023: 2, 2022: 5, 2021: 1, 2020: 2, 2019: 4, 2017: 2, 2016: 4, 2015: 2, 2013: 3}"
26327,Better Rates for Any Adversarial Deterministic MDP,ICML,2013,"['Ofer Dekel', 'Elad Hazan']","We consider regret minimization in adversarial deterministic Markov Decision Processes (ADMDPs) with bandit feedback. We devise a new algorithm that pushes the state-of-the-art forward in two ways: First, it attains a regret of O(T2/3) with respect to the best fixed policy in hindsight, whereas the previous best regret bound was O(T3/4). Second, the algorithm and its analysis are compatible with any feasible ADMDP graph topology, while all previous approaches required additional restrictions on the graph topology.",./data/pdfs/ICML2013/Better Rates for Any Adversarial Deterministic MDP.pdf,./data/imgs/ICML2013/Better Rates for Any Adversarial Deterministic MDP.png,fbd1a58d43ebd671fa2b314561aba45bfd1b87a6,16.0,"{2023: 2, 2022: 1, 2021: 3, 2020: 2, 2019: 3, 2018: 2, 2014: 1, 2013: 2}",30.0,"{2024: 1, 2023: 6, 2022: 3, 2021: 5, 2020: 4, 2019: 6, 2018: 2, 2014: 1, 2013: 2}"
26454,Vanishing Component Analysis,ICML,2013,"['Roi Livni', 'David Lehavi', 'Sagi Schein', 'Hila Nachliely', 'Shai Shalev‐Shwartz', 'Amir Globerson']","The vanishing ideal of a set of points, S ⊂ Rn, is the set of all polynomials that attain the value of zero on all the points in S. Such ideals can be compactly represented using a small set of polynomials known as generators of the ideal. Here we describe and analyze an efficient procedure that constructs a set of generators of a vanishing ideal. Our procedure is numerically stable, and can be used to find approximately vanishing polynomials. The resulting polynomials capture nonlinear structure in data, and can for example be used within supervised learning. Empirical comparison with kernel methods show that our method constructs more compact classifiers with comparable accuracy.",./data/pdfs/ICML2013/Vanishing Component Analysis.pdf,./data/imgs/ICML2013/Vanishing Component Analysis.png,3e15ee53cbe47892b5991c8fb36fd65a3071ca0a,28.0,"{2021: 2, 2020: 2, 2019: 7, 2018: 5, 2017: 1, 2016: 3, 2014: 4, 2013: 4}",41.0,"{2023: 1, 2022: 5, 2021: 3, 2020: 2, 2019: 8, 2018: 5, 2017: 3, 2016: 5, 2015: 1, 2014: 5, 2013: 3}"
26339,Infinite Positive Semidefinite Tensor Factorization for Source Separation of Mixture Signals,ICML,2013,"['Kazuyoshi Yoshii', 'Ryota Tomioka', 'Daichi Mochihashi', 'Masataka Goto']","This paper presents a new class of tensor factorization called positive semidefinite tensor factorization (PSDTF) that decomposes a set of positive semidefinite (PSD) matrices into the convex combinations of fewer PSD basis matrices. PSDTF can be viewed as a natural extension of nonnegative matrix factorization. One of the main problems of PSDTF is that an appropriate number of bases should be given in advance. To solve this problem, we propose a nonparametric Bayesian model based on a gamma process that can instantiate only a limited number of necessary bases from the infinitely many bases assumed to exist. We derive a variational Bayesian algorithm for closed-form posterior inference and a multiplicative update rule for maximum-likelihood estimation. We evaluated PSDTF on both synthetic data and real music recordings to show its superiority.",./data/pdfs/ICML2013/Infinite Positive Semidefinite Tensor Factorization for Source Separation of Mixture Signals.pdf,./data/imgs/ICML2013/Infinite Positive Semidefinite Tensor Factorization for Source Separation of Mixture Signals.png,7d824583219da624c4c43e050a2df6412991b712,40.0,"{2022: 1, 2021: 4, 2020: 7, 2019: 5, 2018: 6, 2017: 2, 2016: 5, 2015: 3, 2014: 5, 2013: 1}",48.0,"{2023: 1, 2021: 7, 2020: 3, 2019: 9, 2018: 6, 2017: 2, 2016: 7, 2015: 5, 2014: 5, 2013: 3}"
26433,Generic Exploration and K-armed Voting Bandits,ICML,2013,"['Tanguy Urvoy', 'Fabrice Clérot', 'Raphael F raud', 'Sami Naamane']","We study a stochastic online learning scheme with partial feedback where the utility of decisions is only observable through an estimation of the environment parameters. We propose a generic pure-exploration algorithm, able to cope with various utility functions from multi-armed bandits settings to dueling bandits. The primary application of this setting is to offer a natural generalization of dueling bandits for situations where the environment parameters reflect the idiosyncratic preferences of a mixed crowd.",./data/pdfs/ICML2013/Generic Exploration and K-armed Voting Bandits.pdf,./data/imgs/ICML2013/Generic Exploration and K-armed Voting Bandits.png,933334cd73c452613daa302c887c38c577afe658,53.0,"{2022: 2, 2021: 3, 2020: 3, 2019: 6, 2018: 9, 2017: 3, 2016: 11, 2015: 9, 2014: 5, 2013: 2}",90.0,"{2024: 3, 2023: 6, 2022: 6, 2021: 5, 2020: 6, 2019: 9, 2018: 15, 2017: 8, 2016: 13, 2015: 13, 2014: 4, 2013: 2}"
26379,Estimating Unknown Sparsity in Compressed Sensing,ICML,2013,['Miles E. Lopes'],"In the theory of compressed sensing (CS), the sparsity ||x||0 of the unknown signal x ∈ Rp is commonly assumed to be a known parameter. However, it is typically unknown in practice. Due to the fact that many aspects of CS depend on knowing ||x||0, it is important to estimate this parameter in a data-driven way. A second practical concern is that ||x||0 is a highly unstable function of x. In particular, for real signals with entries not exactly equal to 0, the value ||x||0 = p is not a useful description of the effective number of coordinates. In this paper, we propose to estimate a stable measure of sparsity s(x) := ||x||12/||x||22, which is a sharp lower bound on ||x||0. Our estimation procedure uses only a small number of linear measurements, does not rely on any sparsity assumptions, and requires very little computation. A confidence interval for s(x) is provided, and its width is shown to have no dependence on the signal dimension p. Moreover, this result extends naturally to the matrix recovery setting, where a soft version of matrix rank can be estimated with analogous guarantees. Finally, we show that the use of randomized measurements is essential to estimating s(x). This is accomplished by proving that the minimax risk for estimating s(x) with deterministic measurements is large when n ≪ p.",./data/pdfs/ICML2013/Estimating Unknown Sparsity in Compressed Sensing.pdf,./data/imgs/ICML2013/Estimating Unknown Sparsity in Compressed Sensing.png,9be75ccb1f9c1885d3c60d1df9792eeee6021a45,89.0,"{2024: 1, 2023: 1, 2022: 3, 2021: 4, 2020: 13, 2019: 11, 2018: 19, 2017: 6, 2016: 9, 2015: 17, 2014: 3, 2013: 2}",108.0,"{2024: 4, 2023: 6, 2022: 5, 2021: 6, 2020: 12, 2019: 14, 2018: 18, 2017: 10, 2016: 10, 2015: 14, 2014: 4, 2013: 5}"
26329,Modeling Information Propagation with Survival Theory,ICML,2013,"['Manuel Gomez-Rodriguez', 'Jure Leskovec', 'Bernhard Schlkopf']","Networks provide a 'skeleton' for the spread of contagions, like, information, ideas, behaviors and diseases. Many times networks over which contagions diffuse are unobserved and need to be inferred. Here we apply survival theory to develop general additive and multiplicative risk models under which the network inference problems can be solved efficiently by exploiting their convexity. Our additive risk model generalizes several existing network inference models. We show all these models are particular cases of our more general model. Our multiplicative model allows for modeling scenarios in which a node can either increase or decrease the risk of activation of another node, in contrast with previous approaches, which consider only positive risk increments. We evaluate the performance of our network inference algorithms on large synthetic and real cascade datasets, and show that our models are able to predict the length and duration of cascades in real data.",./data/pdfs/ICML2013/Modeling Information Propagation with Survival Theory.pdf,./data/imgs/ICML2013/Modeling Information Propagation with Survival Theory.png,def7116f2817418e056b43fd140895b8a271fa6b,144.0,"{2024: 2, 2023: 1, 2022: 3, 2021: 13, 2020: 17, 2019: 19, 2018: 14, 2017: 23, 2016: 16, 2015: 21, 2014: 12, 2013: 3}",174.0,"{2024: 1, 2023: 6, 2022: 8, 2021: 13, 2020: 27, 2019: 18, 2018: 15, 2017: 19, 2016: 25, 2015: 22, 2014: 14, 2013: 6}"
26338,Revisiting the Nystrom method for improved large-scale machine learning,ICML,2013,"['Alex Gittens', 'Michael W. Mahoney']","We reconsider randomized algorithms for the low-rank approximation of symmetric positive semi-definite (SPSD) matrices such as Laplacian and kernel matrices that arise in data analysis and machine learning applications. Our main results consist of an empirical evaluation of the performance quality and running time of sampling and projection methods on a diverse suite of SPSD matrices. Our results highlight complementary aspects of sampling versus projection methods; they characterize the effects of common data preprocessing steps on the performance of these algorithms; and they point to important differences between uniform sampling and nonuniform sampling methods based on leverage scores. In addition, our empirical results illustrate that existing theory is so weak that it does not provide even a qualitative guide to practice. Thus, we complement our empirical results with a suite of worst-case theoretical bounds for both random sampling and random projection methods. These bounds are qualitatively superior to existing bounds---e.g. improved additive-error bounds for spectral and Frobenius norm error and relative-error bounds for trace norm error---and they point to future directions to make these algorithms useful in even larger-scale machine learning applications.",./data/pdfs/ICML2013/Revisiting the Nystrom method for improved large-scale machine learning.pdf,./data/imgs/ICML2013/Revisiting the Nystrom method for improved large-scale machine learning.png,7cda32aeefdd3cabd76871b8ee06bd1a1ea2ba10,184.0,"{2023: 2, 2022: 4, 2021: 21, 2020: 37, 2019: 36, 2018: 22, 2017: 19, 2016: 21, 2015: 21, 2014: 1}",383.0,"{2024: 11, 2023: 22, 2022: 37, 2021: 31, 2020: 48, 2019: 36, 2018: 32, 2017: 38, 2016: 47, 2015: 50, 2014: 26, 2013: 5}"
25988,The f-Adjusted Graph Laplacian: a Diagonal Modification with a Geometric Interpretation,ICML,2014,"['Sven Kurras', 'Ulrike von Luxburg', 'Gilles Blanchard']","Consider a neighborhood graph, for example a k-nearest neighbor graph, that is constructed on sample points drawn according to some density p. Our goal is to re-weight the graph's edges such that all cuts and volumes behave as if the graph was built on a different sample drawn from an alternative density p. We introduce the f -adjusted graph and prove that it provides the correct cuts and volumes as the sample size tends to infinity. From an algebraic perspective, we show that its normalized Laplacian, denoted as the f -adjusted Laplacian, represents a natural family of diagonal perturbations of the original normalized Laplacian. Our technique allows to apply any cut and volume based algorithm to the f -adjusted graph, for example spectral clustering, in order to study the given graph as if it were built on an unaccessible sample from a different density. We point out applications in sample bias correction, data uniformization, and multi-scale analysis of graphs.",./data/pdfs/ICML2014/The f-Adjusted Graph Laplacian: a Diagonal Modification with a Geometric Interpretation.pdf,./data/imgs/ICML2014/The f-Adjusted Graph Laplacian: a Diagonal Modification with a Geometric Interpretation.png,73d43b24fe49bb7b6098cc76c66df1e8ea4307c1,6.0,"{2019: 1, 2018: 1, 2017: 1, 2016: 1, 2015: 2}",8.0,"{2022: 1, 2019: 1, 2017: 2, 2016: 3, 2015: 1}"
26006,Robust and Efficient Kernel Hyperparameter Paths with Guarantees,ICML,2014,"['Joachim Giesen', 'Soeren Laue', 'Patrick Wieschollek']","Algorithmically, many machine learning tasks boil down to solving parameterized optimization problems. The choice of the parameter values in these problems can have a significant influence on the statistical performance of the corresponding methods. Thus, algorithmic support for choosing good parameter values has received quite some attention recently, especially algorithms for computing the whole solution path of a parameterized optimization problem. These algorithms can be used, for instance, to track the solution of a regularized learning problem along the regularization parameter path, or for tracking the solution of kernelized problems along a kernel hyperparameter path. Since exact path following algorithms can be numerically unstable, robust and efficient approximate path tracking algorithms have gained in popularity for regularized learning problems. By now algorithms with optimal path complexity in terms of a guaranteed approximation error are known for many regularized learning problems. That is not the case for kernel hyperparameter path tracking algorithms, where the exact path tracking algorithms can also suffer from numerical problems. Here we address this problem by devising a robust and efficient path tracking algorithm that can also handle kernel hyperparameter paths. The algorithm has asymptotically optimal complexity. We use this algorithm to compute approximate kernel hyperparamter solution paths for support vector machines and robust kernel regression. Experimental results for these problems applied to various data sets confirm the theoretical complexity analysis.",./data/pdfs/ICML2014/Robust and Efficient Kernel Hyperparameter Paths with Guarantees.pdf,./data/imgs/ICML2014/Robust and Efficient Kernel Hyperparameter Paths with Guarantees.png,bd1fa721d789e6d24facd4188418a4419ae0bdae,7.0,"{2022: 1, 2017: 1, 2016: 2, 2015: 3}",12.0,"{2022: 2, 2021: 1, 2018: 2, 2016: 2, 2015: 5}"
25963,Learning by Stretching Deep Networks,ICML,2014,"['Gaurav Pandey', 'Ambedkar Dukkipati']","In recent years, deep architectures have gained a lot of prominence for learning complex AI tasks because of their capability to incorporate complex variations in data within the model. However, these models often need to be trained for a long time in order to obtain good results. In this paper, we propose a technique, called 'stretching', that allows the same models to perform considerably better with very little training. We show that learning can be done tractably, even when the weight matrix is stretched to infinity, for some specific models. We also study tractable algorithms for implementing stretching in deep convolutional architectures in an iterative manner and derive bounds for its convergence. Our experimental results suggest that the proposed stretched deep convolutional networks are capable of achieving good performance for many object recognition tasks. More importantly, for a fixed network architecture, one can achieve much better accuracy using stretching rather than learning the weights using backpropagation.",./data/pdfs/ICML2014/Learning by Stretching Deep Networks.pdf,./data/imgs/ICML2014/Learning by Stretching Deep Networks.png,6fe78db480995464bd97ba3b712ecc82129e6179,21.0,"{2024: 2, 2021: 1, 2020: 1, 2019: 5, 2018: 2, 2017: 3, 2016: 2, 2015: 3, 2014: 2}",22.0,"{2024: 1, 2022: 1, 2021: 1, 2020: 1, 2019: 7, 2018: 1, 2017: 4, 2016: 1, 2015: 2, 2014: 3}"
25964,Active Learning of Parameterized Skills,ICML,2014,"['Bruno da Silva', 'George Konidaris', 'Andrew G. Barto']","We introduce a method for actively learning parameterized skills. Parameterized skills are flexible behaviors that can solve any task drawn from a distribution of parameterized reinforcement learning problems. Approaches to learning such skills have been proposed, but limited attention has been given to identifying which training tasks allow for rapid skill acquisition. We construct a non-parametric Bayesian model of skill performance and derive analytical expressions for a novel acquisition criterion capable of identifying tasks that maximize expected improvement in skill performance. We also introduce a spatiotemporal kernel tailored for non-stationary skill performance models. The proposed method is agnostic to policy and skill representation and scales independently of task dimensionality. We evaluate it on a non-linear simulated catapult control problem over arbitrarily mountainous terrains.",./data/pdfs/ICML2014/Active Learning of Parameterized Skills.pdf,./data/imgs/ICML2014/Active Learning of Parameterized Skills.png,160216cfd90e01db60f33c5e5b4fb869bae34d47,22.0,"{2023: 2, 2021: 1, 2020: 2, 2019: 6, 2018: 4, 2017: 1, 2016: 3, 2015: 2, 2014: 1}",28.0,"{2024: 1, 2023: 3, 2022: 1, 2021: 2, 2020: 3, 2019: 5, 2018: 4, 2017: 3, 2016: 2, 2015: 3, 2014: 1}"
26097,Consistency of Causal Inference under the Additive Noise Model,ICML,2014,"['Samory Kpotufe', 'Eleni Sgouritsa', 'Dominik Janzing', 'Bernhard Schoelkopf']","We analyze a family of methods for statistical causal inference from sample under the socalled Additive Noise Model. While most work on the subject has concentrated on establishing the soundness of the Additive Noise Model, the statistical consistency of the resulting inference methods has received little attention. We derive general conditions under which the given family of inference methods consistently infers the causal direction in a nonparametric setting.",./data/pdfs/ICML2014/Consistency of Causal Inference under the Additive Noise Model.pdf,./data/imgs/ICML2014/Consistency of Causal Inference under the Additive Noise Model.png,e6b6196fd4adb5f794c27b129d9a05a8f3408c96,29.0,"{2021: 5, 2020: 2, 2019: 5, 2018: 2, 2016: 2, 2015: 9, 2014: 4}",42.0,"{2023: 2, 2022: 2, 2021: 8, 2020: 6, 2019: 3, 2018: 3, 2016: 5, 2015: 8, 2014: 4, 2013: 1}"
26002,Stable and Efficient Representation Learning with Nonnegativity Constraints,ICML,2014,"['Tsung-Han Lin', 'H. T. Kung']","Orthogonal matching pursuit (OMP) is an efficient approximation algorithm for computing sparse representations. However, prior research has shown that the representations computed by OMP may be of inferior quality, as they deliver suboptimal classification accuracy on several image datasets. We have found that this problem is caused by OMP's relatively weak stability under data variations, which leads to unreliability in supervised classifier training. We show that by imposing a simple nonnegativity constraint, this nonnegative variant of OMP (NOMP) can mitigate OMP's stability issue and is resistant to noise overfitting. In this work, we provide extensive analysis and experimental results to examine and validate the stability advantage of NOMP. In our experiments, we use a multi-layer deep architecture for representation learning, where we use K-means for feature learning and NOMP for representation encoding. The resulting learning framework is not only efficient and scalable to large feature dictionaries, but also is robust against input noise. This framework achieves the state-of-the-art accuracy on the STL-10 dataset.",./data/pdfs/ICML2014/Stable and Efficient Representation Learning with Nonnegativity Constraints.pdf,./data/imgs/ICML2014/Stable and Efficient Representation Learning with Nonnegativity Constraints.png,ecb9200691f97de4c17243eb73f5fa6a33656827,30.0,"{2022: 1, 2021: 3, 2020: 1, 2019: 3, 2018: 1, 2017: 7, 2016: 4, 2015: 9, 2014: 1}",56.0,"{2023: 1, 2022: 1, 2021: 3, 2020: 3, 2019: 4, 2018: 3, 2017: 9, 2016: 14, 2015: 13, 2014: 5}"
25960,Learning Ordered Representations with Nested Dropout,ICML,2014,"['Oren Rippel', 'Michael A. Gelbart', 'Ryan P. Adams']","In this paper, we present results on ordered representations of data in which different dimensions have different degrees of importance. To learn these representations we introduce nested dropout, a procedure for stochastically removing coherent nested sets of hidden units in a neural network. We first present a sequence of theoretical results for the special case of a semilinear autoencoder. We rigorously show that the application of nested dropout enforces identifiability of the units, which leads to an exact equivalence with PCA. We then extend the algorithm to deep models and demonstrate the relevance of ordered representations to a number of applications. Specifically, we use the ordered property of the learned codes to construct hash-based data structures that permit very fast retrieval, achieving retrieval in time logarithmic in the database size and independent of the dimensionality of the representation. This allows codes that are hundreds of times longer than currently feasible for retrieval. We therefore avoid the diminished quality associated with short codes, while still performing retrieval that is competitive in speed with existing methods. We also show that ordered representations are a promising way to learn adaptive compression for efficient online data reconstruction.",./data/pdfs/ICML2014/Learning Ordered Representations with Nested Dropout.pdf,./data/imgs/ICML2014/Learning Ordered Representations with Nested Dropout.png,7a6fd5573d2679506765d461ec4892fd4017b745,49.0,"{2021: 11, 2020: 8, 2019: 5, 2018: 7, 2017: 1, 2016: 8, 2015: 5, 2014: 4}",75.0,"{2024: 5, 2023: 11, 2022: 9, 2021: 9, 2020: 8, 2019: 5, 2018: 8, 2017: 2, 2016: 10, 2015: 5, 2014: 3}"
26061,Coupled Group Lasso for Web-Scale CTR Prediction in Display Advertising,ICML,2014,"['Ling Yan', 'Wu-Jun Li', 'Gui-Rong Xue', 'Dingyi Han']","In display advertising, click through rate (CTR) prediction is the problem of estimating the probability that an advertisement (ad) is clicked when displayed to a user in a specific context. Due to its easy implementation and promising performance, logistic regression (LR) model has been widely used for CTR prediction, especially in industrial systems. However, it is not easy for LR to capture the nonlinear information, such as the conjunction information, from user features and ad features. In this paper, we propose a novel model, called coupled group lasso (CGL), for CTR prediction in display advertising. CGL can seamlessly integrate the conjunction information from user features and ad features for modeling. Furthermore, CGL can automatically eliminate useless features for both users and ads, which may facilitate fast online prediction. Scalability of CGL is ensured through feature hashing and distributed implementation. Experimental results on real-world data sets show that our CGL model can achieve state-of-the-art performance on webscale CTR prediction tasks.",./data/pdfs/ICML2014/Coupled Group Lasso for Web-Scale CTR Prediction in Display Advertising.pdf,./data/imgs/ICML2014/Coupled Group Lasso for Web-Scale CTR Prediction in Display Advertising.png,c14ef7b146b13086159c5ce104db046a27b3e21c,74.0,"{2024: 1, 2023: 4, 2022: 5, 2021: 15, 2020: 12, 2019: 6, 2018: 12, 2017: 8, 2016: 8, 2015: 3}",107.0,"{2024: 3, 2023: 17, 2022: 18, 2021: 15, 2020: 11, 2019: 7, 2018: 11, 2017: 11, 2016: 8, 2015: 6}"
25954,Scalable Bayesian Low-Rank Decomposition of Incomplete Multiway Tensors,ICML,2014,"['Piyush Rai', 'Yingjian Wang', 'Shengbo Guo', 'Gary Chen', 'David B. Dunson', 'Lawrence Carin']","We present a scalable Bayesian framework for low-rank decomposition of multiway tensor data with missing observations. The key issue of pre-specifying the rank of the decomposition is sidestepped in a principled manner using a multiplicative gamma process prior. Both continuous and binary data can be analyzed under the framework, in a coherent way using fully conjugate Bayesian analysis. In particular, the analysis in the non-conjugate binary case is facilitated via the use of the Polya-Gamma sampling strategy which elicits closed-form Gibbs sampling updates. The resulting samplers are efficient and enable us to apply our framework to large-scale problems, with time-complexity that is linear in the number of observed entries in the tensor. This is especially attractive in analyzing very large but sparsely observed tensors with very few known entries. Moreover, our method admits easy extension to the supervised setting where entities in one or more tensor modes have labels. Our method outperforms several state-of-the-art tensor decomposition methods on various synthetic and benchmark real-world datasets.",./data/pdfs/ICML2014/Scalable Bayesian Low-Rank Decomposition of Incomplete Multiway Tensors.pdf,./data/imgs/ICML2014/Scalable Bayesian Low-Rank Decomposition of Incomplete Multiway Tensors.png,e5fa438acada4b7ad95c6abf4ce841989cc8acef,105.0,"{2024: 2, 2023: 1, 2022: 8, 2021: 14, 2020: 7, 2019: 9, 2018: 17, 2017: 13, 2016: 18, 2015: 13, 2014: 3}",126.0,"{2024: 2, 2023: 7, 2022: 11, 2021: 14, 2020: 14, 2019: 7, 2018: 17, 2017: 15, 2016: 21, 2015: 13, 2014: 5}"
26070,Online Clustering of Bandits,ICML,2014,"['Claudio Gentile', 'Shuai Li', 'Giovanni Zappella']","We introduce a novel algorithmic approach to content recommendation based on adaptive clustering of exploration-exploitation bandit) strategies. We provide a sharp regret analysis of this algorithm in a standard stochastic noise setting, demonstrate its scalability properties, and prove its effectiveness on a number of artificial and real-world datasets. Our experiments show a significant increase in prediction performance over state-of-the-art methods for bandit problems.",./data/pdfs/ICML2014/Online Clustering of Bandits.pdf,./data/imgs/ICML2014/Online Clustering of Bandits.png,253254f518631ab21cad201f3448b48e385eff28,101.0,"{2024: 5, 2023: 12, 2022: 5, 2021: 22, 2020: 16, 2019: 10, 2018: 11, 2017: 8, 2016: 5, 2015: 6, 2014: 1}",242.0,"{2024: 7, 2023: 32, 2022: 25, 2021: 39, 2020: 36, 2019: 18, 2018: 20, 2017: 24, 2016: 26, 2015: 13, 2014: 2}"
26529,Inference in a Partially Observed Queuing Model with Applications in Ecology,ICML,2015,"['Kevin Winner', 'Garrett Bernstein', 'Dan Sheldon']","We consider the problem of inference in a probabilistic model for transient populations where we wish to learn about arrivals, departures, and population size over all time, but the only available data are periodic counts of the population size at specific observation times. The underlying model arises in queueing theory (as an Mt/G/∞ queue) and also in ecological models for short-lived animals such as insects. Our work applies to both systems. Previous work in the ecology literature focused on maximum likelihood estimation and made a simplifying independence assumption that prevents inference over unobserved random variables such as arrivals and departures. The contribution of this paper is to formulate a latent variable model and develop a novel Gibbs sampler based on Markov bases to perform inference using the correct, but intractable, likelihood function. We empirically validate the convergence behavior of our sampler and demonstrate the ability of our model to make much finer-grained inferences than the previous approach.",./data/pdfs/ICML2015/Inference in a Partially Observed Queuing Model with Applications in Ecology.pdf,./data/imgs/ICML2015/Inference in a Partially Observed Queuing Model with Applications in Ecology.png,3ca8c9f2a496170c7c21f4f07bdf82b5231797fc,1.0,{2016: 1},4.0,"{2022: 1, 2020: 1, 2017: 1, 2016: 1}"
26789,A low variance consistent test of relative dependency,ICML,2015,"['Wacha Bounliphone', 'Arthur Gretton', 'Arthur Tenenhaus', 'Matthew B. Blaschko']","We describe a novel non-parametric statistical hypothesis test of relative dependence between a source variable and two candidate target variables. Such a test enables us to determine whether one source variable is significantly more dependent on a first target variable or a second. Dependence is measured via the Hilbert-Schmidt Independence Criterion (HSIC), resulting in a pair of empirical dependence measures (source-target 1, source-target 2). We test whether the first dependence measure is significantly larger than the second. Modeling the covariance between these HSIC statistics leads to a provably more powerful test than the construction of independent HSIC statistics by sub-sampling. The resulting test is consistent and unbiased, and (being based on U-statistics) has favorable convergence properties. The test can be computed in quadratic time, matching the computational complexity of standard empirical HSIC estimators. The effectiveness of the test is demonstrated on several real-world problems: we identify language groups from a multilingual corpus, and we prove that tumor location is more dependent on gene expression than chromosomal imbalances. Source code is available for download at https://github.com/wbounliphone/reldep.",./data/pdfs/ICML2015/A low variance consistent test of relative dependency.pdf,./data/imgs/ICML2015/A low variance consistent test of relative dependency.png,5f0cc224b809981ae3ab0876fc48dbbf7c11c4f7,4.0,"{2021: 1, 2020: 1, 2017: 1, 2014: 1}",8.0,"{2021: 1, 2019: 2, 2016: 3, 2015: 2}"
26759,Generalization error bounds for learning to rank: Does the length of document lists matter?,ICML,2015,"['Ambuj Tewari', 'Sougata Chaudhuri']","We consider the generalization ability of algorithms for learning to rank at a query level, a problem also called subset ranking. Existing generalization error bounds necessarily degrade as the size of the document list associated with a query increases. We show that such a degradation is not intrinsic to the problem. For several loss functions, including the cross-entropy loss used in the well known ListNet method, there is no degradation in generalization ability as document lists become longer. We also provide novel generalization error bounds under l1 regularization and faster convergence rates if the loss function is smooth.",./data/pdfs/ICML2015/Generalization error bounds for learning to rank: Does the length of document lists matter?.pdf,./data/imgs/ICML2015/Generalization error bounds for learning to rank: Does the length of document lists matter?.png,76a0ec787ce1fe87044d3834928fabc8dd4cdca2,8.0,"{2021: 1, 2020: 1, 2019: 1, 2018: 1, 2017: 1, 2016: 2, 2015: 1}",14.0,"{2024: 1, 2023: 2, 2022: 1, 2021: 1, 2020: 1, 2019: 1, 2018: 1, 2017: 3, 2016: 1, 2015: 2}"
26627,On the Rate of Convergence and Error Bounds for LSTD(λ),ICML,2015,"['Manel Tagorti', 'Bruno Scherrer']","We consider LSTD(λ), the least-squares temporal-difference algorithm with eligibility traces algorithm proposed by Boyan (2002). It computes a linear approximation of the value function of a fixed policy in a large Markov Decision Process. Under a β-mixing assumption, we derive, for any value of λ e (0; 1), a high-probability bound on the rate of convergence of this algorithm to its limit. We deduce a high-probability bound on the error of this algorithm, that extends (and slightly improves) that derived by Lazaric et al. (2012) in the specific case where λ = 0. In the context of temporal-difference algorithms with value function approximation, this analysis is to our knowledge the first to provide insight on the choice of the eligibility-trace parameter λ with respect to the approximation quality of the space and the number of samples.",./data/pdfs/ICML2015/On the Rate of Convergence and Error Bounds for LSTD(λ).pdf,./data/imgs/ICML2015/On the Rate of Convergence and Error Bounds for LSTD(λ).png,c234c06ed44b3a9b8670b556fec7af881c4c2cd6,14.0,"{2021: 1, 2020: 1, 2019: 6, 2018: 1, 2016: 4, 2015: 1}",31.0,"{2024: 2, 2023: 3, 2022: 1, 2021: 1, 2020: 3, 2019: 8, 2018: 4, 2016: 5, 2015: 3, 2013: 1}"
26528,A trust-region method for stochastic variational inference with applications to streaming data,ICML,2015,"['Lucas Theis', 'Matt Hoffman']","Stochastic variational inference allows for fast posterior inference in complex Bayesian models. However, the algorithm is prone to local optima which can make the quality of the posterior approximation sensitive to the choice of hyperparameters and initialization. We address this problem by replacing the natural gradient step of stochastic varitional inference with a trust-region update. We show that this leads to generally better results and reduced sensitivity to hyperparameters. We also describe a new strategy for variational inference on streaming data and show that here our trust-region method is crucial for getting good performance.",./data/pdfs/ICML2015/A trust-region method for stochastic variational inference with applications to streaming data.pdf,./data/imgs/ICML2015/A trust-region method for stochastic variational inference with applications to streaming data.png,75f18a713bc47b1abdcc18564b1185342069283b,11.0,"{2023: 1, 2021: 2, 2020: 2, 2019: 1, 2018: 1, 2017: 1, 2016: 1, 2015: 2}",40.0,"{2024: 1, 2022: 8, 2021: 4, 2020: 6, 2019: 6, 2018: 3, 2017: 3, 2016: 4, 2015: 5}"
26705,Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions,ICML,2015,"['Alina Ene', 'Huy L. Nguyễn']","Submodular function minimization is a fundamental optimization problem that arises in several applications in machine learning and computer vision. The problem is known to be solvable in polynomial time, but general purpose algorithms have high running times and are unsuitable for large-scale problems. Recent work have used convex optimization techniques to obtain very practical algorithms for minimizing functions that are sums of ``simple functions. In this paper, we use random coordinate descent methods to obtain algorithms with faster linear convergence rates and cheaper iteration costs. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations and they converge in significantly fewer iterations.",./data/pdfs/ICML2015/Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions.pdf,./data/imgs/ICML2015/Random Coordinate Descent Methods for Minimizing Decomposable Submodular Functions.png,d770cb3a55e9f3c8d04cbcf58cc7c266f0352836,19.0,"{2022: 1, 2021: 2, 2020: 3, 2019: 2, 2018: 4, 2017: 6, 2016: 1}",42.0,"{2024: 1, 2023: 5, 2022: 3, 2021: 2, 2020: 6, 2019: 5, 2018: 6, 2017: 8, 2016: 4, 2015: 2}"
26694,Support Matrix Machines,ICML,2015,"['Luo Luo', 'Yubo Xie', 'Zhihua Zhang', 'Wu-Jun Li']","In many classification problems such as electroencephalogram (EEG) classification and image classification, the input features are naturally represented as matrices rather than vectors or scalars. In general, the structure information of the original feature matrix is useful and informative for data analysis tasks such as classification. One typical structure information is the correlation between columns or rows in the feature matrix. To leverage this kind of structure information, we propose a new classification method that we call support matrix machine (SMM). Specifically, SMM is defined as a hinge loss plus a so-called spectral elastic net penalty which is a spectral extension of the conventional elastic net over a matrix. The spectral elastic net enjoys a property of grouping effect, i.e., strongly correlated columns or rows tend to be selected altogether or not. Since the optimization problem for SMM is convex, this encourages us to devise an alternating direction method of multipliers (ADMM) algorithm for solving the problem. Experimental results on EEG and image classification data show that our model is more robust and efficient than the state-of-the-art methods.",./data/pdfs/ICML2015/Support Matrix Machines.pdf,./data/imgs/ICML2015/Support Matrix Machines.png,ff37cc844c6625ec8c41e8cc54943fdd1f18fee9,60.0,"{2024: 3, 2023: 2, 2022: 5, 2021: 7, 2020: 10, 2019: 12, 2018: 6, 2017: 8, 2016: 5, 2015: 1, 2013: 1}",81.0,"{2024: 3, 2023: 9, 2022: 14, 2021: 9, 2020: 12, 2019: 12, 2018: 9, 2017: 8, 2016: 3, 2015: 1, 2014: 1}"
26542,PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent,ICML,2015,"['Cho‐Jui Hsieh', 'Hsiang‐Fu Yu', 'Inderjit S. Dhillon']","Stochastic Dual Coordinate Descent (SDCD) has become one of the most efficient ways to solve the family of $\ell_2$-regularized empirical risk minimization problems, including linear SVM, logistic regression, and many others. The vanilla implementation of DCD is quite slow; however, by maintaining primal variables while updating dual variables, the time complexity of SDCD can be significantly reduced. Such a strategy forms the core algorithm in the widely-used LIBLINEAR package. In this paper, we parallelize the SDCD algorithms in LIBLINEAR. In recent research, several synchronized parallel SDCD algorithms have been proposed, however, they fail to achieve good speedup in the shared memory multi-core setting. In this paper, we propose a family of asynchronous stochastic dual coordinate descent algorithms (ASDCD). Each thread repeatedly selects a random dual variable and conducts coordinate updates using the primal variables that are stored in the shared memory. We analyze the convergence properties when different locking/atomic mechanisms are applied. For implementation with atomic operations, we show linear convergence under mild conditions. For implementation without any atomic operations or locking, we present the first {\it backward error analysis} for ASDCD under the multi-core environment, showing that the converged solution is the exact solution for a primal problem with perturbed regularizer. Experimental results show that our methods are much faster than previous parallel coordinate descent solvers.",./data/pdfs/ICML2015/PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent.pdf,./data/imgs/ICML2015/PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent.png,ee42544fb9585f8649117d1ffbcebd68f50d3a4d,49.0,"{2021: 2, 2020: 2, 2019: 4, 2018: 9, 2017: 11, 2016: 12, 2015: 8, 2014: 1}",104.0,"{2023: 1, 2022: 3, 2021: 3, 2020: 9, 2019: 12, 2018: 16, 2017: 20, 2016: 26, 2015: 13, 2014: 1}"
26748,A General Analysis of the Convergence of ADMM,ICML,2015,"['Robert Nishihara', 'Laurent Lessard', 'Benjamin Recht', 'Andrew Packard', 'Michael I. Jordan']","We provide a new proof of the linear convergence of the alternating direction method of multipliers (ADMM) when one of the objective terms is strongly convex. Our proof is based on a framework for analyzing optimization algorithms introduced in Lessard et al. (2014), reducing algorithm convergence to verifying the stability of a dynamical system. This approach generalizes a number of existing results and obviates any assumptions about specific choices of algorithm parameters. On a numerical example, we demonstrate that minimizing the derived bound on the convergence rate provides a practical approach to selecting algorithm parameters for particular ADMM instances. We complement our upper bound by constructing a nearly-matching lower bound on the worst-case rate of convergence.",./data/pdfs/ICML2015/A General Analysis of the Convergence of ADMM.pdf,./data/imgs/ICML2015/A General Analysis of the Convergence of ADMM.png,aecf628ad1f28b8ae903bcfbd45af05c05c511fa,169.0,"{2025: 2, 2024: 2, 2023: 6, 2022: 13, 2021: 11, 2020: 29, 2019: 24, 2018: 16, 2017: 30, 2016: 20, 2015: 15, 2014: 1}",274.0,"{2024: 12, 2023: 25, 2022: 32, 2021: 24, 2020: 29, 2019: 37, 2018: 28, 2017: 37, 2016: 30, 2015: 18, 2014: 2}"
26782,Learning Transferable Features with Deep Adaptation Networks,ICML,2015,"['Mingsheng Long', 'Yue Cao', 'Jianmin Wang', 'Michael I. Jordan']","Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.",./data/pdfs/ICML2015/Learning Transferable Features with Deep Adaptation Networks.pdf,./data/imgs/ICML2015/Learning Transferable Features with Deep Adaptation Networks.png,7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7,2552.0,"{2025: 15, 2024: 100, 2023: 262, 2022: 310, 2021: 633, 2020: 509, 2019: 428, 2018: 170, 2017: 76, 2016: 37, 2015: 4, 2014: 1}",4514.0,"{2024: 274, 2023: 768, 2022: 743, 2021: 796, 2020: 755, 2019: 545, 2018: 325, 2017: 193, 2016: 91, 2015: 20, 2014: 4}"
19053,Black-box Optimization with a Politician,ICML,2016,"['Sébastien Bubeck', 'Yin-Tat Lee']","We propose a new framework for black-box convex optimization which is well-suited for situations where gradient computations are expensive. We derive a new method for this framework which leverages several concepts from convex optimization, from standard first-order methods (e.g. gradient descent or quasi-Newton methods) to analytical centers (i.e. minimizers of self-concordant barriers). We demonstrate empirically that our new technique compares favorably with state of the art algorithms (such as BFGS).",./data/pdfs/ICML2016/Black-box Optimization with a Politician.pdf,./data/imgs/ICML2016/Black-box Optimization with a Politician.png,399c70ae8224f0485a07c8174c0e9b32f144b632,3.0,"{2022: 2, 2016: 1}",8.0,"{2022: 2, 2021: 1, 2020: 1, 2018: 2, 2016: 2}"
19043,Gaussian quadrature for matrix inverse forms with applications,ICML,2016,"['Chengtao Li', 'Suvrit Sra', 'Stefanie Jegelka']","We present a framework for accelerating a spectrum of machine learning algorithms that require computation of bilinear inverse forms uτ A-1 u, where A is a positive definite matrix and u a given vector. Our framework is built on Gausstype quadrature and easily scales to large, sparse matrices. Further, it allows retrospective computation of lower and upper bounds on uτ A-1 u, which in turn accelerates several algorithms. We prove that these bounds tighten iteratively and converge at a linear (geometric) rate. To our knowledge, ours is the first work to demonstrate these key properties of Gauss-type quadrature, which is a classical and deeply studied topic. We illustrate empirical consequences of our results by using quadrature to accelerate machine learning tasks involving determinantal point processes and submodular optimization, and observe tremendous speedups in several instances.",./data/pdfs/ICML2016/Gaussian quadrature for matrix inverse forms with applications.pdf,./data/imgs/ICML2016/Gaussian quadrature for matrix inverse forms with applications.png,a020817c63d3cbad470d23881deffc296dba6806,11.0,"{2023: 1, 2021: 1, 2020: 1, 2019: 3, 2017: 2, 2016: 3}",15.0,"{2023: 1, 2021: 2, 2020: 2, 2019: 2, 2017: 5, 2016: 3}"
19203,Minimum Regret Search for Single- and Multi-Task Optimization,ICML,2016,['Jan Hendrik Metzen'],"We propose minimum regret search (MRS), a novel acquisition function for Bayesian optimization. MRS bears similarities with information-theoretic approaches such as entropy search (ES). However, while ES aims in each query at maximizing the information gain with respect to the global maximum, MRS aims at minimizing the expected simple regret of its ultimate recommendation for the optimum. While empirically ES and MRS perform similar in most of the cases, MRS produces fewer outliers with high simple regret than ES. We provide empirical results both for a synthetic single-task optimization problem as well as for a simulated multi-task robotic control problem.",./data/pdfs/ICML2016/Minimum Regret Search for Single- and Multi-Task Optimization.pdf,./data/imgs/ICML2016/Minimum Regret Search for Single- and Multi-Task Optimization.png,a750822a1f2db405487ad814ce15e58fc626e62d,8.0,"{2021: 1, 2020: 1, 2019: 2, 2017: 3, 2016: 1}",20.0,"{2022: 3, 2021: 1, 2020: 3, 2019: 5, 2018: 2, 2017: 4, 2016: 2}"
19185,Binary embeddings with structured hashed projections,ICML,2016,"['Anna Choromanska', 'Krzysztof Choromański', 'Mariusz Bojarski', 'Tony Jebara', 'Sanjiv Kumar', 'Yann LeCun']","We consider the hashing mechanism for constructing binary embeddings, that involves pseudo-random projections followed by nonlinear (sign function) mappings. The pseudorandom projection is described by a matrix, where not all entries are independent random variables but instead a fixed budget of is distributed across the matrix. Such matrices can be efficiently stored in sub-quadratic or even linear space, provide reduction in randomness usage (i.e. number of required random values), and very often lead to computational speed ups. We prove several theoretical results showing that projections via various structured matrices followed by nonlinear mappings accurately preserve the angular distance between input high-dimensional vectors. To the best of our knowledge, these results are the first that give theoretical ground for the use of general structured matrices in the nonlinear setting. We empirically verify our theoretical findings and show the dependence of learning via structured hashed projections on the performance of neural network as well as nearest neighbor classifier.",./data/pdfs/ICML2016/Binary embeddings with structured hashed projections.pdf,./data/imgs/ICML2016/Binary embeddings with structured hashed projections.png,16a26289d7c37e6a0179fa57b14a327286696d33,27.0,"{2021: 3, 2020: 4, 2019: 5, 2018: 6, 2017: 7, 2016: 2}",34.0,"{2023: 1, 2021: 4, 2020: 2, 2019: 5, 2018: 8, 2017: 7, 2016: 5, 2015: 2}"
18983,Efficient Multi-Instance Learning for Activity Recognition from Time Series Data Using an Auto-Regressive Hidden Markov Model,ICML,2016,"['Xinze Guan', 'Raviv Raich', 'Weng‐Keen Wong']","Activity recognition from sensor data has spurred a great deal of interest due to its impact on health care. Prior work on activity recognition from multivariate time series data has mainly applied supervised learning techniques which require a high degree of annotation effort to produce training data with the start and end times of each activity. In order to reduce the annotation effort, we present a weakly supervised approach based on multi-instance learning. We introduce a generative graphical model for multi-instance learning on time series data based on an auto-regressive hidden Markov model. Our model has a number of advantages, including the ability to produce both bag and instance-level predictions as well as an efficient exact inference algorithm based on dynamic programming.",./data/pdfs/ICML2016/Efficient Multi-Instance Learning for Activity Recognition from Time Series Data Using an Auto-Regressive Hidden Markov Model.pdf,./data/imgs/ICML2016/Efficient Multi-Instance Learning for Activity Recognition from Time Series Data Using an Auto-Regressive Hidden Markov Model.png,320e01baed6e4d42d088d3a876c19741cc464355,35.0,"{2023: 4, 2021: 5, 2020: 8, 2019: 2, 2018: 6, 2017: 10}",42.0,"{2024: 1, 2023: 5, 2022: 4, 2021: 4, 2020: 6, 2019: 3, 2018: 7, 2017: 10, 2016: 1, 2014: 1}"
19172,Efficient Private Empirical Risk Minimization for High-dimensional Learning,ICML,2016,"['Shiva Prasad Kasiviswanathan', 'Hongxia Jin']","Dimensionality reduction is a popular approach for dealing with high dimensional data that leads to substantial computational savings. Random projections are a simple and effective method for universal dimensionality reduction with rigorous theoretical guarantees. In this paper, we theoretically study the problem of differentially private empirical risk minimization in the projected subspace (compressed domain). We ask: is it possible to design differentially private algorithms with small excess risk given access to only projected data? In this paper, we answer this question in affirmative, by showing that for the class of generalized linear functions, given only the projected data and the projection matrix, we can obtain excess risk bounds of O(w(C)2/3/n1/3) under e-differential privacy, and O(√w(C)/n) under (eδ)-differential privacy, where n is the sample size and w(C) is the Gaussian width of the parameter space C that we optimize over. A simple consequence of these results is that, for a large class of ERM problems, in the traditional setting (i.e., with access to the original data), under e-differential privacy, we improve the worst-case risk bounds of (Bassily et al., 2014).",./data/pdfs/ICML2016/Efficient Private Empirical Risk Minimization for High-dimensional Learning.pdf,./data/imgs/ICML2016/Efficient Private Empirical Risk Minimization for High-dimensional Learning.png,e49d542882126fc6f5a94fd154ea095505bd876a,33.0,"{2024: 1, 2022: 3, 2021: 9, 2020: 5, 2019: 6, 2018: 6, 2017: 3}",56.0,"{2024: 3, 2023: 7, 2022: 5, 2021: 13, 2020: 5, 2019: 6, 2018: 12, 2017: 5}"
18957,Dynamic Capacity Networks,ICML,2016,"['Amjad Almahairi', 'Nicolas Ballas', 'Tim Cooijmans', 'Yin Zheng', 'Hugo Larochelle', 'Aaron Courville']","We introduce the Dynamic Capacity Network (DCN), a neural network that can adaptively assign its capacity across different portions of the input data. This is achieved by combining modules of two types: low-capacity subnetworks and high-capacity sub-networks. The low-capacity sub-networks are applied across most of the input, but also provide a guide to select a few portions of the input on which to apply the high-capacity sub-networks. The selection is made using a novel gradient-based attention mechanism, that efficiently identifies input regions for which the DCN's output is most sensitive and to which we should devote more capacity. We focus our empirical evaluation on the Cluttered MNIST and SVHN image datasets. Our findings indicate that DCNs are able to drastically reduce the number of computations, compared to traditional convolutional neural networks, while maintaining similar or even better performance.",./data/pdfs/ICML2016/Dynamic Capacity Networks.pdf,./data/imgs/ICML2016/Dynamic Capacity Networks.png,5a5d48986b855b83a7d9df5005bbd155024ce756,74.0,"{2024: 5, 2023: 4, 2022: 6, 2021: 10, 2020: 13, 2019: 12, 2018: 12, 2017: 11, 2016: 1}",83.0,"{2024: 4, 2023: 10, 2022: 13, 2021: 6, 2020: 13, 2019: 11, 2018: 10, 2017: 13, 2016: 3}"
19163,Learning privately from multiparty data,ICML,2016,"['Jihun Hamm', 'Paul Cao', 'Mikhail Belkin']","Learning a classifier from private data collected by multiple parties is an important problem that has many potential applications. How can we build an accurate and differentially private global classifier by combining locally-trained classifiers from different parties, without access to any party's private data? We propose to transfer the `knowledge' of the local classifier ensemble by first creating labeled data from auxiliary unlabeled data, and then train a global $\epsilon$-differentially private classifier. We show that majority voting is too sensitive and therefore propose a new risk weighted by class probabilities estimated from the ensemble. Relative to a non-private solution, our private solution has a generalization error bounded by $O(\epsilon^{-2}M^{-2})$ where $M$ is the number of parties. This allows strong privacy without performance loss when $M$ is large, such as in crowdsensing applications. We demonstrate the performance of our method with realistic tasks of activity recognition, network intrusion detection, and malicious URL detection.",./data/pdfs/ICML2016/Learning privately from multiparty data.pdf,./data/imgs/ICML2016/Learning privately from multiparty data.png,2fe465b7beffa6687452b3ef80f555d8bd8ec0c0,58.0,"{2021: 5, 2020: 13, 2019: 12, 2018: 11, 2017: 13, 2016: 3, 2015: 1}",158.0,"{2024: 5, 2023: 11, 2022: 8, 2021: 15, 2020: 29, 2019: 37, 2018: 30, 2017: 16, 2016: 6, 2015: 1}"
19069,One-Shot Generalization in Deep Generative Models,ICML,2016,"['Danilo Jimenez Rezende', 'Shakir Mohamed', 'Ivo Danihelka', 'Karol Gregor', 'Daan Wierstra']","Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples---having seen new examples just once---providing an important class of general-purpose models for one-shot machine learning.",./data/pdfs/ICML2016/One-Shot Generalization in Deep Generative Models.pdf,./data/imgs/ICML2016/One-Shot Generalization in Deep Generative Models.png,0811597b0851b7ebe21aadce7cb4daac4664b44f,169.0,"{2024: 1, 2023: 9, 2022: 10, 2021: 21, 2020: 17, 2019: 32, 2018: 36, 2017: 24, 2016: 19}",245.0,"{2024: 4, 2023: 19, 2022: 22, 2021: 23, 2020: 31, 2019: 37, 2018: 49, 2017: 38, 2016: 22}"
19086,Texture Networks: Feed-forward Synthesis of Textures and Stylized Images,ICML,2016,"['Dmitry Ulyanov', 'Vadim Lebedev', 'Andrea Vedaldi', 'Victor Lempitsky']","Gatys et al. recently demonstrated that deep networks can generate beautiful textures and stylized images from a single texture example. However, their methods require a slow and memory-consuming optimization process. We propose here an alternative approach that moves the computational burden to a learning stage. Given a single example of a texture, our approach trains compact feed-forward convolutional networks to generate multiple samples of the same texture of arbitrary size and to transfer artistic style from a given image to any other image. The resulting networks are remarkably light-weight and can generate textures of quality comparable to Gatys et al., but hundreds of times faster. More generally, our approach highlights the power and flexibility of generative feed-forward models trained with complex and expressive loss functions.",./data/pdfs/ICML2016/Texture Networks: Feed-forward Synthesis of Textures and Stylized Images.pdf,./data/imgs/ICML2016/Texture Networks: Feed-forward Synthesis of Textures and Stylized Images.png,ed16b5a85e06fc0e6c81b3843a5bb2bb50a35ac1,558.0,"{2023: 1, 2022: 5, 2021: 97, 2020: 95, 2019: 146, 2018: 108, 2017: 88, 2016: 18}",881.0,"{2024: 21, 2023: 82, 2022: 106, 2021: 124, 2020: 114, 2019: 159, 2018: 146, 2017: 96, 2016: 33}"
26817,Sparse + Group-Sparse Dirty Models: Statistical Guarantees without Unreasonable Conditions and a Case for Non-Convexity,ICML,2017,"['Eunho Yang', 'Aurélie Lozano']",,./data/pdfs/ICML2017/Sparse + Group-Sparse Dirty Models: Statistical Guarantees without Unreasonable Conditions and a Case for Non-Convexity.pdf,./data/imgs/ICML2017/Sparse + Group-Sparse Dirty Models: Statistical Guarantees without Unreasonable Conditions and a Case for Non-Convexity.png,d7be502dd2754557fc71c91a4ba03a2c2ac1df14,2.0,"{2020: 1, 2019: 1}",4.0,"{2021: 1, 2020: 1, 2019: 1, 2018: 1}"
26864,Learning Stable Stochastic Nonlinear Dynamical Systems,ICML,2017,"['Jonas Umlauft', 'Sandra Hirche']","A data-driven identification of dynamical systems
requiring only minimal prior knowledge
is promising whenever no analytically derived
model structure is available, e.g., from first principles
in physics. However, meta-knowledge on
the system’s behavior is often given and should
be exploited: Stability as fundamental property
is essential when the model is used for controller
design or movement generation. Therefore, this
paper proposes a framework for learning stable
stochastic systems from data. We focus on
identifying a state-dependent coefficient form of
the nonlinear stochastic model which is globally
asymptotically stable according to probabilistic
Lyapunov methods. We compare our approach
to other state of the art methods on real-world
datasets in terms of flexibility and stability.",./data/pdfs/ICML2017/Learning Stable Stochastic Nonlinear Dynamical Systems.pdf,./data/imgs/ICML2017/Learning Stable Stochastic Nonlinear Dynamical Systems.png,8d3619e5ade1c0d4bca8ea97b27c45370fa6dddc,17.0,"{2023: 1, 2022: 3, 2021: 5, 2020: 4, 2019: 4}",19.0,"{2024: 1, 2023: 2, 2022: 4, 2021: 4, 2020: 5, 2019: 3}"
26870,Diameter-Based Active Learning,ICML,2017,"['Christopher Tosh', 'Sanjoy Dasgupta']","To date, the tightest upper and lower-bounds for the active learning of general concept classes have been in terms of a parameter of the learning problem called the splitting index. We provide, for the first time, an efficient algorithm that is able to realize this upper bound, and we empirically demonstrate its good performance.",./data/pdfs/ICML2017/Diameter-Based Active Learning.pdf,./data/imgs/ICML2017/Diameter-Based Active Learning.png,06dc27ad53914b60f46202800141f5c88a8a52cb,9.0,"{2021: 1, 2020: 2, 2019: 2, 2018: 4}",24.0,"{2024: 1, 2023: 1, 2022: 4, 2021: 2, 2020: 4, 2019: 5, 2018: 6, 2017: 1}"
26936,"Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery",ICML,2017,"['Ashkan Panahi', 'Devdatt Dubhashi', 'Fredrik Johansson', 'Chiranjib Bhattacharyya']","Standard clustering methods such as K-means, Gaussian mixture models, and hierarchical clustering, arc beset by local minima, which are sometimes drastically suboptimal. Moreover the number of clusters K must be known in advance. The recently introduced sum-of-norms (SON) or Clusterpath convex relaxation of k-means and hierarchical clustering shrinks cluster centroids toward one another and ensure a unique global minimizer. We give a scalable stochastic incremental algorithm based on proximal iterations to solve the SON problem with convergence guarantees. We also show that the algorithm recovers clusters under quite general conditions which have a similar form to the unifying proximity condition introduced in the approximation algorithms community (that covers paradigm cases such as Gaussian mixtures and planted partition models). We give experimental results to confirm that our algorithm scales much better than previous methods while producing clusters of comparable quality.","./data/pdfs/ICML2017/Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery.pdf","./data/imgs/ICML2017/Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery.png",c72b2e3ef1e5c7c8ea4ff45c61d265b62f03c3de,30.0,"{2024: 1, 2023: 1, 2022: 4, 2021: 7, 2020: 5, 2019: 7, 2018: 4, 2017: 1}",40.0,"{2023: 5, 2022: 7, 2021: 9, 2020: 7, 2019: 7, 2018: 4, 2017: 1}"
26829,Stochastic Convex Optimization: Faster Local Growth Implies Faster Global Convergence,ICML,2017,"['Yi Xu', 'Qihang Lin', 'Tianbao Yang']",,./data/pdfs/ICML2017/Stochastic Convex Optimization: Faster Local Growth Implies Faster Global Convergence.pdf,./data/imgs/ICML2017/Stochastic Convex Optimization: Faster Local Growth Implies Faster Global Convergence.png,dd969492ce70ae9b2d5f3587a91207e6b535b22f,33.0,"{2023: 1, 2021: 6, 2020: 5, 2019: 6, 2018: 11, 2017: 3, 2016: 1}",47.0,"{2024: 2, 2023: 1, 2022: 1, 2021: 7, 2020: 7, 2019: 12, 2018: 13, 2017: 2, 2016: 1, 2015: 1}"
27215,Natasha: Faster Non-Convex Stochastic Optimization via Strongly Non-Convex Parameter,ICML,2017,['Zeyuan Allen-Zhu'],"Given a nonconvex function that is an average of $n$ smooth functions, we design stochastic first-order methods to find its approximate stationary points. The convergence of our new methods depends on the smallest (negative) eigenvalue $-\sigma$ of the Hessian, a parameter that describes how nonconvex the function is. 
Our methods outperform known results for a range of parameter $\sigma$, and can be used to find approximate local minima. Our result implies an interesting dichotomy: there exists a threshold $\sigma_0$ so that the currently fastest methods for $\sigma>\sigma_0$ and for $\sigma<\sigma_0$ have different behaviors: the former scales with $n^{2/3}$ and the latter scales with $n^{3/4}$.",./data/pdfs/ICML2017/Natasha: Faster Non-Convex Stochastic Optimization via Strongly Non-Convex Parameter.pdf,./data/imgs/ICML2017/Natasha: Faster Non-Convex Stochastic Optimization via Strongly Non-Convex Parameter.png,298138e121aa60c15615b565ebf8a785591da3e0,28.0,"{2022: 2, 2021: 2, 2020: 4, 2019: 3, 2018: 12, 2017: 5}",78.0,"{2023: 5, 2022: 4, 2021: 3, 2020: 17, 2019: 18, 2018: 25, 2017: 6}"
27016,Deep Spectral Clustering Learning,ICML,2017,"['Marc T. Law', 'Raquel Urtasun', 'Richard S. Zemel']",,./data/pdfs/ICML2017/Deep Spectral Clustering Learning.pdf,./data/imgs/ICML2017/Deep Spectral Clustering Learning.png,d592ad48973b2bb0e268d3b404fb7262d56587b1,112.0,"{2024: 1, 2023: 10, 2022: 6, 2021: 19, 2020: 27, 2019: 34, 2018: 13, 2017: 2}",129.0,"{2024: 2, 2023: 16, 2022: 17, 2021: 14, 2020: 25, 2019: 35, 2018: 15, 2017: 5}"
26921,Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study,ICML,2017,"['Samuel Ritter', 'David G. T. Barrett', 'Adam Santoro', 'Matt Botvinick']","Deep neural networks (DNNs) have advanced performance on a wide range of complex tasks, rapidly outpacing our understanding of the nature of their solutions. While past work sought to advance our understanding of these models, none has made use of the rich history of problem descriptions, theories, and experimental methods developed by cognitive psychologists to study the human mind. To explore the potential value of these tools, we chose a well-established analysis from developmental psychology that explains how children learn word labels for objects, and applied that analysis to DNNs. Using datasets of stimuli inspired by the original cognitive psychology experiments, we find that state-of-the-art one shot learning models trained on ImageNet exhibit a similar bias to that observed in humans: they prefer to categorize objects according to shape rather than color. The magnitude of this shape bias varies greatly among architecturally identical, but differently seeded models, and even fluctuates within seeds throughout training, despite nearly equivalent classification performance. These results demonstrate the capability of tools from cognitive psychology for exposing hidden computational properties of DNNs, while concurrently providing us with a computational model for human word learning.",./data/pdfs/ICML2017/Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study.pdf,./data/imgs/ICML2017/Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study.png,39fb9fa2615620f043084a2ecbbdb1a1f8c707c9,80.0,"{2022: 1, 2021: 16, 2020: 24, 2019: 17, 2018: 16, 2017: 6}",184.0,"{2024: 7, 2023: 24, 2022: 26, 2021: 15, 2020: 45, 2019: 22, 2018: 32, 2017: 13}"
27121,RobustFill: Neural Program Learning under Noisy I O,ICML,2017,"['Jacob Devlin', 'Jonathan Uesato', 'Surya Bhupatiraju', 'Rishabh Singh', 'Abdelrahman Mohamed', 'Pushmeet Kohli']","The problem of automatically generating a computer program from some specification has been studied since the early days of AI. Recently, two competing approaches for automatic program learning have received significant attention: (1) neural program synthesis, where a neural network is conditioned on input/output (I/O) examples and learns to generate a program, and (2) neural program induction, where a neural network generates new outputs directly using a latent program representation. Here, for the first time, we directly compare both approaches on a large-scale, real-world learning task and we additionally contrast to rule-based program synthesis, which uses hand-crafted semantics to guide the program generation. Our neural models use a modified attention RNN to allow encoding of variable-sized sets of I/O pairs, which achieve 92% accuracy on a real-world test set, compared to the 34% accuracy of the previous best neural synthesis approach. The synthesis model also outperforms a comparable induction model on this task, but we more importantly demonstrate that the strength of each approach is highly dependent on the evaluation metric and end-user application. Finally, we show that we can train our neural models to remain very robust to the type of noise expected in real-world data (e.g., typos), while a highly-engineered rule-based system fails entirely.",./data/pdfs/ICML2017/RobustFill: Neural Program Learning under Noisy I O.pdf,./data/imgs/ICML2017/RobustFill: Neural Program Learning under Noisy I O.png,3ff0af64279929a952ee340e645256b7e0580f65,112.0,"{2022: 2, 2021: 31, 2020: 37, 2019: 28, 2018: 10, 2017: 4}",354.0,"{2024: 11, 2023: 48, 2022: 43, 2021: 53, 2020: 74, 2019: 52, 2018: 48, 2017: 25}"
26934,On the Expressive Power of Deep Neural Networks,ICML,2017,"['Maithra Raghu', 'Ben Poole', 'Jon Kleinberg', 'Surya Ganguli', 'Jascha Sohl‐Dickstein']","We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings can be summarized as follows: (1) The complexity of the computed function grows exponentially with depth. (2) All weights are not equal: trained networks are more sensitive to their lower (initial) layer weights. (3) Regularizing on trajectory length (trajectory regularization) is a simpler alternative to batch normalization, with the same performance.",./data/pdfs/ICML2017/On the Expressive Power of Deep Neural Networks.pdf,./data/imgs/ICML2017/On the Expressive Power of Deep Neural Networks.png,03e04983f7ce6a9c2b42948840b3312aea33f9f3,437.0,"{2025: 2, 2024: 10, 2023: 23, 2022: 39, 2021: 116, 2020: 86, 2019: 93, 2018: 53, 2017: 14, 2016: 1}",690.0,"{2024: 30, 2023: 96, 2022: 99, 2021: 103, 2020: 97, 2019: 119, 2018: 94, 2017: 41, 2016: 11}"
25834,Covariate Adjusted Precision Matrix Estimation via Nonconvex Optimization,ICML,2018,"['Jinghui Chen', 'Pan Xu', 'Lingxiao Wang', 'Jian Ma', 'Quanquan Gu']",,./data/pdfs/ICML2018/Covariate Adjusted Precision Matrix Estimation via Nonconvex Optimization.pdf,./data/imgs/ICML2018/Covariate Adjusted Precision Matrix Estimation via Nonconvex Optimization.png,26eae7a58a57a780228417d0d2e8769fa306a298,10.0,"{2022: 1, 2021: 2, 2020: 4, 2019: 2, 2018: 1}",7.0,"{2023: 1, 2021: 3, 2020: 2, 2019: 1}"
25362,Dependent Relational Gamma Process Models for Longitudinal Networks,ICML,2018,"['Sikun Yang', 'Heinz Koeppl']","A probabilistic framework based on the covariate-dependent relational gamma process is developed to analyze relational data arising from longitudinal networks. The proposed framework characterizes networked nodes by nonnegative node-group memberships, which allow each node to belong to multiple latent groups simultaneously, and encodes edge probabilities between each pair of nodes using a Bernoulli Poisson link to the embedded latent space. Within the latent space, our framework models the birth and death dynamics of individual groups via a thinning function. Our framework also captures the evolution of individual node-group memberships over time using gamma Markov processes. Exploiting the recent advances in data augmentation and marginalization techniques, a simple and efficient Gibbs sampler is proposed for posterior computation. Experimental results on a simulation study and three real-world temporal network data sets demonstrate the model’s capability, competitive performance and scalability compared to state-of-the-art methods.",./data/pdfs/ICML2018/Dependent Relational Gamma Process Models for Longitudinal Networks.pdf,./data/imgs/ICML2018/Dependent Relational Gamma Process Models for Longitudinal Networks.png,80f7c85163f92c831b54399db492f204550d93dd,16.0,"{2024: 1, 2023: 1, 2022: 1, 2021: 1, 2020: 7, 2019: 4, 2018: 1}",14.0,"{2024: 3, 2023: 2, 2021: 2, 2020: 3, 2019: 3, 2018: 1}"
25676,Let’s be Honest: An Optimal No-Regret Framework for Zero-Sum Games,ICML,2018,[],,./data/pdfs/ICML2018/Let’s be Honest: An Optimal No-Regret Framework for Zero-Sum Games.pdf,./data/imgs/ICML2018/Let’s be Honest: An Optimal No-Regret Framework for Zero-Sum Games.png,6f3182d2a9d4edd16fdba6994e1f92764d2b5ac5,,{},20.0,"{2024: 2, 2023: 4, 2022: 3, 2021: 7, 2020: 4}"
25423,Invariance of Weight Distributions in Rectified MLPs,ICML,2018,"['Russell Tsuchida', 'Farbod Roosta-Khorasani', 'Marcus Gallagher']","An interesting approach to analyzing neural networks that has received renewed attention is to examine the equivalent kernel of the neural network. This is based on the fact that a fully connected feedforward network with one hidden layer, a certain weight distribution, an activation function, and an infinite number of neurons can be viewed as a mapping into a Hilbert space. We derive the equivalent kernels of MLPs with ReLU or Leaky ReLU activations for all rotationally-invariant weight distributions, generalizing a previous result that required Gaussian weight distributions. Additionally, the Central Limit Theorem is used to show that for certain activation functions, kernels corresponding to layers with weight distributions having $0$ mean and finite absolute third moment are asymptotically universal, and are well approximated by the kernel corresponding to layers with spherical Gaussian weights. In deep networks, as depth increases the equivalent kernel approaches a pathological fixed point, which can be used to argue why training randomly initialized networks can be difficult. Our results also have implications for weight initialization.",./data/pdfs/ICML2018/Invariance of Weight Distributions in Rectified MLPs.pdf,./data/imgs/ICML2018/Invariance of Weight Distributions in Rectified MLPs.png,8bff7353fa4f75629ea418ca8db60477a751db93,15.0,"{2022: 1, 2021: 6, 2020: 2, 2019: 5, 2018: 1}",33.0,"{2024: 2, 2023: 4, 2022: 3, 2021: 4, 2020: 6, 2019: 10, 2018: 4}"
25458,Locally Private Hypothesis Testing,ICML,2018,['Or Sheffet'],"We initiate the study of differentially private hypothesis testing in the local-model, under both the standard (symmetric) randomized-response mechanism (Warner, 1965, Kasiviswanathan et al, 2008) and the newer (non-symmetric) mechanisms (Bassily and Smith, 2015, Bassily et al, 2017). First, we study the general framework of mapping each user's type into a signal and show that the problem of finding the maximum-likelihood distribution over the signals is feasible. Then we discuss the randomized-response mechanism and show that, in essence, it maps the null- and alternative-hypotheses onto new sets, an affine translation of the original sets. We then give sample complexity bounds for identity and independence testing under randomized-response. We then move to the newer non-symmetric mechanisms and show that there too the problem of finding the maximum-likelihood distribution is feasible. Under the mechanism of Bassily et al (2007) we give identity and independence testers with better sample complexity than the testers in the symmetric case, and we also propose a $\chi^2$-based identity tester which we investigate empirically.",./data/pdfs/ICML2018/Locally Private Hypothesis Testing.pdf,./data/imgs/ICML2018/Locally Private Hypothesis Testing.png,897cfd9b46c65d95147f94d26d3626fe098d9b3c,23.0,"{2023: 1, 2022: 1, 2021: 5, 2020: 5, 2019: 3, 2018: 8}",49.0,"{2024: 1, 2023: 3, 2022: 9, 2021: 6, 2020: 9, 2019: 5, 2018: 14, 2017: 2}"
25674,Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints,ICML,2018,"['Ehsan Kazemi', 'Morteza Zadimoghaddam', 'Amin Karbasi']",,./data/pdfs/ICML2018/Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints.pdf,./data/imgs/ICML2018/Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints.png,62599f741551c1f8a12336714f6cd7d4a7004e54,42.0,"{2025: 1, 2024: 1, 2023: 3, 2022: 3, 2021: 14, 2020: 9, 2019: 8, 2018: 2, 2017: 1}",57.0,"{2024: 1, 2023: 11, 2022: 8, 2021: 14, 2020: 12, 2019: 4, 2018: 6, 2017: 1}"
25541,SparseMAP: Differentiable Sparse Structured Inference,ICML,2018,"['Vlad Niculae', 'André F. T. Martins', 'Mathieu Blondel', 'Claire Cardie']","Structured prediction requires searching over a combinatorial number of structures. To tackle it, we introduce SparseMAP: a new method for sparse structured inference, and its natural loss function. SparseMAP automatically selects only a few global structures: it is situated between MAP inference, which picks a single structure, and marginal inference, which assigns probability mass to all structures, including implausible ones. Importantly, SparseMAP can be computed using only calls to a MAP oracle, making it applicable to problems with intractable marginal inference, e.g., linear assignment. Sparsity makes gradient backpropagation efficient regardless of the structure, enabling us to augment deep neural networks with generic and sparse structured hidden layers. Experiments in dependency parsing and natural language inference reveal competitive accuracy, improved interpretability, and the ability to capture natural language ambiguities, which is attractive for pipeline systems.",./data/pdfs/ICML2018/SparseMAP: Differentiable Sparse Structured Inference.pdf,./data/imgs/ICML2018/SparseMAP: Differentiable Sparse Structured Inference.png,bd575b481405fa6d6ada0277c30cb8f58a4e6d44,56.0,"{2022: 1, 2021: 17, 2020: 19, 2019: 14, 2018: 5}",113.0,"{2024: 4, 2023: 15, 2022: 14, 2021: 23, 2020: 23, 2019: 19, 2018: 15}"
25737,Deep Models of Interactions Across Sets,ICML,2018,"['Jason Hartford', 'Devon R. Graham', 'Kevin Leyton‐Brown', 'Siamak Ravanbakhsh']","We use deep learning to model interactions across two or more sets of objects, such as user-movie ratings, protein-drug bindings, or ternary user-item-tag interactions. The canonical representation of such interactions is a matrix (or a higher-dimensional tensor) with an exchangeability property: the encoding's meaning is not changed by permuting rows or columns. We argue that models should hence be Permutation Equivariant (PE): constrained to make the same predictions across such permutations. We present a parameter-sharing scheme and prove that it could not be made any more expressive without violating PE. This scheme yields three benefits. First, we demonstrate state-of-the-art performance on multiple matrix completion benchmarks. Second, our models require a number of parameters independent of the numbers of objects, and thus scale well to large datasets. Third, models can be queried about new objects that were not available at training time, but for which interactions have since been observed. In experiments, our models achieved surprisingly good generalization performance on this matrix extrapolation task, both within domains (e.g., new users and new movies drawn from the same distribution used for training) and even across domains (e.g., predicting music ratings after training on movies).",./data/pdfs/ICML2018/Deep Models of Interactions Across Sets.pdf,./data/imgs/ICML2018/Deep Models of Interactions Across Sets.png,a1172d0de6164bb7eaadbcdbc10e7b03b773b6ad,55.0,"{2024: 2, 2023: 7, 2022: 6, 2021: 8, 2020: 13, 2019: 13, 2018: 6}",137.0,"{2024: 7, 2023: 24, 2022: 30, 2021: 23, 2020: 22, 2019: 23, 2018: 8}"
25788,Gradient Descent Learns One-hidden-layer CNN: Don’t be Afraid of Spurious Local Minima,ICML,2018,[],,./data/pdfs/ICML2018/Gradient Descent Learns One-hidden-layer CNN: Don’t be Afraid of Spurious Local Minima.pdf,./data/imgs/ICML2018/Gradient Descent Learns One-hidden-layer CNN: Don’t be Afraid of Spurious Local Minima.png,f91248a4f587f89f1d1d8e557cee08b8114686d9,,{},224.0,"{2024: 2, 2023: 15, 2022: 14, 2021: 19, 2020: 43, 2019: 72, 2018: 57, 2017: 2}"
25418,Adversarial Risk and the Dangers of Evaluating Against Weak Attacks,ICML,2018,"['Jonathan Uesato', 'Brendan O’Donoghue', 'Aäron van den Oord', 'Pushmeet Kohli']","This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate 'adversarial risk' as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as defining a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as 'obscurity to an adversary,' and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a significant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.",./data/pdfs/ICML2018/Adversarial Risk and the Dangers of Evaluating Against Weak Attacks.pdf,./data/imgs/ICML2018/Adversarial Risk and the Dangers of Evaluating Against Weak Attacks.png,f4b434c3ab979ecdd71bbed894b34de77590c6dd,315.0,"{2024: 21, 2023: 15, 2022: 24, 2021: 72, 2020: 97, 2019: 63, 2018: 22, 2017: 1}",533.0,"{2024: 24, 2023: 75, 2022: 74, 2021: 93, 2020: 121, 2019: 104, 2018: 40, 2017: 1, 2016: 1}"
19983,TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning,ICML,2019,"['Tameem Adel', 'Adrian Weller']","One of the challenges to reinforcement learning (RL) is scalable transferability among complex tasks. Incorporating a graphical model (GM), along with the rich family of related methods, as a basis for RL frameworks provides potential to address issues such as transferability, generalisation and exploration. Here we propose a flexible GM-based RL framework which leverages efficient inference procedures to enhance generalisation and transfer power. In our proposed transferable and information-based graphical model framework ‘TibGM’, we show the equivalence between our mutual information-based objective in the GM, and an RL consolidated objective consisting of a standard reward maximisation target and a generalisation/transfer objective. In settings where there is a sparse or deceptive reward signal, our TibGM framework is flexible enough to incorporate exploration bonuses depicting intrinsic rewards. We empirically verify improved performance and exploration power.",./data/pdfs/ICML2019/TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning.pdf,./data/imgs/ICML2019/TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning.png,6b36776e5c0473d82cbdd2c92cd97cca7925ae08,1.0,{2021: 1},1.0,{2021: 1}
19845,Adjustment Criteria for Generalizing Experimental Findings,ICML,2019,"['Juan D. Correa', 'Jin Tian', 'Elias Bareinboim']",,./data/pdfs/ICML2019/Adjustment Criteria for Generalizing Experimental Findings.pdf,./data/imgs/ICML2019/Adjustment Criteria for Generalizing Experimental Findings.png,401fb17ee43121a9ceff740ea6af80561c863d03,2.0,"{2023: 1, 2021: 1}",10.0,"{2024: 1, 2023: 2, 2022: 4, 2020: 2, 2019: 1}"
19987,AReS and MaRS Adversarial and MMD-Minimizing Regression for SDEs,ICML,2019,"['Gabriele Abbati', 'Philippe Wenk', 'Michael A. Osborne', 'Andreas Krause', 'Bernhard Schölkopf', 'Stefan Bauer']","Stochastic differential equations are an important modeling class in many disciplines. Consequently, there exist many methods relying on various discretization and numerical integration schemes. In this paper, we propose a novel, probabilistic model for estimating the drift and diffusion given noisy observations of the underlying stochastic system. Using state-of-the-art adversarial and moment matching inference techniques, we avoid the discretization schemes of classical approaches. This leads to significant improvements in parameter accuracy and robustness given random initial guesses. On four established benchmark systems, we compare the performance of our algorithms to state-of-the-art solutions based on extended Kalman filtering and Gaussian processes.",./data/pdfs/ICML2019/AReS and MaRS Adversarial and MMD-Minimizing Regression for SDEs.pdf,./data/imgs/ICML2019/AReS and MaRS Adversarial and MMD-Minimizing Regression for SDEs.png,24da5d49b1aa8d43b931d3f28e1039d4d47eba00,11.0,"{2022: 2, 2021: 2, 2020: 5, 2019: 2}",15.0,"{2023: 2, 2022: 1, 2021: 3, 2020: 5, 2019: 4}"
19406,Rehashing Kernel Evaluation in High Dimensions,ICML,2019,"['Paris Siminelakis', 'Kexin Rong', 'Peter Bailis', 'Moses Charikar', 'Philip Levis']",,./data/pdfs/ICML2019/Rehashing Kernel Evaluation in High Dimensions.pdf,./data/imgs/ICML2019/Rehashing Kernel Evaluation in High Dimensions.png,717604f4e1b533f68504ed1187eeff288807cbb8,12.0,"{2023: 1, 2022: 1, 2021: 5, 2020: 4, 2019: 1}",29.0,"{2024: 1, 2023: 9, 2022: 8, 2021: 6, 2020: 3, 2019: 2}"
19776,Optimal Mini-Batch and Step Sizes for SAGA,ICML,2019,"['Nidham Gazagnadou', 'Robert M. Gower', 'Joseph Salmon']","Recently it has been shown that the step sizes of a family of variance reduced gradient methods called the JacSketch methods depend on the expected smoothness constant. In particular, if this expected smoothness constant could be calculated a priori, then one could safely set much larger step sizes which would result in a much faster convergence rate. We fill in this gap, and provide simple closed form expressions for the expected smoothness constant and careful numerical experiments verifying these bounds. Using these bounds, and since the SAGA algorithm is part of this JacSketch family, we suggest a new standard practice for setting the step sizes and mini-batch size for SAGA that are competitive with a numerical grid search. Furthermore, we can now show that the total complexity of the SAGA algorithm decreases linearly in the mini-batch size up to a pre-defined value: the optimal mini-batch size. This is a rare result in the stochastic variance reduced literature, only previously shown for the Katyusha algorithm. Finally we conjecture that this is the case for many other stochastic variance reduced methods and that our bounds and analysis of the expected smoothness constant is key to extending these results.",./data/pdfs/ICML2019/Optimal Mini-Batch and Step Sizes for SAGA.pdf,./data/imgs/ICML2019/Optimal Mini-Batch and Step Sizes for SAGA.png,40078cf8fd68cbf7237467e44fa52d49a5eb8fd1,20.0,"{2023: 1, 2021: 6, 2020: 7, 2019: 6}",33.0,"{2023: 3, 2022: 3, 2021: 7, 2020: 11, 2019: 9}"
19986,Communication Complexity in Locally Private Distribution Estimation and Heavy Hitters,ICML,2019,"['Jayadev Acharya', 'Ziteng Sun']","We consider the problems of distribution estimation and heavy hitter (frequency) estimation under privacy and communication constraints. While these constraints have been studied separately, optimal schemes for one are sub-optimal for the other. We propose a sample-optimal $\varepsilon$-locally differentially private (LDP) scheme for distribution estimation, where each user communicates only one bit, and requires no public randomness. We show that Hadamard Response, a recently proposed scheme for $\varepsilon$-LDP distribution estimation is also utility-optimal for heavy hitter estimation. Finally, we show that unlike distribution estimation, without public randomness where only one bit suffices, any heavy hitter estimation algorithm that communicates $o(\min \{\log n, \log k\})$ bits from each user cannot be optimal.",./data/pdfs/ICML2019/Communication Complexity in Locally Private Distribution Estimation and Heavy Hitters.pdf,./data/imgs/ICML2019/Communication Complexity in Locally Private Distribution Estimation and Heavy Hitters.png,3d553785e8ec0bae51dadf6a4d50cd4de04281ae,26.0,"{2022: 1, 2021: 11, 2020: 10, 2019: 3, 2018: 1}",49.0,"{2024: 4, 2023: 11, 2022: 5, 2021: 10, 2020: 14, 2019: 4, 2018: 1}"
19644,Bit-Swap: Recursive Bits-Back Coding for Lossless Compression with Hierarchical Latent Variables,ICML,2019,"['Friso H. Kingma', 'Pieter Abbeel', 'Jonathan Ho']","The bits-back argument suggests that latent variable models can be turned into lossless compression schemes. Translating the bits-back argument into efficient and practical lossless compression schemes for general latent variable models, however, is still an open problem. Bits-Back with Asymmetric Numeral Systems (BB-ANS), recently proposed by Townsend et al. (2019), makes bits-back coding practically feasible for latent variable models with one latent layer, but it is inefficient for hierarchical latent variable models. In this paper we propose Bit-Swap, a new compression scheme that generalizes BB-ANS and achieves strictly better compression rates for hierarchical latent variable models with Markov chain structure. Through experiments we verify that Bit-Swap results in lossless compression rates that are empirically superior to existing techniques. Our implementation is available at https://github.com/fhkingma/bitswap.",./data/pdfs/ICML2019/Bit-Swap: Recursive Bits-Back Coding for Lossless Compression with Hierarchical Latent Variables.pdf,./data/imgs/ICML2019/Bit-Swap: Recursive Bits-Back Coding for Lossless Compression with Hierarchical Latent Variables.png,92f7c3cf0824732cdc1ec5a46af9a48cba3fa698,40.0,"{2024: 5, 2023: 7, 2022: 8, 2021: 12, 2020: 7}",84.0,"{2024: 6, 2023: 17, 2022: 23, 2021: 22, 2020: 11, 2019: 5}"
19823,Finite-Time Analysis of Distributed TD(0) with Linear Function Approximation on Multi-Agent Reinforcement Learning,ICML,2019,"['Thinh T. Doan', 'Siva Theja Maguluri', 'Justin Romberg']",,./data/pdfs/ICML2019/Finite-Time Analysis of Distributed TD(0) with Linear Function Approximation on Multi-Agent Reinforcement Learning.pdf,./data/imgs/ICML2019/Finite-Time Analysis of Distributed TD(0) with Linear Function Approximation on Multi-Agent Reinforcement Learning.png,60f72077a904b70a8aa093c9d04ea4e7768bb072,42.0,"{2022: 1, 2021: 20, 2020: 14, 2019: 6, 2018: 1}",113.0,"{2024: 14, 2023: 17, 2022: 19, 2021: 27, 2020: 23, 2019: 10, 2018: 3}"
19783,MetricGAN: Generative Adversarial Networks based Black-box Metric Scores Optimization for Speech Enhancement,ICML,2019,"['Szu‐Wei Fu', 'Chien-Feng Liao', 'Yu Tsao', 'Shou-De Lin']","Adversarial loss in a conditional generative adversarial network (GAN) is not designed to directly optimize evaluation metrics of a target task, and thus, may not always guide the generator in a GAN to generate data with improved metric scores. To overcome this issue, we propose a novel MetricGAN approach with an aim to optimize the generator with respect to one or multiple evaluation metrics. Moreover, based on MetricGAN, the metric scores of the generated data can also be arbitrarily specified by users. We tested the proposed MetricGAN on a speech enhancement task, which is particularly suitable to verify the proposed approach because there are multiple metrics measuring different aspects of speech signals. Moreover, these metrics are generally complex and could not be fully optimized by Lp or conventional adversarial losses.",./data/pdfs/ICML2019/MetricGAN: Generative Adversarial Networks based Black-box Metric Scores Optimization for Speech Enhancement.pdf,./data/imgs/ICML2019/MetricGAN: Generative Adversarial Networks based Black-box Metric Scores Optimization for Speech Enhancement.png,f9ad5ca6e44683fc73339eed3aafe8c0e8725c09,132.0,"{2025: 1, 2024: 4, 2023: 33, 2022: 46, 2021: 24, 2020: 20, 2019: 3, 2018: 1}",260.0,"{2024: 20, 2023: 71, 2022: 78, 2021: 48, 2020: 34, 2019: 8, 2018: 1}"
19957,Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,ICML,2019,"['Sanjeev Arora', 'Simon S. Du', 'Wei Hu', 'Zhiyuan Li', 'Ruosong Wang']","Recent works have cast some light on the mystery of why deep nets fit any data and generalize despite being very overparametrized. This paper analyzes training and generalization for a simple 2-layer ReLU net with random initialization, and provides the following improvements over recent works: (i) Using a tighter characterization of training speed than recent papers, an explanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR'17]. (ii) Generalization bound independent of network size, using a data-dependent complexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by experiments. Moreover, recent papers require sample complexity to increase (slowly) with the size, while our sample complexity is completely independent of the network size. (iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent. The key idea is to track dynamics of training and generalization via properties of a related kernel.",./data/pdfs/ICML2019/Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks.pdf,./data/imgs/ICML2019/Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks.png,14558cb69319eed0d5bfc5648aafcd09d882f443,256.0,"{2025: 1, 2024: 7, 2023: 17, 2022: 28, 2021: 109, 2020: 72, 2019: 22}",858.0,"{2024: 49, 2023: 136, 2022: 154, 2021: 194, 2020: 187, 2019: 129, 2018: 8, 2017: 1}"
24842,Double Reinforcement Learning for Efficient and Robust Off-Policy Evaluation,ICML,2020,"['Nathan Kallus', 'Masatoshi Uehara']",,./data/pdfs/ICML2020/Double Reinforcement Learning for Efficient and Robust Off-Policy Evaluation.pdf,./data/imgs/ICML2020/Double Reinforcement Learning for Efficient and Robust Off-Policy Evaluation.png,77e6aad66e50106654b52a57c6f7a19c4295f845,2.0,"{2023: 1, 2020: 1}",6.0,"{2024: 2, 2022: 2, 2020: 1, 2019: 1}"
24492,On Conditional Versus Marginal Bias in Multi-Armed Bandits,ICML,2020,"['Jaehyeok Shin', 'Aaditya Ramdas', 'Alessandro Rinaldo']","The bias of the sample means of the arms in multi-armed bandits is an important issue in adaptive data analysis that has recently received considerable attention in the literature. Existing results relate in precise ways the sign and magnitude of the bias to various sources of data adaptivity, but do not apply to the conditional inference setting in which the sample means are computed only if some specific conditions are satisfied. In this paper, we characterize the sign of the conditional bias of monotone functions of the rewards, including the sample mean. Our results hold for arbitrary conditioning events and leverage natural monotonicity properties of the data collection policy. We further demonstrate, through several examples from sequential testing and best arm identification, that the sign of the conditional and marginal bias of the sample mean of an arm can be different, depending on the conditioning event. Our analysis offers new and interesting perspectives on the subtleties of assessing the bias in data adaptive settings.",./data/pdfs/ICML2020/On Conditional Versus Marginal Bias in Multi-Armed Bandits.pdf,./data/imgs/ICML2020/On Conditional Versus Marginal Bias in Multi-Armed Bandits.png,753b80599357a2d9223b0eb1cd653d731af60949,7.0,"{2023: 2, 2021: 4, 2019: 1}",10.0,"{2024: 1, 2023: 1, 2022: 2, 2021: 5, 2020: 1}"
24560,Decentralised Learning with Random Features and Distributed Gradient Descent,ICML,2020,"['Dominic Richards', 'Patrick Rebeschini', 'Lorenzo Rosasco']","We investigate the generalisation performance of Distributed Gradient Descent with Implicit Regularisation and Random Features in the homogenous setting where a network of agents are given data sampled independently from the same unknown distribution. Along with reducing the memory footprint, Random Features are particularly convenient in this setting as they provide a common parameterisation across agents that allows to overcome previous difficulties in implementing Decentralised Kernel Regression. Under standard source and capacity assumptions, we establish high probability bounds on the predictive performance for each agent as a function of the step size, number of iterations, inverse spectral gap of the communication matrix and number of Random Features. By tuning these parameters, we obtain statistical rates that are minimax optimal with respect to the total number of samples in the network. The algorithm provides a linear improvement over single machine Gradient Descent in memory cost and, when agents hold enough data with respect to the network size and inverse spectral gap, a linear speed-up in computational runtime for any network topology. We present simulations that show how the number of Random Features, iterations and samples impact predictive performance.",./data/pdfs/ICML2020/Decentralised Learning with Random Features and Distributed Gradient Descent.pdf,./data/imgs/ICML2020/Decentralised Learning with Random Features and Distributed Gradient Descent.png,dba6c643d6cd26b8e5d30c51c2f935df9ac90a7f,8.0,"{2024: 1, 2023: 3, 2022: 1, 2021: 2, 2020: 1}",15.0,"{2024: 2, 2023: 1, 2022: 6, 2021: 1, 2020: 2, 2019: 3}"
24880,Inverse Active Sensing: Modeling and Understanding Timely Decision-Making,ICML,2020,"['Daniel Jarrett', 'Mihaela van der Schaar']","Evidence-based decision-making entails collecting (costly) observations about an underlying phenomenon of interest, and subsequently committing to an (informed) decision on the basis of accumulated evidence. In this setting, active sensing is the goal-oriented problem of efficiently selecting which acquisitions to make, and when and what decision to settle on. As its complement, inverse active sensing seeks to uncover an agent's preferences and strategy given their observable decision-making behavior. In this paper, we develop an expressive, unified framework for the general setting of evidence-based decision-making under endogenous, context-dependent time pressure---which requires negotiating (subjective) tradeoffs between accuracy, speediness, and cost of information. Using this language, we demonstrate how it enables modeling intuitive notions of surprise, suspense, and optimality in decision strategies (the forward problem). Finally, we illustrate how this formulation enables understanding decision-making behavior by quantifying preferences implicit in observed decision strategies (the inverse problem).",./data/pdfs/ICML2020/Inverse Active Sensing: Modeling and Understanding Timely Decision-Making.pdf,./data/imgs/ICML2020/Inverse Active Sensing: Modeling and Understanding Timely Decision-Making.png,cbc5fa7a6446fd1f06077ed3c5943b2793b9cf92,5.0,"{2021: 3, 2020: 2}",17.0,"{2023: 8, 2022: 4, 2021: 2, 2020: 3}"
24477,Hypernetwork approach to generating point clouds,ICML,2020,"['Przemysław Spurek', 'Sebastian Winczowski', 'Jacek Tabor', 'Maciej Zamorski', 'Maciej Zięba', 'T. P. Trzcinski']","In this work, we propose a novel method for generating 3D point clouds that leverage properties of hyper networks. Contrary to the existing methods that learn only the representation of a 3D object, our approach simultaneously finds a representation of the object and its 3D surface. The main idea of our HyperCloud method is to build a hyper network that returns weights of a particular neural network (target network) trained to map points from a uniform unit ball distribution into a 3D shape. As a consequence, a particular 3D shape can be generated using point-by-point sampling from the assumed prior distribution and transforming sampled points with the target network. Since the hyper network is based on an auto-encoder architecture trained to reconstruct realistic 3D shapes, the target network weights can be considered a parametrization of the surface of a 3D shape, and not a standard representation of point cloud usually returned by competitive approaches. The proposed architecture allows finding mesh-based representation of 3D objects in a generative manner while providing point clouds en pair in quality with the state-of-the-art methods.",./data/pdfs/ICML2020/Hypernetwork approach to generating point clouds.pdf,./data/imgs/ICML2020/Hypernetwork approach to generating point clouds.png,0e951c7f88461934913c79824e9f9ca8b1df097c,11.0,"{2024: 1, 2023: 2, 2022: 5, 2021: 3}",31.0,"{2024: 1, 2023: 8, 2022: 5, 2021: 12, 2020: 5}"
25128,Unbiased Risk Estimators Can Mislead: A Case Study of Learning with Complementary Labels,ICML,2020,"['Yu-Ting Chou', 'Gang Niu', 'Hsuan-Tien Lin', 'Masashi Sugiyama']","In weakly supervised learning, unbiased risk estimator(URE) is a powerful tool for training classifiers when training and test data are drawn from different distributions. Nevertheless, UREs lead to overfitting in many problem settings when the models are complex like deep networks. In this paper, we investigate reasons for such overfitting by studying a weakly supervised problem called learning with complementary labels. We argue the quality of gradient estimation matters more in risk minimization. Theoretically, we show that a URE gives an unbiased gradient estimator(UGE). Practically, however, UGEs may suffer from huge variance, which causes empirical gradients to be usually far away from true gradients during minimization. To this end, we propose a novel surrogate complementary loss(SCL) framework that trades zero bias with reduced variance and makes empirical gradients more aligned with true gradients in the direction. Thanks to this characteristic, SCL successfully mitigates the overfitting issue and improves URE-based methods.",./data/pdfs/ICML2020/Unbiased Risk Estimators Can Mislead: A Case Study of Learning with Complementary Labels.pdf,./data/imgs/ICML2020/Unbiased Risk Estimators Can Mislead: A Case Study of Learning with Complementary Labels.png,5c6ecea6ef4de8752f88b681fa4b6dd19f27fe92,12.0,"{2021: 8, 2020: 4}",44.0,"{2024: 1, 2023: 12, 2022: 12, 2021: 11, 2020: 6, 2018: 1, 2017: 1}"
24917,Statistically Preconditioned Accelerated Gradient Method for Distributed Optimization,ICML,2020,"['Hadrien Hendrikx', 'Lin Xiao', 'Sébastien Bubeck', 'Francis Bach', 'Laurent Massoulié']","We consider the setting of distributed empirical risk minimization where multiple machines compute the gradients in parallel and a centralized server updates the model parameters. In order to reduce the number of communications required to reach a given accuracy, we propose a precon-ditioned accelerated gradient method where the preconditioning is done by solving a local optimization problem over a subsampled dataset at the server. The convergence rate of the method depends on the square root of the relative condition number between the global and local loss functions. We estimate the relative condition number for linear prediction models by studying uniform concentration of the Hessians over a bounded domain , which allows us to derive improved convergence rates for existing preconditioned gradient methods and our accelerated method. Experiments on real-world datasets illustrate the benefits of acceleration in the ill-conditioned regime.",./data/pdfs/ICML2020/Statistically Preconditioned Accelerated Gradient Method for Distributed Optimization.pdf,./data/imgs/ICML2020/Statistically Preconditioned Accelerated Gradient Method for Distributed Optimization.png,1420120d938b7641652303523b566099e7790d8e,12.0,"{2023: 1, 2022: 3, 2021: 8}",53.0,"{2024: 8, 2023: 9, 2022: 11, 2021: 19, 2020: 5, 2019: 1}"
24485,A Generative Model for Molecular Distance Geometry,ICML,2020,"['Gregor N. C. Simm', 'José Miguel Herńandez-Lobato']","Great computational effort is invested in generating equilibrium states for molecular systems using, for example, Markov chain Monte Carlo. We present a probabilistic model that generates statistically independent samples for molecules from their graph representations. Our model learns a low-dimensional manifold that preserves the geometry of local atomic neighborhoods through a principled learning representation that is based on Euclidean distance geometry. In a new benchmark for molecular conformation generation, we show experimentally that our generative model achieves state-of-the-art accuracy. Finally, we show how to use our model as a proposal distribution in an importance sampling scheme to compute molecular properties.",./data/pdfs/ICML2020/A Generative Model for Molecular Distance Geometry.pdf,./data/imgs/ICML2020/A Generative Model for Molecular Distance Geometry.png,c21d2d07cda6d8cb113de40fae726582d677ab3c,29.0,"{2024: 7, 2023: 8, 2022: 7, 2021: 5, 2020: 2}",88.0,"{2024: 8, 2023: 33, 2022: 22, 2021: 19, 2020: 6}"
25010,Fast and Three-rious: Speeding Up Weak Supervision with Triplet Methods,ICML,2020,"['Daniel Y. Fu', 'Mayee Chen', 'Frédéric Sala', 'Sarah Hooper', 'Kayvon Fatahalian', 'Christopher Ré']","Weak supervision is a popular method for building machine learning models without relying on ground truth annotations. Instead, it generates probabilistic training labels by estimating the accuracies of multiple noisy labeling sources (e.g., heuristics, crowd workers). Existing approaches use latent variable estimation to model the noisy sources, but these methods can be computationally expensive, scaling superlinearly in the data. In this work, we show that, for a class of latent variable models highly applicable to weak supervision, we can find a closed-form solution to model parameters, obviating the need for iterative solutions like stochastic gradient descent (SGD). We use this insight to build FlyingSquid, a weak supervision framework that runs orders of magnitude faster than previous weak supervision approaches and requires fewer assumptions. In particular, we prove bounds on generalization error without assuming that the latent variable model can exactly parameterize the underlying data distribution. Empirically, we validate FlyingSquid on benchmark weak supervision datasets and find that it achieves the same or higher quality compared to previous approaches without the need to tune an SGD procedure, recovers model parameters 170 times faster on average, and enables new video analysis and online learning applications.",./data/pdfs/ICML2020/Fast and Three-rious: Speeding Up Weak Supervision with Triplet Methods.pdf,./data/imgs/ICML2020/Fast and Three-rious: Speeding Up Weak Supervision with Triplet Methods.png,5c9c0ba883c9d05bec79afb0c2a57dda53c5d93e,34.0,"{2024: 1, 2023: 10, 2022: 6, 2021: 15, 2020: 2}",100.0,"{2024: 10, 2023: 25, 2022: 45, 2021: 16, 2020: 4}"
24899,XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation,ICML,2020,"['Junjie Hu', 'Sebastian Ruder', 'Aditya Siddhant', 'Graham Neubig', 'Orhan Fırat', 'Melvin Johnson']","Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.",./data/pdfs/ICML2020/XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation.pdf,./data/imgs/ICML2020/XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation.png,ba4a34680e09e77984624c95f5245d91b54373f6,305.0,"{2024: 6, 2023: 100, 2022: 92, 2021: 101, 2020: 6}",828.0,"{2024: 68, 2023: 193, 2022: 249, 2021: 218, 2020: 99, 2019: 1}"
23785,Objective Bound Conditional Gaussian Process for Bayesian Optimization,ICML,2021,"['Sinno Jialin Pan', 'Qiang Yang']","A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.",./data/pdfs/ICML2021/Objective Bound Conditional Gaussian Process for Bayesian Optimization.pdf,./data/imgs/ICML2021/Objective Bound Conditional Gaussian Process for Bayesian Optimization.png,ed51f312ffa6c5eb22aee7972843f05b395a78ea,20807.0,"{2025: 459, 2024: 2009, 2023: 2529, 2022: 2657, 2021: 3309, 2020: 2963, 2019: 2318, 2018: 1467, 2017: 832, 2016: 545, 2015: 445, 2014: 379, 2013: 306, 2012: 219}",1.0,{2023: 1}
23495,Causality-aware counterfactual confounding adjustment as an alternative to linear residualization in anticausal prediction tasks based on linear learners,ICML,2021,['Elias Chaibub Neto'],"Linear residualization is a common practice for confounding adjustment in machine learning (ML) applications. Recently, causality-aware predictive modeling has been proposed as an alternative causality-inspired approach for adjusting for confounders. The basic idea is to simulate counterfactual data that is free from the spurious associations generated by the observed confounders. In this paper, we compare the linear residualization approach against the causality-aware confounding adjustment in anticausal prediction tasks, and show that the causality-aware approach tends to (asymptotically) outperform the residualization adjustment in terms of predictive performance in linear learners. Importantly, our results still holds even when the true model is not linear. We illustrate our results in both regression and classification tasks, where we compared the causality-aware and residualization approaches using mean squared errors and classification accuracy in synthetic data experiments where the linear regression model is mispecified, as well as, when the linear model is correctly specified. Furthermore, we illustrate how the causality-aware approach is more stable than residualization with respect to dataset shifts in the joint distribution of the confounders and outcome variables.",./data/pdfs/ICML2021/Causality-aware counterfactual confounding adjustment as an alternative to linear residualization in anticausal prediction tasks based on linear learners.pdf,./data/imgs/ICML2021/Causality-aware counterfactual confounding adjustment as an alternative to linear residualization in anticausal prediction tasks based on linear learners.png,a000c7d589877ecb748327e91377c7ec4ae739ec,4.0,"{2022: 2, 2021: 1, 2020: 1}",6.0,"{2023: 1, 2022: 3, 2021: 1, 2020: 1}"
24023,Convex Regularization in Monte-Carlo Tree Search,ICML,2021,"['Tuan Dam', 'Carlo D’Eramo', 'Jan Peters', 'Joni Pajarinen']","Monte-Carlo planning and Reinforcement Learning (RL) are essential to sequential decision making. The recent AlphaGo and AlphaZero algorithms have shown how to successfully combine these two paradigms in order to solve large scale sequential decision problems. These methodologies exploit a variant of the well-known UCT algorithm to trade off exploitation of good actions and exploration of unvisited states, but their empirical success comes at the cost of poor sample-efficiency and high computation time. In this paper, we overcome these limitations by considering convex regularization in Monte-Carlo Tree Search (MCTS), which has been successfully used in RL to efficiently drive exploration. First, we introduce a unifying theory on the use of generic convex regularizers in MCTS, deriving the regret analysis and providing guarantees of exponential convergence rate. Second, we exploit our theoretical framework to introduce novel regularized backup operators for MCTS, based on the relative entropy of the policy update, and on the Tsallis entropy of the policy. Finally, we empirically evaluate the proposed operators in AlphaGo and AlphaZero on problems of increasing dimensionality and branching factor, from a toy problem to several Atari games, showing their superiority w.r.t. representative baselines.",./data/pdfs/ICML2021/Convex Regularization in Monte-Carlo Tree Search.pdf,./data/imgs/ICML2021/Convex Regularization in Monte-Carlo Tree Search.png,6b8ef53e7fda5cd6656256d71c630e89ec0d0d51,1.0,{2022: 1},8.0,"{2024: 1, 2022: 5, 2021: 2}"
23931,Reserve Price Optimization for First Price Auctions in Display Advertising,ICML,2021,"['Zhe Feng', 'Sébastien Lahaie', 'Jon Schneider', 'Jinchao Ye']",,./data/pdfs/ICML2021/Reserve Price Optimization for First Price Auctions in Display Advertising.pdf,./data/imgs/ICML2021/Reserve Price Optimization for First Price Auctions in Display Advertising.png,38c64dcbf215c6cc6682ed0f814ebf22d1fdf583,3.0,"{2023: 2, 2020: 1}",12.0,"{2023: 5, 2022: 4, 2021: 2, 2020: 1}"
23422,Differentially Private Sliced Wasserstein Distance,ICML,2021,"['Alain Rakotomamonjy', 'Liva Ralaivola']","Developing machine learning methods that are privacy preserving is today a central topic of research, with huge practical impacts. Among the numerous ways to address privacy-preserving learning, we here take the perspective of computing the divergences between distributions under the Differential Privacy (DP) framework -- being able to compute divergences between distributions is pivotal for many machine learning problems, such as learning generative models or domain adaptation problems. Instead of resorting to the popular gradient-based sanitization method for DP, we tackle the problem at its roots by focusing on the Sliced Wasserstein Distance and seamlessly making it differentially private. Our main contribution is as follows: we analyze the property of adding a Gaussian perturbation to the intrinsic randomized mechanism of the Sliced Wasserstein Distance, and we establish the sensitivityof the resulting differentially private mechanism. One of our important findings is that this DP mechanism transforms the Sliced Wasserstein distance into another distance, that we call the Smoothed Sliced Wasserstein Distance. This new differentially private distribution distance can be plugged into generative models and domain adaptation algorithms in a transparent way, and we empirically show that it yields highly competitive performance compared with gradient-based DP approaches from the literature, with almost no loss in accuracy for the domain adaptation problems that we consider.",./data/pdfs/ICML2021/Differentially Private Sliced Wasserstein Distance.pdf,./data/imgs/ICML2021/Differentially Private Sliced Wasserstein Distance.png,ef357174ed84714dbbcb48b53b65ae2d0e2a7dcb,3.0,"{2024: 2, 2023: 1}",17.0,"{2024: 6, 2023: 5, 2022: 5, 2021: 1}"
23848,"Learning Representations by Humans, for Humans",ICML,2021,['John P. O’Doherty'],,"./data/pdfs/ICML2021/Learning Representations by Humans, for Humans.pdf","./data/imgs/ICML2021/Learning Representations by Humans, for Humans.png",3bf9f7780c6079ca83c4d0681f3ae7c1c31e626f,1421.0,"{2025: 8, 2024: 32, 2023: 35, 2022: 43, 2021: 63, 2020: 134, 2019: 65, 2018: 65, 2017: 58, 2016: 84, 2015: 91, 2014: 96, 2013: 105, 2012: 92}",27.0,"{2024: 2, 2023: 5, 2022: 9, 2021: 6, 2020: 4, 2019: 1}"
23243,Learning to Weight Imperfect Demonstrations,ICML,2021,"['Yunke Wang', 'Chang Xu', 'Bo Du', 'Honglak Lee']",,./data/pdfs/ICML2021/Learning to Weight Imperfect Demonstrations.pdf,./data/imgs/ICML2021/Learning to Weight Imperfect Demonstrations.png,fab87cc094c0aedf2a283371de8339f466fdf3f8,10.0,"{2024: 2, 2023: 6, 2022: 1, 2021: 1}",36.0,"{2024: 5, 2023: 15, 2022: 15, 2021: 1}"
23741,When Does Data Augmentation Help With Membership Inference Attacks?,ICML,2021,"['Yiğitcan Kaya', 'Tudor Dumitraş']",,./data/pdfs/ICML2021/When Does Data Augmentation Help With Membership Inference Attacks?.pdf,./data/imgs/ICML2021/When Does Data Augmentation Help With Membership Inference Attacks?.png,390c52cf0549039e8f8f8999132c90c52cca26a9,11.0,"{2024: 4, 2023: 3, 2022: 2, 2021: 2}",47.0,"{2024: 8, 2023: 20, 2022: 15, 2021: 4}"
23265,Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies,ICML,2021,"['Paul Vicol', 'Luke Metz', 'Jascha Sohl‐Dickstein']","Unrolled computation graphs arise in many scenarios, including training RNNs, tuning hyperparameters through unrolled optimization, and training learned optimizers. Current approaches to optimizing parameters in such computation graphs suffer from high variance gradients, bias, slow updates, or large memory usage. We introduce a method called Persistent Evolution Strategies (PES), which divides the computation graph into a series of truncated unrolls, and performs an evolution strategies-based update step after each unroll. PES eliminates bias from these truncations by accumulating correction terms over the entire sequence of unrolls. PES allows for rapid parameter updates, has low memory usage, is unbiased, and has reasonable variance characteristics. We experimentally demonstrate the advantages of PES compared to several other methods for gradient estimation on synthetic tasks, and show its applicability to training learned optimizers and tuning hyperparameters.",./data/pdfs/ICML2021/Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies.pdf,./data/imgs/ICML2021/Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies.png,0b0cd862d6820b24fc4e3ca1c42cbe557ca49d9c,7.0,"{2024: 1, 2023: 3, 2022: 3}",54.0,"{2024: 5, 2023: 26, 2022: 23}"
23664,SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning,ICML,2021,"['Kimin Lee', 'Michael Laskin', 'Aravind Srinivas', 'Pieter Abbeel']","Off-policy deep reinforcement learning (RL) has been successful in a range of challenging domains. However, standard off-policy RL algorithms can suffer from several issues, such as instability in Q-learning and balancing exploration and exploitation. To mitigate these issues, we present SUNRISE, a simple unified ensemble method, which is compatible with various off-policy RL algorithms. SUNRISE integrates two key ingredients: (a) ensemble-based weighted Bellman backups, which re-weight target Q-values based on uncertainty estimates from a Q-ensemble, and (b) an inference method that selects actions using the highest upper-confidence bounds for efficient exploration. By enforcing the diversity between agents using Bootstrap with random initialization, we show that these different ideas are largely orthogonal and can be fruitfully integrated, together further improving the performance of existing off-policy RL algorithms, such as Soft Actor-Critic and Rainbow DQN, for both continuous and discrete control tasks on both low-dimensional and high-dimensional environments. Our training code is available at https://github.com/pokaxpoka/sunrise.",./data/pdfs/ICML2021/SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning.pdf,./data/imgs/ICML2021/SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning.png,324effa5a7c737257b675f85bd93d777fc486878,46.0,"{2024: 4, 2023: 12, 2022: 9, 2021: 21}",172.0,"{2024: 19, 2023: 64, 2022: 46, 2021: 36, 2020: 6, 2019: 1}"
20014,Approximate Frank-Wolfe Algorithms over Graph-structured Support Sets,ICML,2022,"['Baojian Zhou', 'Yifan Sun']","In this paper, we consider approximate Frank-Wolfe (FW) algorithms to solve convex optimization problems over graph-structured support sets where the linear minimization oracle (LMO) cannot be efficiently obtained in general. We first demonstrate that two popular approximation assumptions (additive and multiplicative gap errors) are not applicable in that no cheap gap-approximate LMO oracle exists. Thus, approximate dual maximization oracles (DMO) are proposed, which approximate the inner product rather than the gap. We prove that the standard FW method using a $\delta$-approximate DMO converges as $\mathcal{O}((1-\delta) \sqrt{s}/\delta)$ in the worst case, and as $\mathcal{O}(L/(\delta^2 t))$ over a $\delta$-relaxation of the constraint set. Furthermore, when the solution is on the boundary, a variant of FW converges as $\mathcal{O}(1/t^2)$ under the quadratic growth assumption. Our empirical results suggest that even these improved bounds are pessimistic, showing fast convergence in recovering real-world images with graph-structured sparsity.",./data/pdfs/ICML2022/Approximate Frank-Wolfe Algorithms over Graph-structured Support Sets.pdf,./data/imgs/ICML2022/Approximate Frank-Wolfe Algorithms over Graph-structured Support Sets.png,c5392a4e79e9976ecad3b0baac47c0eac00f8aff,0.0,{},0.0,{}
20824,Wide Bayesian neural networks have a simple weight posterior: theory and accelerated sampling,ICML,2022,"['Jiri Hron', 'Roman Novak', 'Jeffrey Pennington', 'Jascha Sohl‐Dickstein']","We introduce repriorisation, a data-dependent reparameterisation which transforms a Bayesian neural network (BNN) posterior to a distribution whose KL divergence to the BNN prior vanishes as layer widths grow. The repriorisation map acts directly on parameters, and its analytic simplicity complements the known neural network Gaussian process (NNGP) behaviour of wide BNNs in function space. Exploiting the repriorisation, we develop a Markov chain Monte Carlo (MCMC) posterior sampling algorithm which mixes faster the wider the BNN. This contrasts with the typically poor performance of MCMC in high dimensions. We observe up to 50x higher effective sample size relative to no reparametrisation for both fully-connected and residual networks. Improvements are achieved at all widths, with the margin between reparametrised and standard BNNs growing with layer width.",./data/pdfs/ICML2022/Wide Bayesian neural networks have a simple weight posterior: theory and accelerated sampling.pdf,./data/imgs/ICML2022/Wide Bayesian neural networks have a simple weight posterior: theory and accelerated sampling.png,af1a75b34fe1cf5a7ba143843fe376a9eadcb131,1.0,{2024: 1},3.0,"{2023: 2, 2022: 1}"
21050,Task-aware Privacy Preservation for Multi-dimensional Data,ICML,2022,"['Jiangnan Cheng', 'Ao Tang', 'Sandeep Chinchali']","Local differential privacy (LDP) can be adopted to anonymize richer user data attributes that will be input to sophisticated machine learning (ML) tasks. However, today's LDP approaches are largely task-agnostic and often lead to severe performance loss -- they simply inject noise to all data attributes according to a given privacy budget, regardless of what features are most relevant for the ultimate task. In this paper, we address how to significantly improve the ultimate task performance with multi-dimensional user data by considering a task-aware privacy preservation problem. The key idea is to use an encoder-decoder framework to learn (and anonymize) a task-relevant latent representation of user data. We obtain an analytical near-optimal solution for the linear setting with mean-squared error (MSE) task loss. We also provide an approximate solution through a gradient-based learning algorithm for general nonlinear cases. Extensive experiments demonstrate that our task-aware approach significantly improves ultimate task accuracy compared to standard benchmark LDP approaches with the same level of privacy guarantee.",./data/pdfs/ICML2022/Task-aware Privacy Preservation for Multi-dimensional Data.pdf,./data/imgs/ICML2022/Task-aware Privacy Preservation for Multi-dimensional Data.png,743c5923e1d581373e9b1910372be774aff049d8,1.0,{2024: 1},5.0,"{2024: 1, 2023: 2, 2022: 2}"
21034,Entropic Causal Inference: Graph Identifiability,ICML,2022,"['Jakob Runge', 'Sebastian Bathiany', 'Erik M. Bollt', 'Gustau Camps‐Valls', 'Dim Coumou', 'Ethan R. Deyle', 'Clark Glymour', 'Marlene Kretschmer', 'Miguel D. Mahecha', 'Jordi Muñoz-Marı́', 'Egbert H. van Nes', 'Jonas Peters', 'Rick Quax', 'Markus Reichstein', 'Marten Scheffer', 'Bernhard Schölkopf', 'Peter Spirtes', 'George Sugihara', 'Jie Sun', 'Kun Zhang', 'Jakob Zscheischler']","Abstract The heart of the scientific enterprise is a rational effort to understand the causes behind the phenomena we observe. In large-scale complex dynamical systems such as the Earth system, real experiments are rarely feasible. However, a rapidly increasing amount of observational and simulated data opens up the use of novel data-driven causal methods beyond the commonly adopted correlation techniques. Here, we give an overview of causal inference frameworks and identify promising generic application cases common in Earth system sciences and beyond. We discuss challenges and initiate the benchmark platform causeme.net to close the gap between method users and developers.",./data/pdfs/ICML2022/Entropic Causal Inference: Graph Identifiability.pdf,./data/imgs/ICML2022/Entropic Causal Inference: Graph Identifiability.png,8f15c7737d388f2d021a3d3d039fff324700a290,697.0,"{2025: 48, 2024: 129, 2023: 163, 2022: 149, 2021: 121, 2020: 68, 2019: 15}",9.0,"{2024: 2, 2023: 4, 2022: 3}"
20655,Deconfounded Value Decomposition for Multi-Agent Reinforcement Learning,ICML,2022,"['Hao Zhang', 'Aixin Sun', 'Wei Jing', 'Joey Tianyi Zhou']","Temporal sentence grounding in videos (TSGV), a.k.a., natural language video localization (NLVL) or video moment retrieval (VMR), aims to retrieve a temporal moment that semantically corresponds to a language query from an untrimmed video. Connecting computer vision and natural language, TSGV has drawn significant attention from researchers in both communities. This survey attempts to provide a summary of fundamental concepts in TSGV and current research status, as well as future research directions. As the background, we present a common structure of functional components in TSGV, in a tutorial style: from feature extraction from raw video and language query, to answer prediction of the target moment. Then we review the techniques for multimodal understanding and interaction, which is the key focus of TSGV for effective alignment between the two modalities. We construct a taxonomy of TSGV techniques and elaborate the methods in different categories with their strengths and weaknesses. Lastly, we discuss issues with the current TSGV research and share our insights about promising research directions.",./data/pdfs/ICML2022/Deconfounded Value Decomposition for Multi-Agent Reinforcement Learning.pdf,./data/imgs/ICML2022/Deconfounded Value Decomposition for Multi-Agent Reinforcement Learning.png,6deb0b694ab8b23842720cc3fead8a87417d2867,32.0,"{2025: 4, 2024: 24, 2023: 4}",11.0,"{2024: 3, 2023: 5, 2022: 3}"
20843,GSmooth: Certified Robustness against Semantic Transformations via Generalized Randomized Smoothing,ICML,2022,"['Zhongkai Hao', 'Chengyang Ying', 'Yinpeng Dong', 'Hang Su', 'Jun Zhu', 'Jian Song']","Certified defenses such as randomized smoothing have shown promise towards building reliable machine learning systems against $\ell_p$-norm bounded attacks. However, existing methods are insufficient or unable to provably defend against semantic transformations, especially those without closed-form expressions (such as defocus blur and pixelate), which are more common in practice and often unrestricted. To fill up this gap, we propose generalized randomized smoothing (GSmooth), a unified theoretical framework for certifying robustness against general semantic transformations via a novel dimension augmentation strategy. Under the GSmooth framework, we present a scalable algorithm that uses a surrogate image-to-image network to approximate the complex transformation. The surrogate model provides a powerful tool for studying the properties of semantic transformations and certifying robustness. Experimental results on several datasets demonstrate the effectiveness of our approach for robustness certification against multiple kinds of semantic transformations and corruptions, which is not achievable by the alternative baselines.",./data/pdfs/ICML2022/GSmooth: Certified Robustness against Semantic Transformations via Generalized Randomized Smoothing.pdf,./data/imgs/ICML2022/GSmooth: Certified Robustness against Semantic Transformations via Generalized Randomized Smoothing.png,f8f00236de15642d0583c142626a9ed3da96b404,2.0,{2023: 2},17.0,"{2024: 4, 2023: 10, 2022: 2, 2020: 1}"
20359,Constrained Optimization with Dynamic Bound-scaling for Effective NLP Backdoor Defense,ICML,2022,"['Wei Yang Bryan Lim', 'Nguyen Cong Luong', 'Dinh Thai Hoang', 'Yutao Jiao', 'Ying‐Chang Liang', 'Qiang Yang', 'Dusit Niyato', 'Chunyan Miao']","In recent years, mobile devices are equipped with increasingly advanced sensing and computing capabilities. Coupled with advancements in Deep Learning (DL), this opens up countless possibilities for meaningful applications, e.g., for medical purposes and in vehicular networks. Traditional cloud-based Machine Learning (ML) approaches require the data to be centralized in a cloud server or data center. However, this results in critical issues related to unacceptable latency and communication inefficiency. To this end, Mobile Edge Computing (MEC) has been proposed to bring intelligence closer to the edge, where data is produced. However, conventional enabling technologies for ML at mobile edge networks still require personal data to be shared with external parties, e.g., edge servers. Recently, in light of increasingly stringent data privacy legislations and growing privacy concerns, the concept of Federated Learning (FL) has been introduced. In FL, end devices use their local data to train an ML model required by the server. The end devices then send the model updates rather than raw data to the server for aggregation. FL can serve as an enabling technology in mobile edge networks since it enables the collaborative training of an ML model and also enables DL for mobile edge network optimization. However, in a large-scale and complex mobile edge network, heterogeneous devices with varying constraints are involved. This raises challenges of communication costs, resource allocation, and privacy and security in the implementation of FL at scale. In this survey, we begin with an introduction to the background and fundamentals of FL. Then, we highlight the aforementioned challenges of FL implementation and review existing solutions. Furthermore, we present the applications of FL for mobile edge network optimization. Finally, we discuss the important challenges and future research directions in FL.",./data/pdfs/ICML2022/Constrained Optimization with Dynamic Bound-scaling for Effective NLP Backdoor Defense.pdf,./data/imgs/ICML2022/Constrained Optimization with Dynamic Bound-scaling for Effective NLP Backdoor Defense.png,8355c3bf17986a2e43beb4fd4436292fdcf890cb,1845.0,"{2025: 55, 2024: 330, 2023: 503, 2022: 426, 2021: 367, 2020: 149, 2019: 7}",24.0,"{2024: 6, 2023: 11, 2022: 7}"
20972,Robust Counterfactual Explanations for Tree-Based Ensembles,ICML,2022,"['Sanghamitra Dutta', 'Jason Long', 'Saumitra Mishra', 'Cecilia Tilli', 'Daniele Magazzeni']","Counterfactual explanations inform ways to achieve a desired outcome from a machine learning model. However, such explanations are not robust to certain real-world changes in the underlying model (e.g., retraining the model, changing hyperparameters, etc.), questioning their reliability in several applications, e.g., credit lending. In this work, we propose a novel strategy -- that we call RobX -- to generate robust counterfactuals for tree-based ensembles, e.g., XGBoost. Tree-based ensembles pose additional challenges in robust counterfactual generation, e.g., they have a non-smooth and non-differentiable objective function, and they can change a lot in the parameter space under retraining on very similar data. We first introduce a novel metric -- that we call Counterfactual Stability -- that attempts to quantify how robust a counterfactual is going to be to model changes under retraining, and comes with desirable theoretical properties. Our proposed strategy RobX works with any counterfactual generation method (base method) and searches for robust counterfactuals by iteratively refining the counterfactual generated by the base method using our metric Counterfactual Stability. We compare the performance of RobX with popular counterfactual generation methods (for tree-based ensembles) across benchmark datasets. The results demonstrate that our strategy generates counterfactuals that are significantly more robust (nearly 100% validity after actual model changes) and also realistic (in terms of local outlier factor) over existing state-of-the-art methods.",./data/pdfs/ICML2022/Robust Counterfactual Explanations for Tree-Based Ensembles.pdf,./data/imgs/ICML2022/Robust Counterfactual Explanations for Tree-Based Ensembles.png,9627efce9888bbf78538e6d259f49081e91f391a,5.0,"{2024: 1, 2023: 4}",31.0,"{2024: 8, 2023: 18, 2022: 3, 2021: 1, 2020: 1}"
21006,DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations,ICML,2022,"['Fei Deng', 'Ingook Jang', 'Sungjin Ahn']","Top-performing Model-Based Reinforcement Learning (MBRL) agents, such as Dreamer, learn the world model by reconstructing the image observations. Hence, they often fail to discard task-irrelevant details and struggle to handle visual distractions. To address this issue, previous work has proposed to contrastively learn the world model, but the performance tends to be inferior in the absence of distractions. In this paper, we seek to enhance robustness to distractions for MBRL agents. Specifically, we consider incorporating prototypical representations, which have yielded more accurate and robust results than contrastive approaches in computer vision. However, it remains elusive how prototypical representations can benefit temporal dynamics learning in MBRL, since they treat each image independently without capturing temporal structures. To this end, we propose to learn the prototypes from the recurrent states of the world model, thereby distilling temporal structures from past observations and actions into the prototypes. The resulting model, DreamerPro, successfully combines Dreamer with prototypes, making large performance gains on the DeepMind Control suite both in the standard setting and when there are complex background distractions. Code available at https://github.com/fdeng18/dreamer-pro .",./data/pdfs/ICML2022/DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations.pdf,./data/imgs/ICML2022/DreamerPro: Reconstruction-Free Model-Based Reinforcement Learning with Prototypical Representations.png,48632df62a1e1c8ed7ad04b3ffd1bc62c133b3af,9.0,"{2024: 1, 2023: 5, 2022: 2}",41.0,"{2024: 9, 2023: 21, 2022: 11}"
20017,FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting,ICML,2022,"['Tian Zhou', 'Ziqing Ma', 'Qingsong Wen', 'Xue Wang', 'Liang Sun', 'Rong Jin']","Although Transformer-based methods have significantly improved state-of-the-art results for long-term series forecasting, they are not only computationally expensive but more importantly, are unable to capture the global view of time series (e.g. overall trend). To address these problems, we propose to combine Transformer with the seasonal-trend decomposition method, in which the decomposition method captures the global profile of time series while Transformers capture more detailed structures. To further enhance the performance of Transformer for long-term prediction, we exploit the fact that most time series tend to have a sparse representation in well-known basis such as Fourier transform, and develop a frequency enhanced Transformer. Besides being more effective, the proposed method, termed as Frequency Enhanced Decomposed Transformer ({\bf FEDformer}), is more efficient than standard Transformer with a linear complexity to the sequence length. Our empirical studies with six benchmark datasets show that compared with state-of-the-art methods, FEDformer can reduce prediction error by $14.8\%$ and $22.6\%$ for multivariate and univariate time series, respectively. Code is publicly available at https://github.com/MAZiqing/FEDformer.",./data/pdfs/ICML2022/FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting.pdf,./data/imgs/ICML2022/FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting.png,563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06,240.0,"{2025: 28, 2024: 104, 2023: 96, 2022: 6}",611.0,"{2024: 253, 2023: 309, 2022: 46, 2021: 2, 2020: 1}"
21617,Robust Weak Supervision with Variational Auto-Encoders,ICML,2023,"['Geert Litjens', 'Thijs Kooi', 'Babak Ehteshami Bejnordi', 'Arnaud Arindra Adiyoso Setio', 'Francesco Ciompi', 'Mohsen Ghafoorian', 'Jeroen van der Laak', 'Bram van Ginneken', 'Clara I. Sá\u200enchez']",,./data/pdfs/ICML2023/Robust Weak Supervision with Variational Auto-Encoders.pdf,./data/imgs/ICML2023/Robust Weak Supervision with Variational Auto-Encoders.png,40443d526d9dacfe4e311022dfb53727644f6087,11285.0,"{2025: 409, 2024: 1442, 2023: 1577, 2022: 1678, 2021: 1957, 2020: 1801, 2019: 1502, 2018: 734, 2017: 102, 2015: 1}",0.0,{}
21632,Concurrent Shuffle Differential Privacy Under Continual Observation,ICML,2023,"['Jay M. Tenenbaum', 'Haim Kaplan', 'Yishay Mansour', 'Uri Stemmer']","We introduce the concurrent shuffle model of differential privacy. In this model we have multiple concurrent shufflers permuting messages from different, possibly overlapping, batches of users. Similarly to the standard (single) shuffle model, the privacy requirement is that the concatenation of all shuffled messages should be differentially private. We study the private continual summation problem (a.k.a. the counter problem) and show that the concurrent shuffle model allows for significantly improved error compared to a standard (single) shuffle model. Specifically, we give a summation algorithm with error $\tilde{O}(n^{1/(2k+1)})$ with $k$ concurrent shufflers on a sequence of length $n$. Furthermore, we prove that this bound is tight for any $k$, even if the algorithm can choose the sizes of the batches adaptively. For $k=\log n$ shufflers, the resulting error is polylogarithmic, much better than $\tilde{\Theta}(n^{1/3})$ which we show is the smallest possible with a single shuffler. We use our online summation algorithm to get algorithms with improved regret bounds for the contextual linear bandit problem. In particular we get optimal $\tilde{O}(\sqrt{n})$ regret with $k= \tilde{\Omega}(\log n)$ concurrent shufflers.",./data/pdfs/ICML2023/Concurrent Shuffle Differential Privacy Under Continual Observation.pdf,./data/imgs/ICML2023/Concurrent Shuffle Differential Privacy Under Continual Observation.png,504a38c582fc12bda8147258cebd9038cf7a264a,0.0,{},1.0,{2023: 1}
22303,Tied-Augment: Controlling Representation Similarity Improves Data Augmentation,ICML,2023,"['Emirhan Kurtuluş', 'Zichao Li', 'Yann Dauphin', 'Ekin D. Cubuk']","Data augmentation methods have played an important role in the recent advance of deep learning models, and have become an indispensable component of state-of-the-art models in semi-supervised, self-supervised, and supervised training for vision. Despite incurring no additional latency at test time, data augmentation often requires more epochs of training to be effective. For example, even the simple flips-and-crops augmentation requires training for more than 5 epochs to improve performance, whereas RandAugment requires more than 90 epochs. We propose a general framework called Tied-Augment, which improves the efficacy of data augmentation in a wide range of applications by adding a simple term to the loss that can control the similarity of representations under distortions. Tied-Augment can improve state-of-the-art methods from data augmentation (e.g. RandAugment, mixup), optimization (e.g. SAM), and semi-supervised learning (e.g. FixMatch). For example, Tied-RandAugment can outperform RandAugment by 2.0% on ImageNet. Notably, using Tied-Augment, data augmentation can be made to improve generalization even when training for a few epochs and when fine-tuning. We open source our code at https://github.com/ekurtulus/tied-augment/tree/main.",./data/pdfs/ICML2023/Tied-Augment: Controlling Representation Similarity Improves Data Augmentation.pdf,./data/imgs/ICML2023/Tied-Augment: Controlling Representation Similarity Improves Data Augmentation.png,08f41705bda78b185a2e27bd6e613d138de7e8a7,0.0,{},2.0,"{2024: 1, 2023: 1}"
22353,Unifying Nesterov’s Accelerated Gradient Methods for Convex and Strongly Convex Objective Functions,ICML,2023,[],,./data/pdfs/ICML2023/Unifying Nesterov’s Accelerated Gradient Methods for Convex and Strongly Convex Objective Functions.pdf,./data/imgs/ICML2023/Unifying Nesterov’s Accelerated Gradient Methods for Convex and Strongly Convex Objective Functions.png,e419d3b29bc561bab58d68cacfb59d2230fdb460,,{},3.0,"{2024: 2, 2023: 1}"
22521,Monotonicity and Double Descent in Uncertainty Estimation with Gaussian Processes,ICML,2023,"['Liam Hodgkinson', 'Chris van der Heide', 'Fred Roosta', 'Michael W. Mahoney']","Despite their importance for assessing reliability of predictions, uncertainty quantification (UQ) measures for machine learning models have only recently begun to be rigorously characterized. One prominent issue is the curse of dimensionality: it is commonly believed that the marginal likelihood should be reminiscent of cross-validation metrics and that both should deteriorate with larger input dimensions. We prove that by tuning hyperparameters to maximize marginal likelihood (the empirical Bayes procedure), the performance, as measured by the marginal likelihood, improves monotonically} with the input dimension. On the other hand, we prove that cross-validation metrics exhibit qualitatively different behavior that is characteristic of double descent. Cold posteriors, which have recently attracted interest due to their improved performance in certain settings, appear to exacerbate these phenomena. We verify empirically that our results hold for real data, beyond our considered assumptions, and we explore consequences involving synthetic covariates.",./data/pdfs/ICML2023/Monotonicity and Double Descent in Uncertainty Estimation with Gaussian Processes.pdf,./data/imgs/ICML2023/Monotonicity and Double Descent in Uncertainty Estimation with Gaussian Processes.png,aa5e0b471948690e1271adb94df7b967360072d6,1.0,{2024: 1},4.0,"{2024: 1, 2023: 3}"
22218,GraphCleaner: Detecting Mislabelled Samples in Popular Graph Learning Benchmarks,ICML,2023,"['Yuwen Li', 'Miao Xiong', 'Bryan Hooi']","Label errors have been found to be prevalent in popular text, vision, and audio datasets, which heavily influence the safe development and evaluation of machine learning algorithms. Despite increasing efforts towards improving the quality of generic data types, such as images and texts, the problem of mislabel detection in graph data remains underexplored. To bridge the gap, we explore mislabelling issues in popular real-world graph datasets and propose GraphCleaner, a post-hoc method to detect and correct these mislabelled nodes in graph datasets. GraphCleaner combines the novel ideas of 1) Synthetic Mislabel Dataset Generation, which seeks to generate realistic mislabels; and 2) Neighborhood-Aware Mislabel Detection, where neighborhood dependency is exploited in both labels and base classifier predictions. Empirical evaluations on 6 datasets and 6 experimental settings demonstrate that GraphCleaner outperforms the closest baseline, with an average improvement of 0.14 in F1 score, and 0.16 in MCC. On real-data case studies, GraphCleaner detects real and previously unknown mislabels in popular graph benchmarks: PubMed, Cora, CiteSeer and OGB-arxiv; we find that at least 6.91% of PubMed data is mislabelled or ambiguous, and simply removing these mislabelled data can boost evaluation performance from 86.71% to 89.11%.",./data/pdfs/ICML2023/GraphCleaner: Detecting Mislabelled Samples in Popular Graph Learning Benchmarks.pdf,./data/imgs/ICML2023/GraphCleaner: Detecting Mislabelled Samples in Popular Graph Learning Benchmarks.png,a0fdfa167893903e2115f876594013bdbdafc21e,0.0,{},5.0,"{2024: 3, 2023: 2}"
21960,A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed Bandit Problems with Bandit Feedback,ICML,2023,"['Guanyu Nie', 'Yididiya Y. Nadew', 'Yanhui Zhu', 'Vaneet Aggarwal', 'Christopher J. Quinn']","We investigate the problem of stochastic, combinatorial multi-armed bandits where the learner only has access to bandit feedback and the reward function can be non-linear. We provide a general framework for adapting discrete offline approximation algorithms into sublinear $\alpha$-regret methods that only require bandit feedback, achieving $\mathcal{O}\left(T^\frac{2}{3}\log(T)^\frac{1}{3}\right)$ expected cumulative $\alpha$-regret dependence on the horizon $T$. The framework only requires the offline algorithms to be robust to small errors in function evaluation. The adaptation procedure does not even require explicit knowledge of the offline approximation algorithm -- the offline algorithm can be used as a black box subroutine. To demonstrate the utility of the proposed framework, the proposed framework is applied to diverse applications in submodular maximization. The new CMAB algorithms for submodular maximization with knapsack constraints outperform a full-bandit method developed for the adversarial setting in experiments with real-world data.",./data/pdfs/ICML2023/A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed Bandit Problems with Bandit Feedback.pdf,./data/imgs/ICML2023/A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed Bandit Problems with Bandit Feedback.png,08225d7c36879d2433e1bf8fc5d617b79f2e5185,1.0,{2024: 1},7.0,"{2024: 3, 2023: 4}"
22494,"For Pre-Trained Vision Models in Motor Control, Not All Policy Learning Methods are Created Equal",ICML,2023,"['Yingdong Hu', 'Renhao Wang', 'Li Erran Li', 'Yang Gao']","In recent years, increasing attention has been directed to leveraging pre-trained vision models for motor control. While existing works mainly emphasize the importance of this pre-training phase, the arguably equally important role played by downstream policy learning during control-specific fine-tuning is often neglected. It thus remains unclear if pre-trained vision models are consistent in their effectiveness under different control policies. To bridge this gap in understanding, we conduct a comprehensive study on 14 pre-trained vision models using 3 distinct classes of policy learning methods, including reinforcement learning (RL), imitation learning through behavior cloning (BC), and imitation learning with a visual reward function (VRF). Our study yields a series of intriguing results, including the discovery that the effectiveness of pre-training is highly dependent on the choice of the downstream policy learning algorithm. We show that conventionally accepted evaluation based on RL methods is highly variable and therefore unreliable, and further advocate for using more robust methods like VRF and BC. To facilitate more universal evaluations of pre-trained models and their policy learning methods in the future, we also release a benchmark of 21 tasks across 3 different environments alongside our work.","./data/pdfs/ICML2023/For Pre-Trained Vision Models in Motor Control, Not All Policy Learning Methods are Created Equal.pdf","./data/imgs/ICML2023/For Pre-Trained Vision Models in Motor Control, Not All Policy Learning Methods are Created Equal.png",480877d41ef927b901d67b9b7e1ab861564c079f,2.0,{2024: 2},11.0,"{2024: 6, 2023: 5}"
22610,Self-supervised learning of Split Invariant Equivariant representations,ICML,2023,"['Quentin Garrido', 'Laurent Najman', 'Yann LeCun']","Recent progress has been made towards learning invariant or equivariant representations with self-supervised learning. While invariant methods are evaluated on large scale datasets, equivariant ones are evaluated in smaller, more controlled, settings. We aim at bridging the gap between the two in order to learn more diverse representations that are suitable for a wide range of tasks. We start by introducing a dataset called 3DIEBench, consisting of renderings from 3D models over 55 classes and more than 2.5 million images where we have full control on the transformations applied to the objects. We further introduce a predictor architecture based on hypernetworks to learn equivariant representations with no possible collapse to invariance. We introduce SIE (Split Invariant-Equivariant) which combines the hypernetwork-based predictor with representations split in two parts, one invariant, the other equivariant, to learn richer representations. We demonstrate significant performance gains over existing methods on equivariance related tasks from both a qualitative and quantitative point of view. We further analyze our introduced predictor and show how it steers the learned latent space. We hope that both our introduced dataset and approach will enable learning richer representations without supervision in more complex scenarios. Code and data are available at https://github.com/facebookresearch/SIE.",./data/pdfs/ICML2023/Self-supervised learning of Split Invariant Equivariant representations.pdf,./data/imgs/ICML2023/Self-supervised learning of Split Invariant Equivariant representations.png,10923e416d15ab36161f4ab9ad40aa15bb91f541,1.0,{2024: 1},16.0,"{2024: 8, 2023: 8}"
22702,Are Diffusion Models Vulnerable to Membership Inference Attacks?,ICML,2023,"['Jinhao Duan', 'Fei Kong', 'Shiqi Wang', 'Xiaoshuang Shi', 'Kaidi Xu']","Diffusion-based generative models have shown great potential for image synthesis, but there is a lack of research on the security and privacy risks they may pose. In this paper, we investigate the vulnerability of diffusion models to Membership Inference Attacks (MIAs), a common privacy concern. Our results indicate that existing MIAs designed for GANs or VAE are largely ineffective on diffusion models, either due to inapplicable scenarios (e.g., requiring the discriminator of GANs) or inappropriate assumptions (e.g., closer distances between synthetic samples and member samples). To address this gap, we propose Step-wise Error Comparing Membership Inference (SecMI), a query-based MIA that infers memberships by assessing the matching of forward process posterior estimation at each timestep. SecMI follows the common overfitting assumption in MIA where member samples normally have smaller estimation errors, compared with hold-out samples. We consider both the standard diffusion models, e.g., DDPM, and the text-to-image diffusion models, e.g., Latent Diffusion Models and Stable Diffusion. Experimental results demonstrate that our methods precisely infer the membership with high confidence on both of the two scenarios across multiple different datasets. Code is available at https://github.com/jinhaoduan/SecMI.",./data/pdfs/ICML2023/Are Diffusion Models Vulnerable to Membership Inference Attacks?.pdf,./data/imgs/ICML2023/Are Diffusion Models Vulnerable to Membership Inference Attacks?.png,38c14931cf5dc7781b4f24af15e8938dfb898317,11.0,"{2024: 7, 2023: 4}",57.0,"{2024: 21, 2023: 36}"
